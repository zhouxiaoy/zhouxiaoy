<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta property="og:type" content="website">
<meta property="og:title" content="My Blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="zhouxy">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>My Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">My Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhouxy</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/24/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/4.%20%E8%8C%83%E5%BC%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/24/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/4.%20%E8%8C%83%E5%BC%8F/" class="post-title-link" itemprop="url">范式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-01-24 19:45:57 / 修改时间：15:16:18" itemprop="dateCreated datePublished" datetime="2022-01-24T19:45:57+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Matrix-Analysis/" itemprop="url" rel="index"><span itemprop="name">Matrix Analysis</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="Schur分解及Hermite矩阵"><a href="#Schur分解及Hermite矩阵" class="headerlink" title="Schur分解及Hermite矩阵"></a>Schur分解及Hermite矩阵</h4><h5 id="Schur定理"><a href="#Schur定理" class="headerlink" title="Schur定理"></a>Schur定理</h5><p>任意矩阵 A \in C ^{n \times n} ，一定存在酉矩阵 U ，使得 U^{-1}AU 为上三角矩阵</p>
<p>证明：</p>
<p>之前学习过任意矩阵可相似于Jordan标准型，因此存在非奇异矩阵 P 成立</p>
<script type="math/tex; mode=display">AP=PJ</script><p>根据上一节的QR分解，任意非奇异矩阵都可以分解为QR的形式，其中Q为标准正交矩阵，因此 P 可以分解为</p>
<script type="math/tex; mode=display">P=UR</script><p>其中 $U$ 是酉矩阵， $R $是主对角线上元素都大于0的上三角矩阵，因此</p>
<script type="math/tex; mode=display">
AUR=URJ</script><p>对等式做进一步变形，则</p>
<script type="math/tex; mode=display">U^{-1}AU=RJR^{-1}</script><p>因为 $R,J$ 为上三角矩阵，所以 </p>
<script type="math/tex; mode=display">
R^{-1},RJR^{-1}</script><p>也为上三角矩阵，因此矩阵 A 可相似于对角矩阵</p>
<h5 id="酉相似对角矩阵定理"><a href="#酉相似对角矩阵定理" class="headerlink" title="酉相似对角矩阵定理"></a>酉相似对角矩阵定理</h5><p>前面的Schurd定理给出了酉矩阵一定可相似于上三角矩阵，那什么情况下酉矩阵会相似于对角矩阵呢</p>
<p>给定矩阵 $A \in C^{n \times n} $，存在酉矩阵$ U $，使得 $U^{-1}AU $为对角矩阵的充要条件是</p>
<script type="math/tex; mode=display">
\overline A^TA = A\overline A^T \quad\quad(*)</script><p>其中满足 (*) 条件的矩阵 $A $称为正规矩阵，因此酉矩阵 $A$ 可相似对角化的充要条件是 $A$ 为正规矩阵</p>
<p>证明</p>
<ul>
<li>必要性：<br>已知：<script type="math/tex; mode=display">
U^{-1}AU = \begin{bmatrix}   \lambda_1\\  &\ddots\\ && \lambda_n \end{bmatrix}\\ 记\begin{bmatrix}   \lambda_1\\  &\ddots\\ && \lambda_n \end{bmatrix} = \Lambda</script>则<script type="math/tex; mode=display">
A=U \Lambda U^{-1}=U \Lambda \overline U^T</script>因此<script type="math/tex; mode=display">
\overline A^T A \overline {(U \Lambda \overline U^T)}^T U \Lambda \overline U^T=U\Lambda U^{-1}U \overline \Lambda ^T \overline U^T = A \overline A^T</script></li>
<li>充分性：<br>已知：<script type="math/tex; mode=display">
\overline A^TA = A\overline A^T</script></li>
</ul>
<p>由Schur定理，存在 $U$ 矩阵，使 $U^{-1}AU$ 为上三角矩阵，即</p>
<script type="math/tex; mode=display">
U^{-1}AU = \begin{bmatrix}   \lambda_1 & \cdots & * & \\  &\ddots & \vdots\\  &&\lambda_n \end{bmatrix}=\widetilde \Lambda</script><p>因此</p>
<script type="math/tex; mode=display">
A = U\widetilde \Lambda U^{-1}= U\widetilde \Lambda \overline U^T,A^T=U \overline {\widetilde \Lambda}^T \overline U^T</script><p>由已知条件</p>
<script type="math/tex; mode=display">
U \overline {\widetilde \Lambda}^T \overline U^TU\widetilde \Lambda \overline U^T = U\widetilde \Lambda \overline U^TU \overline {\widetilde \Lambda}^T \overline U^T\\可以得出\overline {\widetilde \Lambda}^T  \widetilde  \Lambda  = \widetilde \Lambda \overline {\widetilde \Lambda}^T</script><p>因此</p>
<script type="math/tex; mode=display">
\begin{bmatrix}   \overline \lambda_1 \\  \vdots & \ddots\\  \overline *&\cdots& \overline \lambda_n \end{bmatrix} \begin{bmatrix}   \lambda_1 & \cdots & * & \\  &\ddots & \vdots\\  &&\lambda_n \end{bmatrix} = \begin{bmatrix}   \lambda_1 & \cdots & * & \\  &\ddots & \vdots\\  &&\lambda_n \end{bmatrix} \begin{bmatrix}   \overline \lambda_1 \\  \vdots & \ddots\\  \overline *&\cdots& \overline \lambda_n \end{bmatrix}\\</script><p>通过比较元素可得，  $\widetilde  \Lambda $为对角矩阵，即</p>
<script type="math/tex; mode=display">
\widetilde  \Lambda  = \begin{bmatrix}   \lambda_1\\  &\ddots\\  &&\lambda_n \end{bmatrix} \\</script><h5 id="Hermite矩阵"><a href="#Hermite矩阵" class="headerlink" title="Hermite矩阵"></a>Hermite矩阵</h5><h6 id="Hermite矩阵的定义"><a href="#Hermite矩阵的定义" class="headerlink" title="Hermite矩阵的定义"></a>Hermite矩阵的定义</h6><p>矩阵 $A \in C^{n \times n}$ ，若满足 $\overline A^T = A $，则矩阵 $A$ 为Hermite矩阵</p>
<h6 id="Hermite矩阵的性质"><a href="#Hermite矩阵的性质" class="headerlink" title="Hermite矩阵的性质"></a>Hermite矩阵的性质</h6><ul>
<li><p>Hermite矩阵为正规矩阵<br>证明：<br>显然</p>
</li>
<li><p>Hermite矩阵可对角化<br>证明：Hermite矩阵为正规矩阵，由定理正规矩阵可对角化</p>
</li>
<li><p>Hermite矩阵的特征值为实数<br>证明：</p>
<script type="math/tex; mode=display">
A \alpha = \lambda \alpha</script><p>等式两边取共轭转置，则</p>
<script type="math/tex; mode=display">
\overline \alpha^T\overline  A^T   = \overline \alpha^T A  = \overline \lambda \overline\alpha ^T</script><p>等式两边右乘 $\alpha$, 则 </p>
<script type="math/tex; mode=display">
\overline \alpha^T A \alpha  =\overline \alpha^T \lambda \alpha  = \lambda  \overline \alpha^T\alpha  = \overline \lambda \overline\alpha^T \alpha</script><p>因此</p>
<script type="math/tex; mode=display">
\overline \lambda = \lambda</script><p>所以$\lambda$为实数</p>
</li>
</ul>
<h5 id="正定、非负定Hermite矩阵"><a href="#正定、非负定Hermite矩阵" class="headerlink" title="正定、非负定Hermite矩阵"></a>正定、非负定Hermite矩阵</h5><h6 id="正定、非负定Hermite矩阵定义"><a href="#正定、非负定Hermite矩阵定义" class="headerlink" title="正定、非负定Hermite矩阵定义"></a>正定、非负定Hermite矩阵定义</h6><p>已知Hermite矩阵 A</p>
<ul>
<li>正定： $\overline x^TAx &gt; 0$</li>
<li>非负定：$\overline x^TAx \geq 0$<h6 id="非负定Hermite矩阵的最大特征值的极值刻画"><a href="#非负定Hermite矩阵的最大特征值的极值刻画" class="headerlink" title="非负定Hermite矩阵的最大特征值的极值刻画"></a>非负定Hermite矩阵的最大特征值的极值刻画</h6>特征值的最大值<script type="math/tex; mode=display">
\lambda_{\max} =\max  \limits_{x \ne 0}\{ \frac{\overline x^TAx}{\overline x^Tx} \} =\max  \limits_{\|x\| = 1}\{\overline x^TAx \}</script>首先说明一下第二个等式如何过去的，因为<script type="math/tex; mode=display">
\overline x^Tx= \langle x,x\rangle = \|x\|^2</script>因此<script type="math/tex; mode=display">
\frac{\overline x^TAx}{\overline x^Tx} = \frac{\overline x^TAx}{\|x\|^2} = \frac{ \overline x^T}{\|x\|}A\frac{x}{\|x\|} = \overline x^TAx</script>其中新的 $x=\frac{x}{|x|} $，满足<script type="math/tex; mode=display">\|x\|=\|\frac{x}{\|x\|}\|=\frac{1}{\|x\|}\|x\|=1</script>即 x 在单位球上</li>
</ul>
<p>证明：</p>
<p>因为 $A$ 为Hermite矩阵，因此矩阵$ A$ 可相似于对角矩阵，即</p>
<script type="math/tex; mode=display">
U^{-1}AU  = \overline U^TAU = \begin{bmatrix}   \lambda_1\\  &\ddots\\ && \lambda_n \end{bmatrix}=\Lambda</script><p>因此可以得到</p>
<script type="math/tex; mode=display">
A=(\overline U^T)^{-1}\Lambda U^{-1}=U\Lambda U^{-1}</script><p>因此</p>
<script type="math/tex; mode=display">
\overline x^TAx = \overline x^T(U \Lambda \overline U^T)x = \overline {(\overline U^Tx)}^T \Lambda (\overline U^Tx)</script><p>令$y=\overline U^Tx$,因此</p>
<script type="math/tex; mode=display">
\overline x^TAx  = \overline y^T\Lambda y  = \lambda_1\|y_1\|^2+\lambda_2\|y_2\|^2+...+\lambda_n\|y_n\|^2</script><p>其中</p>
<script type="math/tex; mode=display">
\langle y,y \rangle  = \overline y^Ty = \overline{ \overline U^Tx}^T \overline U^Tx = \overline {x^T \overline U} \overline U^Tx = \overline x^T  U\overline U^Tx =\overline  x^Tx =\langle x,x \rangle =1</script><p>因此</p>
<script type="math/tex; mode=display">
\|y_1\|^2+\|y_2\|^2+...+\|y_n\|^2 =1</script><p>所以</p>
<script type="math/tex; mode=display">
\lambda_{\min}\leq\overline x^TAx  \leq  \lambda_{\max}
$$ 注：对正交矩阵也成立

其中 $\frac{\overline x^TAx}{\overline x^Tx}$ 被称为Rayleigh商，用商来刻画特征值极值的问题


### 矩阵的SVD分解
##### SVD分解定理
设 $A \in C^{m \times n}$，则存在 m 阶酉矩阵 $U $和 n 阶酉矩阵 $V$ ，使得</script><p>U^{-1}AV =\overline U^TAV =\begin{bmatrix}    \Sigma_r &amp; 0_{r \times (n-r)}\\   0_{(m-r) \times r}&amp;0_{(m-r)\times(n-r)}\\  \end{bmatrix}</p>
<script type="math/tex; mode=display">
其中</script><p>\Sigma_r =  \begin{bmatrix}    \sigma_1\\ &amp;\ddots\\ &amp;&amp; \sigma_r \end{bmatrix}, r = rank(A)</p>
<script type="math/tex; mode=display">
并且</script><p>\sigma_i = \sqrt{\lambda_i(\overline A^TA)}&gt; 0,i=1,2,…,r </p>
<script type="math/tex; mode=display">称为矩阵 $A$ 的奇异值

证明：

记 $H=\overline A^TA $，容易得到矩阵 H 为Hermite矩阵，因此一定存在酉矩阵 V 使其相似于对角矩阵，即</script><p>\overline V^T(\overline A^TA)V = \overline{(AV)}^T(AV) = \left [\begin{array}{c:c}     \begin{bmatrix}        \lambda_1\\&amp;\ddots\\     &amp;&amp; \lambda_r\\     \end{bmatrix} &amp; 0\\   \hdashline\\     0 &amp; 0\\   \end{array}\right]\\</p>
<script type="math/tex; mode=display">
其中</script><p>r=rank(\overline A^TA),\lambda_i=\lambda_i(\overline A^TA)&gt;0</p>
<script type="math/tex; mode=display">
记 $AV=B \in C ^{m \times n}$对矩阵$ B $做分割</script><p>B= \left [\begin{array}{cccc:ccc}         b_1&amp;b_2&amp;…&amp;b_r&amp;b_{r+1}&amp;…&amp;b_n \end{array}\right] =\left [\begin{array}{c:c}         B_1&amp;B_2 \end{array}\right]</p>
<script type="math/tex; mode=display">
因此等式变为
$$\overline B^TB = \begin{bmatrix}    \overline B_1^T\\ \overline B_2^T \end{bmatrix} \begin{bmatrix}    B_1 & B_2 \end{bmatrix} =  \begin{bmatrix}    \overline B_1^TB_1 & \overline B_1^TB_2\\ \overline B_2^TB_1 & \overline B_2^TB_2 \end{bmatrix} = \left [\begin{array}{c:c}     \begin{bmatrix}        \lambda_1\\&\ddots\\     && \lambda_r\\     \end{bmatrix} & 0\\   \hdashline\\     0 & 0\\   \end{array}\right]</script><p>因此可以得到</p>
<script type="math/tex; mode=display">
B_2=0, \overline B_1^TB_1  = \begin{bmatrix}    \lambda_1\\&\ddots\\ && \lambda_r\\ \end{bmatrix}</script><p>即</p>
<script type="math/tex; mode=display">
\overline B_1^TB_1  = \begin{bmatrix}    \overline b_1^T\\ \overline b_2^T\\ \vdots\\ \overline b_r^T \end{bmatrix} \begin{bmatrix}    b_1 &b_2 &\cdots &b_r \end{bmatrix} = \begin{bmatrix}    \langle b_1,b_1 \rangle & \langle b_1,b_2 \rangle & \cdots & \langle b_1,b_r \rangle\\ \langle b_2,b_1 \rangle & \langle b_2,b_2 \rangle & \cdots & \langle b_2,b_r \rangle\\ \vdots& \vdots&\ddots&\vdots\\ \langle b_r,b_1 \rangle & \langle b_r,b_2 \rangle & \cdots & \langle b_r,b_r \rangle \end{bmatrix} = \begin{bmatrix}    \lambda_1\\&\ddots\\ && \lambda_r\\ \end{bmatrix}</script><p>因此 $b_1,b_2,…,b_r$ 是 $C^m $中的正交向量组，而不是标准正交向量组</p>
<p>将 $b_1,b_2,…,b_r$ 单位化，即</p>
<script type="math/tex; mode=display">
\widetilde b_i=\frac{b_i}{\|b_i\|} = \frac{b_i}{\sqrt{\lambda_i}} \Rightarrow b_i= \widetilde b_i \sqrt{\lambda_i}</script><p>将 $\widetilde b_1,…,\widetilde b_r $扩充为 $C^m $的一组标准正交基</p>
<script type="math/tex; mode=display">
\widetilde b_1,...,\widetilde b_r,\beta_{r+1},...,\beta_m</script><p>记为$ U$</p>
<p>则</p>
<script type="math/tex; mode=display">
AV = B = \begin{bmatrix}    b_1  & \cdots &b_r & 0 & \cdots & 0 \end{bmatrix} = \begin{bmatrix}    \widetilde  b_1  & \cdots & \widetilde b_r & \beta_{r+1} & \cdots & \beta_m \end{bmatrix} \left [\begin{array}{c:c}     \begin{bmatrix}        \sqrt{\lambda_1}\\&\ddots\\     && \sqrt{\lambda_r}\\     \end{bmatrix} & 0\\   \hdashline\\     0 & 0\\   \end{array}\right]</script><h5 id="最大奇异值的意义"><a href="#最大奇异值的意义" class="headerlink" title="最大奇异值的意义"></a>最大奇异值的意义</h5><script type="math/tex; mode=display">
\delta_{\max}=\sqrt{\lambda_{\max}(\overline A^TA)}\\ = \sqrt{\max  \limits_{x \ne0}\frac{\overline x^T(\overline A^TA)x}{\overline x^Tx}}\\ = \max \limits_{x \ne0}\sqrt{\frac{\overline (Ax)^T(Ax)}{\overline x^Tx}}\\ = \max \limits_{x \ne0}\sqrt{\frac{\|Ax\|^2}{\|x\|^2}}\\ = \max \limits_{x \ne0}\frac{\|Ax\|^2}{\|x\|^2}</script><p>因此最大奇异值可以理解为长度的最大放大率</p>
<h3 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h3><h4 id="向量范数与矩阵范数（一）"><a href="#向量范数与矩阵范数（一）" class="headerlink" title="向量范数与矩阵范数（一）"></a>向量范数与矩阵范数（一）</h4><h5 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h5><p>之前我们在学欧几里得空间或者酉空间时利用内积定义了两个向量之间的距离。对于 $\mathbb C^n$ ,它作为酉空间已经定义好了向量的长度以及两个向量之间的距离。事实上，对于 $\mathbb C^n$ 我们还可以定义新的长度以及新的距离。由此我们定义向量范数</p>
<p>定义1：设 $\mathbb C^n\rightarrow \mathbb R $的函数 $\Vert\bullet\Vert$ 满足</p>
<p>(1)正定性： $\forall x\in \mathbb C^n,\Vert x\Vert\ge0 $,其中$ \Vert x\Vert=0 $当且仅当 $x=0$</p>
<p>(2)齐次性： $\forall x\in\mathbb C^n,\forall\alpha\in\mathbb C $,有 $\Vert \alpha x\Vert=\vert\alpha\vert\cdot\Vert x\Vert$</p>
<p>(3)三角不等式： $\forall x,y\in\mathbb C^n $,有$ \Vert x+y\Vert\le\Vert x\Vert+\Vert y\Vert$</p>
<p>则称函数$ \Vert\bullet\Vert$为$ \mathbb C^n $上的向量范数</p>
<p>下面我们看一看常见的四个向量范数</p>
<p>常见的 $\mathbb C^n $上的向量范数:对于 $x=(x_1,x_2,…,x_n)^T$</p>
<ul>
<li><p>(1) 1 -范数: $\Vert x||_1=\sum_{j=1}^{n}\vert x_j\vert$</p>
</li>
<li><p>(2) $\infty$ -范数: $\Vert x\Vert_\infty =\max_{1\le j\le n}\vert x_j\vert$</p>
</li>
<li><p>(3) 2-范数: $\Vert x\Vert_2=\sqrt{x^Hx}=(x,x)=\sqrt{\sum_{j=1}^n\vert x_j\vert^2} $，我们也称2-范数为 <strong><em>欧几里得范数</em></strong></p>
</li>
<li><p>(4) $p$-范数$（ p\ge1 ）: \Vert x\Vert_p=(\sum_{j=1}^n\vert x_j|^p)^{\frac{1}{p}}$</p>
</li>
</ul>
<p>情形(1)(2)容易验证，情形(4)证明过程比较繁琐，涉及到Young不等式，Hölder不等式，Minkovski不等式，自行了解，在此仅针对(3)中三角不等式做说明</p>
<p>此时</p>
<script type="math/tex; mode=display">
\begin{align*} \Vert x+y\Vert^2_2&=(x+y,x+y)\\ &=(x,x)+(x,y)+(y,x)+(y,y)\\&=\Vert x\Vert^2+\Vert y\Vert^2+(x,y)+(y,x)\\ &\le\Vert x\Vert^2_2+\Vert y\Vert^2_2+2\Vert x\Vert_2\,\,\Vert y\Vert_2\\&=(\Vert x\Vert_2+\Vert y\Vert_2)^2 \end{align*}</script><p>在实际问题中我们经常要估计计算出来的向量值与真实的向量值之间的误差，下面定理告诉我们无论用哪个向量范数衡量误差，效果是一样的</p>
<p>定理1(向量范数的等价性)：设<br>$\Vert\bullet\Vert_s,\Vert\bullet\Vert_t$ 均为$ \mathbb C^n$ 上的向量范数，则一定存在常数 $C_1,C_2 $,使得对 $\forall x\in\mathbb C^n$ 有$ C_1\Vert x\Vert_s\le\Vert x\Vert_t\le C_2 \Vert x\Vert_s$</p>
<p>下面我们仅说明任意一个向量范数与 $\infty$ -范数等价</p>
<p>考虑函数 $f(x)=\Vert x\Vert_t$ 以及集合<br>$<br>S=\{x\in\mathbb C^n\bigg|\,||x||_\infty=1\} $,则 S 是 $\mathbb C^n$ 上的有界闭集，而 f(x) 是实函数，在 S 上一定有最小值与最大值，分别设为 $C_1,C_2$</p>
<p>则对任意的 $0\ne x\in \mathbb C^n$ ,有 $\frac{x}{\Vert x|_\infty}\in S $,从而 $C_1\le f(\frac{x}{\Vert x|_\infty})\le C_2$</p>
<p>从而 $C_1\le \frac{|x|_t}{\Vert x|_\infty}\le C_2 $,从而 $C_1| x|_\infty\le| x|_t\le C_2|x|_\infty$</p>
<h4 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h4><p>对于 $n\times n$ 复矩阵空间 $\mathbb C^{n\times n}$ ,我们也希望定义一个长度衡量矩阵的大小，定义距离比较两个矩阵之间的接近程度，由此我们引进了矩阵范数</p>
<p>定义：设 $\mathbb C^{n\times n}\rightarrow \mathbb R $的函数 $\Vert\bullet\Vert $满足</p>
<ul>
<li><p>(1)正定性： $\forall A\in \mathbb C^{n\times n},\Vert A\Vert\ge0 $,其中 $\Vert A\Vert=0$ 当且仅当 A=0</p>
</li>
<li><p>(2)齐次性： $\forall A\in\mathbb C^{n\times n},\forall\alpha\in\mathbb C ,$有 $\Vert \alpha A\Vert=\vert\alpha\vert\cdot\Vert A\Vert$</p>
</li>
<li><p>(3)三角不等式： $\forall A,B\in\mathbb C^{n\times n}$ ,有 $\Vert A+B\Vert\le\Vert A\Vert+\Vert B\Vert$</p>
</li>
</ul>
<p>(4)乘法相容性：$\forall A,B\in\mathbb C^{n\times n}$ ,有 $\Vert AB\Vert\le\Vert A\Vert\,\Vert B\Vert$</p>
<p>则称函数 $\Vert\bullet\Vert为 \mathbb C^{n\times n} $上的矩阵范数</p>
<p>有的资料上直接用条件(1)(2)(3)定义矩阵范数（因为条件(1)(2)(3)也适用于一般的范数，具有某种普适性），然而很多情况下我们要用条件(4)分析矩阵，探究矩阵级数等问题。所以这里我们统一用条件(1)(2)(3)(4)定义矩阵范数。以后谈到矩阵范数，默认矩阵范数满足对乘法的相容性。</p>
<p>很多时候我们要将矩阵级数与向量级数放到一起进行研究，希望能保持对矩阵乘以向量的运算的相容性，即 $|Ax|_t\le |A|_s|x|_t $。由此我们得到如下定理</p>
<p>定理：设$\Vert\bullet\Vert_s$为 $\mathbb C^{n\times n}$ 上的矩阵范数，则一定存在 $\mathbb C^n$ 上的向量范数 $|\bullet|_t$ 满足对 $\forall A\in \mathbb C^{n\times n},\forall x\in\mathbb C^n $,有 $|Ax|_t\le|A|_s|x|_t$</p>
<p>证：任意给定非零向量 $\eta_0$ ,定义向量范数 $|x|_t=|x\eta_0^T|_s$</p>
<p>此时该向量范数满足正定性，齐次性，三角不等式，且</p>
<script type="math/tex; mode=display">
\|Ax\|_t=\|Ax\eta_0^T\|_s\le\|A\|_s\|x\eta_0^T\|_s=\|A\|_s\|x\|_t</script><p>由定理可知这样定义的向量范数 <strong><em>不一定唯一</em></strong></p>
<p>给定一个向量范数，我们总是可以构造出矩阵范数，并且满足与矩阵范数对矩阵与向量乘法的相容性。由此我们定义矩阵的算子范数</p>
<p>定义：设 $|\bullet|_t$ 是 $\mathbb C^n $上的向量范数，我们定义矩阵范数 $|\bullet|_t$ 如下（容易验证满足条件(1)(2)(3)(4)）:</p>
<script type="math/tex; mode=display">
\|A\|_t=\max_{x\ne0}\frac{\|Ax\|_t}{\|x\|_t}</script><p>则称该矩阵范数是由向量范数 $|\bullet|_t$ 诱导出来的算子范数。此时 $|Ax|_t\le|A|_t|x|_t$</p>
<p>事实上， </p>
<script type="math/tex; mode=display">
\|ABx\|_t\le\|A\|_t\|B\|_t\|x\|_t</script><p>从而 </p>
<script type="math/tex; mode=display">
\|AB\|_t=\max_{x\ne0}\frac{\|ABx\|_t}{\|x\|_t}\le\|A\|_t\|B\|_t</script><p>下面我们来看一看常见的算子范数</p>
<p>定理：设 $A\in\mathbb C^{n\times n} , x\in\mathbb C^{n\times n}$ ,则</p>
<ul>
<li><p>(1) $|A|_\infty=\max_{1\le i\le n}\sum_{j=1}^{n}|x_{ij}|$ （行范数）</p>
</li>
<li><p>(2) $|A|_1=\max_{1\le j\le n}\sum_{i=1}^{n}|x_{ij}|$ （列范数）</p>
</li>
<li><p>(3) $|A|_2=\sqrt{\lambda_{\max}(A^HA)}$（2-范数），其中 $\lambda_{\max}(A^HA)$ 表示 $A^HA$ 的最大特征值</p>
</li>
</ul>
<p>证：<br>(1)设 $x=(x_1,x_2,…,x_n)^T\ne 0$ ,<br>记 </p>
<script type="math/tex; mode=display">
\begin{align*}
t &=\|x\|_\infty=\max_{1\le i\le n}|x_i|,\mu=\max_{1\le i\le n}\sum_{j=1}^{n}|a_{ij}| \\
\|Ax\|_\infty &=\max_{1\le i\le n}|\sum_{j=1}^na_{ij}x_j|\le \max_{1\le i\le n}\sum_{j=1}^n|a_{ij}||x_j|\le t\max_{1\le i\le n}\sum_{j=1}^n|a_{ij}|=\|x\|_\infty \mu
\end{align*}</script><p>则</p>
<script type="math/tex; mode=display">
\|A\|_\infty=\max_{x\ne0}\frac{\|Ax\|_\infty}{\|x\|_\infty}\le\mu</script><p>设 $\mu=\sum_{j=1}^n|a_{i_0j}| $,令 $x_0=(sgn{(a_{i_{0}1})},sgn{(a_{i_{0}2})},…,sgn{(a_{i_{0}n})})^T $,则 $Ax_0$ 的第 $i_0$ 个分量为 $\sum_{j=1}^na_{i_0j}sgn{(a_{i_0j})}=\sum_{j=1}^n|a_{i_0j}|=\mu ,且 |x_0|_\infty=1 ,从而 \frac{|Ax_0|_\infty}{|x_0|_\infty}=\mu$</p>
<p>从而 $|A|_\infty=\mu=\max_{1\le i\le n}\sum_{j=1}^{n}|a_{ij}|$</p>
<p>(2)设 $x=(x_1,x_2,…,x_n)^T\ne 0 $,记 $t=|x|_1=\sum_{j=1}^n|x_j|,\mu=\max_{1\le j\le n}\sum_{i=1}^{n}|a_{ij}|$，则</p>
<script type="math/tex; mode=display">
\begin{align*} \|Ax\|_1&=\sum_{i=1}^n|\sum_{j=1}^na_{ij}x_j|\\ &\le\sum_{i=1}^n\sum_{j=1}^n|a_{ij}||x_j|\\ &\le\sum_{j=1}^n\sum_{i=1}^n|a_{ij}||x_j|\\ &=\sum_{j=1}^n(\sum_{i=1}^n|a_{ij}|)|x_j|\\ &\le\sum_{j=1}^n \mu|x_j|=\mu t=\mu\|x\|_1 \end{align*}</script><p>则 $|A|_1=\max_{x\ne0}\frac{|Ax|_1}{|x|_1}\le\mu$</p>
<p>令 $\mu=\sum_{i=1}^n|a_{ij_0}| , e_{j_0} $为第 $j_0$ 个分量为1，其余分量为0的列向量</p>
<p>则 $|Ae_{j_0}|_1=\sum_{i=1}^n|a_{ij_0}|=\mu , |e_{j_0}|_1=1$</p>
<p>故$|A|_1=\mu =\max_{1\le j\le n}\sum_{i=1}^{n}|a_{ij}|$</p>
<p>(3)注意到 $A^HA$ 是Hermite矩阵，且$ x^HA^HAx=(Ax)^H(Ax)\ge 0 $,故$ A^HA$ 是半正定Hermite矩阵，从而 $A^HA $的特征值全为非负数，不妨设为</p>
<script type="math/tex; mode=display">
\lambda_1\ge\lambda_2\ge...\ge\lambda_n\ge0</script><p>则一定存在酉矩阵 $U$ 使得</p>
<script type="math/tex; mode=display">
U^HA^HAU=\begin{pmatrix} \lambda_1\\&\lambda_2\\&&\ddots\\&&&\lambda_n \end{pmatrix}</script><p>设 $x=Uy$ ,则 $|Ax|_2^2=(Ax)^H(Ax)=x^HA^HAx=y^HU^HA^HAUy=\sum_{j=1}^n\lambda_j|y_j|^2$</p>
<p>|x|_2^2=x^Hx=y^HU^HUy=y^Hy=\sum_{j=1}^n|y_j|^2</p>
<p>从而 </p>
<script type="math/tex; mode=display">
\|Ax\|^2_2\le\lambda_1\|x\|_2^2</script><p>而</p>
<script type="math/tex; mode=display">
\|AUe_1\|_2^2=\lambda_1\|Ue_1\|_2^2</script><p>故</p>
<script type="math/tex; mode=display">
\|A\|_2=\sqrt{\lambda_1}=\sqrt{\lambda_{\max}(A^HA)}</script><p>看起来矩阵的行范数以及列范数是容易计算的，编程起来比较容易实现（只需遍历矩阵中的元素，求和后弄一弄排序算法即可实现），而2-范数相对来说计算起来不太方便，通过编程实现不太好操作。但2-范数在理论上应用广泛，性质良好。</p>
<h5 id="矩阵范数的等价性"><a href="#矩阵范数的等价性" class="headerlink" title="矩阵范数的等价性"></a>矩阵范数的等价性</h5><p>首先，与向量范数类似，矩阵范数同样也有等价性</p>
<p>定理1：设 $|\bullet|_s,|\bullet|_t 均为 \mathbb C^{n\times n}$ 上的矩阵范数，则一定存在常数 $C_1,C_2$ 使得对任意的$ A\in \mathbb C^{n\times n}$ 有 $C_1|A|_s\le|A|_t\le C_2|A|_s$</p>
<p>2.矩阵范数与特征值的模的比较<br>给定一个n阶矩阵，它的矩阵范数与特征值的模到底有什么联系？我们先看如下定义</p>
<p>定义：设$ A\in \mathbb C^{n\times n} 的特征值为 \lambda_1,\lambda_2,…,\lambda_n $,称 $\rho(A)=\max_{1\le i\le n}|\lambda_i|$ 为矩阵 $A $的谱半径</p>
<p>对于矩阵范数与矩阵的谱半径，我们有</p>
<p>定理：设$\Vert\bullet\Vert为\mathbb C^{n\times n}$上的矩阵范数，则 $\forall{A}\in \mathbb C^{n\times n}$ ,有 $\rho(A)\le|A|$</p>
<p>证：设 $\lambda$ 是 $A$ 的任意特征值，对应非零特征向量为 $\eta$ ,则 $O\ne \eta\eta^H\in\mathbb C^{n\times n}<br>$<br>则 </p>
<script type="math/tex; mode=display">
\|A\eta\eta^H\|=\|\lambda \eta\eta^H\|=|\lambda|\|\eta\eta^H\|
\|A\eta\eta^H\|\le\|A\|\|\eta\eta^H\|</script><p>从而</p>
<script type="math/tex; mode=display">
|\lambda|\le\|A\| ,从而 \rho(A)\le\|A\|</script><p>特别的，我们有<br>定理：设 $A$ 是正规矩阵，则$ A$ 的2-范数 $|A|_2=\rho(A)$</p>
<p>证：此时存在酉矩阵 $U$ 使得 $A=U^HDU$ ,其中 $D=diag(\lambda_1,\lambda_2,…,\lambda_n) $是对角矩阵</p>
<p>不妨设 $|\lambda_1|\ge|\lambda_2|\ge…\ge|\lambda_n|$ ,则 $\rho(A)=|\lambda_1|$</p>
<p>此时 $A^HA=U^HD^HDU$</p>
<script type="math/tex; mode=display">
D^HD=diag(|\lambda_1|^2,|\lambda_2|^2,...,|\lambda_n|^2)
\|A\|_2=|\lambda_1|=\rho(A)</script><p>前面我们讨论的是一般的矩阵范数，以下我们要讨论矩阵的算子范数</p>
<h5 id="E-A的可逆性"><a href="#E-A的可逆性" class="headerlink" title="E+A的可逆性"></a>E+A的可逆性</h5><p>我们先看一个引理，尽管这个引理非常简单</p>
<p>引理：设$\Vert\bullet\Vert为\mathbb C^{n\times n}$上的算子范数，$E$ 为 n 阶单位阵，则 $|E|= 1$</p>
<p>证：此时 $\frac{|Ex|}{|x|}=\frac{|x|}{|x|}=1$ ,故 $|E|=1$</p>
<p>定理4：设$\Vert\bullet\Vert为\mathbb C^{n\times n}$上的算子范数， $A\in\mathbb C^{n\times n} $满足 $|A|&lt; 1 , E$ 为 n 阶单位阵，则$ E+A $可逆，且$ |(E+A)^{-1}|\le\frac{1}{1-|A|}$</p>
<p>证：反设 $E+A $不可逆，则一定存在$ 0\ne x_0\in\mathbb C^n 满足 (E+A)x_0=0 $,即 $$<br>x_0=-Ax_0</p>
<script type="math/tex; mode=display">
此时</script><p>|x_0|=|-Ax_0|=|Ax_0|\le|A||x_0|&lt;|x_0|</p>
<script type="math/tex; mode=display">
矛盾，故 $E+A$ 可逆

此时 $(E+A)(E+A)^{-1}=E$ ,即$ (E+A)^{-1}=E-A(E+A)^{-1}$

则</script><p>|(E+A)^{-1}|\le|E|+|A(E+A)^{-1}|\le1+|A||(E+A)^{-1}|</p>
<script type="math/tex; mode=display">
即</script><p>(1-|A|)|(E+A)^{-1}|\le1</p>
<script type="math/tex; mode=display">
即</script><p>|(E+A)^{-1}|\le\frac{1}{1-|A|}</p>
<script type="math/tex; mode=display">
然而并不是所有的矩阵范数都是算子范数，下面我们就来介绍一个常用的非算子范数的矩阵范数。

##### 非算子范数的矩阵范数示例——F范数
定义：设 $A=(a_{ij})_{n \times n}\in \mathbb C^{n \times n} $,定义 $\|A\|_F=({\sum_{i=1}^n\sum_{j=1}^n|a_{ij}|^2})^\frac{1}{2}$
称 $\|\bullet\|_F 为\mathbb C^{n\times n}$ 上的Frobenius范数，简称F-范数

容易验证F-范数满足正定性，齐次性，三角不等式。下面验证F-范数满足对乘法的相容性

设 $A=(a_{ij})_{n\times n},B=(b_{ij})_{n\times n}$ ,则</script><p>\begin{align<em>} |AB|_F^2&amp;=\sum_{i=1}^{n}\sum_{j=1}^n(\sum_{k=1}^na_{ik}b_{kj})^2\\ &amp;\le\sum_{i=1}^{n}\sum_{j=1}^n(\sum_{k=1}^n|a_{ik}b_{kj}|)^2\\ &amp;\le \sum_{i=1}^{n}\sum_{j=1}^n\bigg((\sum_{k=1}^n|a_{ik}|^2)(\sum_{k=1}^n |b_{kj}|^2)\bigg)\\ &amp;=(\sum_{i=1}^n\sum_{k=1}^n|a_{ik}|^2)(\sum_{k=1}^n\sum_{j=1}^n|b_{kj}|^2)\\&amp;=|A|_F^2|B|_F^2 \end{align</em>}</p>
<script type="math/tex; mode=display">
从而F-范数是矩阵范数

事实上，我们还可以验证</script><p>|A|_F=(tr(A^HA))^{\frac{1}{2}}=(tr(AA^H))^\frac{1}{2}</p>
<script type="math/tex; mode=display">

事实上，在 $n\ge2$ 时，由于 $\|E\|_F=n^\frac{1}{2} $,故 由前面的引理1可知F-范数不是 $\mathbb C^{n\times n} $上的算子范数。但我们有

定理：对 $\forall{A}\in\mathbb C^{n\times n},x\in\mathbb C^n $，有 $\|Ax\|_2\le\|A\|_F\|x\|_2$

证:</script><p>\begin{align<em>} |Ax|_2^2&amp;=\sum_{i=1}^n(\sum_{j=1}^na_{ij}x_j)^2\\ &amp;\le\sum_{i=1}^n(\sum_{j=1}^n|a_{ij}||x_j|)^2\\ &amp;\le(\sum_{i=1}^n\sum_{j=1}^n|a_{ij}|^2)(\sum_{j=1}^n|x_j|^2)\\&amp; =|A|_F^2|x|_2^2 \end{align</em>}</p>
<script type="math/tex; mode=display">
故得证

事实上，我们还可以得到F-范数的相关性质

定理：设 $A\in\mathbb C^{n\times n} , U,V $均为 n 阶酉矩阵，则 $\|A\|_F=\|UA\|_F=\|AV\|_F$

证：此时</script><p>|UA|_F=(tr((UA)^H(UA)))^\frac{1}{2}=(tr(A^HU^HUA))^\frac{1}{2}=(tr(A^HA))^\frac{1}{2}=|A|_F</p>
<script type="math/tex; mode=display">
</script><p>|AV|_F=(tr((AV)(AV)^H))^\frac{1}{2}=(tr(AVV^HA^H))^\frac{1}{2}=(tr(AA^H))^\frac{1}{2}=|A|_F<br>$$</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/24/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/3.%E5%86%85%E7%A7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/24/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/3.%E5%86%85%E7%A7%AF/" class="post-title-link" itemprop="url">内积</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-01-24 19:45:56 / 修改时间：15:16:12" itemprop="dateCreated datePublished" datetime="2022-01-24T19:45:56+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Matrix-Analysis/" itemprop="url" rel="index"><span itemprop="name">Matrix Analysis</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="内积"><a href="#内积" class="headerlink" title="内积"></a>内积</h3><h4 id="内积定义"><a href="#内积定义" class="headerlink" title="内积定义"></a>内积定义</h4><p>设 $\mathbb{V}$ 是 $\mathbb{R}$ 上的一个线性空间，定义映射 $\tau: \mathbb{V}\times\mathbb{V}\rightarrow\mathbb{R}$ 。称映射 $\tau$ 为 $\mathbb{V}$ 上的一个内积，且记 $\tau(v_1,v_2)=<v_1, v_2>$ ，该映射需满足以下三个条件：</p>
<ul>
<li><p>(1) 对称性: $\forall v_1,v_2 \in V,<v_1,v_2> = <v_2,v_1>$</p>
</li>
<li><p>(2) 固定第一个变元，对第二个变元有<code>线性性</code>: $\forall v_1,v_2,v_3 \in V,k,l \in R,<v_1,v_2k+v_3l> = <v_1,v_2>k+<v_1,v_3>l$</p>
</li>
<li><p>(3) 正定性: 对 $\forall v \in V,v ≠ 0 , 〈v,v〉 &gt; 0$</p>
</li>
</ul>
<p>定义了内积的线性空间称为内积空间，有限维的内积空间称为欧几里得空间（有限维内积空间）</p>
<p>内积有如下性质：</p>
<ul>
<li><p>(1) 任意向量与0向量做内积为0，即$\forall v \in V，&lt;\boldsymbol{0},\boldsymbol{v}&gt;=0$</p>
<p>  证明：</p>
<script type="math/tex; mode=display">
  <0,v> = <v,0>=<v,0+0> = <v,0>+<v,0> \Rightarrow  <0,v> = <v,0> = 0\\</script></li>
<li><p>(2) 双线性：固定第二个变元，对第一个变元也满足线性性,即$\forall v_1,v_2,v_3 \in V,k,l \in R,<v_1k+v_2l,v_3> = k<v_1,v_3>+l<v_2,v_3>\$<br>  证明：</p>
<script type="math/tex; mode=display">
  <v_1k+v_2l,v_3> = <v_3,v_1k+v_2l> \\ = <v_3,v_1>k+<v_3,v_2>l\\ =k<v_1,v_3>+l<v_2,v_3></script></li>
</ul>
<h5 id="mathbb-R-n-上的标准内积"><a href="#mathbb-R-n-上的标准内积" class="headerlink" title="$\mathbb{R}^n$ 上的标准内积"></a>$\mathbb{R}^n$ 上的标准内积</h5><p>定义映射 $\tau: \mathbb{R}^n\times\mathbb{R}^n\rightarrow\mathbb{R}$ ，满足 $&lt;\boldsymbol{x},\boldsymbol{y}&gt;=\boldsymbol{x}^T\boldsymbol{y}=x_1y_1+x_2y_2+…+x_ny_n$ ，称 $\tau$ 为 $\mathbb{R}^n$ 上的标准内积</p>
<h5 id="函数空间的内积"><a href="#函数空间的内积" class="headerlink" title="函数空间的内积"></a>函数空间的内积</h5><p>设 $\mathbb{V} = \Zeta(I, R)$是一个函数空间，其中$f \in \zeta([a,b], R^n)，f(t) = \begin{bmatrix}  f_1(t) \\f_2(t) \...\\f_n(t)\end{bmatrix},\\f(t)\text{整体看为一个元素且}f_i(t)\text{为连续函数}t \in [a,b]$。<br>$<f, g> = \int_a^b {f^T(t)g(t)} \,{\rm d}t = \int_a^b {\{f_1(t)g_1(t) + f_2(t)g_2(t) + … + f_n(t)g_n(t)\}}{\rm d}t$</p>
<p><code>注：是内积空间而非欧几里得空间。</code></p>
<h4 id="复内积与酉空间"><a href="#复内积与酉空间" class="headerlink" title="复内积与酉空间"></a>复内积与酉空间</h4><p>设 $\mathbb{V} \text{是复数域}\mathbb{C}$上的线性空间。即$\mathbb{V} \times \mathbb{V}  = \mathbb{C}$。满足以下3个条件称之为复内积，所属空间称之为复内积空间。</p>
<ul>
<li><p>(1) 共轭对称性：$\forall v_1,v_2 \in V,<v_1,v_2> = \overline{<v_2,v_1>}$</p>
</li>
<li><p>(2) 第二变元的线性性：$\forall v_1,v_2,v_3 \in V,k,l \in R,<v_1,v_2k+v_3l> = <v_1,v_2>k+<v_1,v_3>l$</p>
</li>
<li><p>(3) 正定性：$\forall v \in V,v ≠ 0 , 〈v,v〉 &gt; 0$</p>
</li>
</ul>
<p>有限维复内积空间也称之为酉空间。</p>
<ul>
<li><p>性质 ：<br>  相比于欧式空间中的双线性，在酉空间中对第一变元是<code>共轭线性</code>的，第二变元则不是。即<br>  $\forall v_1,v_2,v_3 \in V,k,l \in R,<v_1k+v_2l,v_3> = \overline k<v_1,v_3>+\overline l<v_2,v_3> $</p>
<p>  证明：<br>  $<v_1k+v_2l,v_3> = \overline {<v_3,v_1k+v_2l>}\\ = \overline{<v_3,v_1>k+<v_3,v_2>l}\\ =\overline{<v_3,v_1>}\overline k+\overline {<v_3,v_2l>}\overline l\\ = <v_1l,v_3>\overline k+<v_2l,v_3>\overline l\\ =\overline k <v_1l,v_3>+\overline l<v_2l,v_3>\$</p>
</li>
</ul>
<h5 id="标准酉空间-mathbb-C-n"><a href="#标准酉空间-mathbb-C-n" class="headerlink" title="标准酉空间$\mathbb{C}^n$"></a>标准酉空间$\mathbb{C}^n$</h5><p>$<x, y> = x^Ty = \overline{[x_1,x_2,…,x_n]}\begin{bmatrix}y_1 \\ y_2 \...\\ y_n\end{bmatrix}$</p>
<h5 id="线性组合的内积的矩阵表示"><a href="#线性组合的内积的矩阵表示" class="headerlink" title="线性组合的内积的矩阵表示"></a>线性组合的内积的矩阵表示</h5><p>若$\mathbb{V}$是一个n维欧式空间，$\alpha_1,\alpha_2,…,\alpha_n$是它的一组基，对 $\mathbb{V}$ 中任意向量$\alpha,\beta$,其中</p>
<script type="math/tex; mode=display">
\alpha = x_1\alpha_1+...+x_n\alpha_n\\ \beta = y_1\alpha_1+...+y_n\alpha_n</script><p>利用内积的线性性可以得到</p>
<script type="math/tex; mode=display">
<\alpha,\beta>=X^TAY\\</script><p>其中</p>
<script type="math/tex; mode=display">
A= \begin{bmatrix}     <\alpha_1,\alpha_1> & \cdots & <\alpha_1,\alpha_n>\\     \vdots & \ddots & \vdots\\   <\alpha_n,\alpha_1> & \cdots & <\alpha_n,\alpha_n>\\   \end{bmatrix} ,X= \begin{bmatrix}     x_1 \\     \vdots \\   x_n\\   \end{bmatrix} ,Y= \begin{bmatrix}     y_1 \\     \vdots \\   y_n\\   \end{bmatrix}\\</script><p>若$\mathbb{V}$是一个酉空间，则</p>
<script type="math/tex; mode=display">
<\alpha,\beta>=\overline X^TAY\\</script><p>由此可以得到，<strong><em>内积与矩阵$A$是一一对应的</em></strong>。</p>
<h4 id="向量组的Gram矩阵和度量矩阵"><a href="#向量组的Gram矩阵和度量矩阵" class="headerlink" title="向量组的Gram矩阵和度量矩阵"></a>向量组的Gram矩阵和度量矩阵</h4><p>Gram矩阵定义：n维欧式空间中任意k个向量之间两两的内积所组成的矩阵，称为这k个向量的格拉姆矩阵(Gram matrix)，很明显，这是一个对称矩阵。</p>
<p>设 $\beta_1,\beta_2,…,\beta_s$ 是内积空间的一个向量组，矩阵</p>
<script type="math/tex; mode=display">
G=\begin{bmatrix}     <\beta_1,\beta_1> & \cdots & <\beta_1,\beta_n>\\     \vdots & \ddots & \vdots\\   <\beta_n,\beta_1> & \cdots & <\beta_n,\beta_n>\\   \end{bmatrix}\\</script><p>称为向量组 $\beta_1,\beta_2,…,\beta_s$ 的Gram矩阵</p>
<p>若向量组 $\beta_1,\beta_2,…,\beta_s$是 <strong><em>一组基</em></strong>,则将Gram矩阵称为该基的度量矩阵。<br><code>内积由度量矩阵唯一决定</code></p>
<h6 id="Gram矩阵的性质及证明"><a href="#Gram矩阵的性质及证明" class="headerlink" title="Gram矩阵的性质及证明"></a>Gram矩阵的性质及证明</h6><ul>
<li><p>Hermite性</p>
<ul>
<li><p>酉空间上：$\overline G^T = G$</p>
<p>  等式右边第 $i$ 行 $j$ 列元素为 $&lt;\beta_i,\beta_j&gt;$</p>
<p>  等式左边第 $i$ 行 $j$ 列元素为 $\overline{&lt;\beta_j,\beta_i&gt;}$</p>
<p>  由酉空间内积的性质可得 $&lt;\beta_i,\beta_j&gt; = \overline{&lt;\beta_j,\beta_i&gt;} $ ，因此 $\overline G^T = G$</p>
</li>
<li><p>欧式空间上： $G^T = G$</p>
<p>  同理可证<br>&lt;/br&gt;</p>
</li>
</ul>
</li>
<li><p>非负性</p>
<ul>
<li>酉空间上：$\overline Z^TGZ\geq 0$<script type="math/tex; mode=display">
\overline Z^TGZ = \begin{bmatrix}     \overline z_1,\overline z_2,...,\overline z_s  \end{bmatrix} \begin{bmatrix}     <\beta_1,\beta_1> & \cdots & <\beta_1,\beta_s>\\     \vdots & \ddots & \vdots\\   <\beta_s,\beta_1> & \cdots & <\beta_s,\beta_s>\\   \end{bmatrix}  \begin{bmatrix}     z_1\\ z_2\\ \vdots\\ z_s  \end{bmatrix}\\ = <\beta_1z_1+\beta_2z_2+...+\beta_sz_s,\beta_1z_1+\beta_2z_2+...+\beta_sz_s></script>由内积的正定性得 $\overline Z^TGZ\geq 0$</li>
<li>欧式空间上：$Z^TGZ\geq 0$<br>&lt;/br&gt;</li>
</ul>
</li>
<li><p>$G$ 正定 $\Leftrightarrow \{\beta_i\}$ 线性无关<br>$G$ 正定， $\forall  z \ne0,\beta_1z_1+\beta_2z_2+…+\beta_sz_s,\beta_1z_1+\beta_2z_2+…+\beta_sz_s\ne0$,则 $\beta_1,…,\beta_s$ 线性无关<br>&lt;/br&gt;</p>
</li>
<li><p>$rank(G)=rank(\{\beta_1,…,\beta_s\})$</p>
</li>
</ul>
<p><strong><em>度量矩阵是特殊的Gram矩阵，因此Gram具有的性质Gram矩阵也具有</em></strong></p>
<h6 id="例：几何空间张成内积空间"><a href="#例：几何空间张成内积空间" class="headerlink" title="例：几何空间张成内积空间"></a>例：几何空间张成内积空间</h6><p>$&lt;\alpha, \beta&gt; = ||\alpha||  \cdot ||\beta||cos\theta$</p>
<p>$G(\alpha, \beta) = \begin{bmatrix}&lt;\alpha, \alpha&gt; &amp; &lt;\alpha, \beta&gt; \\ &lt;\beta, \alpha&gt; &amp; &lt;\beta,\beta&gt; \end{bmatrix}<br>$</p>
<p>$||G|| = ||\alpha||^2||\beta||^2 - ||\alpha||^2||\beta||^2cos^2\theta = ||\alpha||^2||\beta||^2sin^2\theta$<br><img src="./images/向量图.png" alt="向量图"><br>即$G\text{的行列式的值就为由}\alpha,\beta$所张成的平行四边形面积</p>
<p><code>同理向量组的G也就是所对应的多边体的体积</code></p>
<h4 id="向量长度和距离的定义"><a href="#向量长度和距离的定义" class="headerlink" title="向量长度和距离的定义"></a>向量长度和距离的定义</h4><ul>
<li>一个向量的长度：$|\alpha|=\sqrt{\langle \alpha,\alpha \rangle}$</li>
<li>两个向量的距离：$d(\alpha,\beta)=|\alpha-\beta|$</li>
</ul>
<h5 id="长度的性质"><a href="#长度的性质" class="headerlink" title="长度的性质"></a>长度的性质</h5><ul>
<li><p>正性<br>$|\alpha|\geq0$ ，当且仅当$\alpha$为0时取等号<br>证明：<br>因为 $\langle \alpha,\alpha \rangle$ 非负，因此 $|\alpha| = \sqrt{\langle \alpha,\alpha \rangle}  \quad\geq 0$</p>
</li>
<li><p>正齐次<br>$|\alpha k|=|k||\alpha|$<br>证明：<br>按定义$|\alpha k| = \sqrt{\langle \alpha k,\alpha k \rangle} =\sqrt{\overline{|k|}\langle \alpha,\alpha \rangle|k|}\\  =|k|\sqrt{\langle \alpha,\alpha \rangle} =|k||\alpha|\\ $<br>证明是在酉空间上，若在欧式空间上也如此，只不过对第一变元是线性的而非共轭线性</p>
</li>
<li><p>平行四边形公式<br>$|\alpha-\beta|^2 + |\alpha+\beta|^2=2(|\alpha|^2+|\beta|^2)$<br>证明：<br>$|\alpha - \beta|^2   = \langle \alpha - \beta,\alpha - \beta \rangle  = \langle \alpha,\alpha \rangle  + \langle \alpha,-\beta \rangle +  \langle -\beta,\alpha \rangle  + \langle -\beta,-\beta \rangle = |\alpha|^2 - \langle \beta,\alpha \rangle   - \langle \alpha,\beta \rangle +|\beta|^2\\   |\alpha+\beta|^2   = \langle \alpha + \beta,\alpha + \beta \rangle  = \langle \alpha,\alpha \rangle  + \langle \alpha,\beta \rangle +  \langle \beta,\alpha \rangle  + \langle \beta,\beta \rangle = |\alpha|^2 + \langle \beta,\alpha \rangle   + \langle \alpha,\beta \rangle +|\beta|^2\\ $<br>所以<br>$|\alpha-\beta|^2 + |\alpha+\beta|^2 = 2\langle \alpha,\alpha \rangle +2\langle \beta,\beta \rangle  = 2|\alpha|^2+2|\beta|^2=2(|\alpha|^2+|\beta|^2)\$<br>注意：这里不要手痒痒将 $\langle \alpha,\beta \rangle, \langle \beta,\alpha \rangle $ 合为 $2\langle \alpha,\beta \rangle$，这个在酉空间是不成立的</p>
</li>
<li><p>Cauchy-schwarz不等式<br>$|\langle \alpha,\beta \rangle| \leq |\alpha| |\beta|$<br>证明：</p>
<ul>
<li>当 $\alpha = 0$ 或 $\beta = 0$ ，显然成立</li>
<li>当 $\alpha,\beta$ 都不为0时<br>在欧式空间中： $&lt;\alpha,\beta&gt;$ 为一实数<br>$|\langle \alpha,\beta \rangle| \leq |\alpha| |\beta| \Leftrightarrow (2\langle \alpha,\beta \rangle)^2 \leq 4|\alpha|^2|\beta|^2 \\<br>\text{记}2\langle \alpha,\beta \rangle = b, |\alpha|=a,|\beta|=c，\text{且}a,b,c &gt; 0\\<br>\text{则}(2\langle \alpha,\beta \rangle )^2 \leq 4|\alpha|^2|\beta|^2  \Leftrightarrow b^2 \leq 4ac\\<br>\text{构造二次函数：}f(t)=at^2+bt+c\\<br>\text{则等价于}f(t)=at^2+bt+c \geq 0，<br>$对$\forall t \in R$成立<br>则<br>$f(t)=at^2+bt+c  = |\alpha|^2t^2+2 \langle \alpha,\beta \rangle t+|\beta|^2\\ = \langle \alpha,\alpha \rangle t^2+2 \langle \alpha,\beta \rangle t+ \langle \beta,\beta \rangle\\ = t \langle \alpha,\alpha \rangle t+ \langle \alpha t,\beta \rangle + \langle \beta,\alpha t \rangle + \langle \beta,\beta \rangle\\ = \langle \alpha t,\alpha t \rangle  + \langle \alpha t,\beta \rangle + \langle \beta,\alpha t \rangle + \langle \beta,\beta \rangle \\ = &lt;\alpha t+ \beta,\alpha t+ \beta&gt;\\ = |\alpha t+ \beta| \geq 0$<br>在酉空间中： $\langle  \alpha,\beta \rangle$ 不一定为实数，但是根据复数的三角表示，$\langle \alpha,\beta \rangle =| \langle \alpha,\beta \rangle|e^{i\theta}  $ ，可以得到 $\langle \alpha ,\beta \rangle e^{-i\theta} = | \langle \alpha,\beta \rangle|  $<br>将 $e^{-i\theta}$ 放到内积中得$\langle \alpha e^{i\theta},\beta \rangle = |\langle \alpha ,\beta \rangle| $ 为一实数<br>因此$|\langle \alpha e^{i\theta},\beta \rangle|  \leq |\langle \alpha e^{i\theta}\rangle| |\beta| $<br>不等式左边： $|\langle \alpha e^{i\theta},\beta \rangle|   = |\langle \alpha ,\beta \rangle e^{-i\theta}|  = | \langle \alpha,\beta \rangle|   $<br>不等式右边：$|\langle \alpha e^{i\theta}\rangle| |\beta|  = \sqrt{\langle \alpha e^{i\theta},\alpha e^{i\theta}\rangle} =  \sqrt{e^{-i\theta} \langle \alpha ,\alpha \rangle e^{i\theta}}|\beta|  =  \sqrt{ \langle \alpha ,\alpha \rangle }|\beta| =  |\alpha| |\beta|\$<br>因此得到：$|\langle \alpha,\beta \rangle| \leq |\alpha| |\beta|$</li>
</ul>
</li>
<li><p>三角不等式<br>$|\alpha \pm \beta| \leq |\alpha| +|\beta|$<br>证明加号的情况，减号的情况下只需要将 $\alpha -\beta$ 看成是 $\alpha + (-\beta)$ 即可<br>证明：<br>$|\alpha +\beta| \leq |\alpha| +|\beta|  \Leftrightarrow |\alpha + \beta|^2 \leq (|\alpha| +|\beta|)^2\\ \Leftrightarrow \langle  \alpha,\alpha \rangle + \langle  \alpha,\beta \rangle + \langle  \beta,\alpha \rangle + \langle  \beta,\beta \rangle \leq |\alpha|^2+|\beta|^2+2|\alpha||\beta|\\ \Leftrightarrow |\alpha|^2 + \langle  \alpha,\beta \rangle + \langle  \beta,\alpha \rangle + |\beta|^2  \leq |\alpha|^2+|\beta|^2+2|\alpha||\beta|\\ \Leftrightarrow \langle  \alpha,\beta \rangle + \langle  \beta,\alpha \rangle  \leq 2|\alpha||\beta|$<br>又因为$\langle  \alpha,\beta \rangle + \langle  \beta,\alpha \rangle  = \langle  \alpha,\beta \rangle +  \overline{\langle  \alpha,\beta \rangle} = 2Re\langle  \alpha,\beta \rangle$为一个实数。<br>实数一定满足小于等于它的绝对值的性质，即<br>$\langle  \alpha,\beta \rangle + \langle  \beta,\alpha \rangle \leq |\langle  \alpha,\beta \rangle + \langle  \beta,\alpha \rangle|  \leq |\langle  \alpha,\beta \rangle | + |\langle  \beta,\alpha \rangle|$<br>在利用Cauchy-schwarz不等式<br>$|\langle  \alpha,\beta \rangle | + |\langle  \beta,\alpha \rangle|   \leq |\alpha||\beta|+|\beta||\alpha| = 2|\alpha||\beta|\$<br>所以三角不等式成立</p>
<h5 id="距离的性质"><a href="#距离的性质" class="headerlink" title="距离的性质"></a>距离的性质</h5></li>
<li>距离的三角不等式<br>两边距离之和大于第三边。<br>$d(\alpha,\beta) \leq d(\alpha,\gamma) +d(\gamma,\beta)$<br>证明：<br>由三角不等式很容易得到<br>$|\alpha-\beta|=|\alpha -\gamma + \gamma - \beta| \leq  |\alpha -\gamma |+ |\gamma - \beta|$<br>按照距离的定义可以得到<br>$d(\alpha,\beta) \leq d(\alpha,\gamma) +d(\gamma,\beta)$</li>
</ul>
<h4 id="夹角的定义"><a href="#夹角的定义" class="headerlink" title="夹角的定义"></a>夹角的定义</h4><p>假设 $\theta$ 是向量 $\alpha$ 和 $\beta$ 的夹角，则 $\theta = arc cos \frac{\langle  \alpha,\beta \rangle}{|\alpha||\beta|}$<br>注意：<strong><em>夹角的定义在欧式空间中才有</em></strong>，因为在酉空间中 $\langle  \alpha,\beta \rangle$ 可能是负数。由Cauchy-schwarz不等式可得： $|\langle  \alpha,\beta \rangle| \leq|\alpha||\beta|$ ，因此可以保证 $\frac{|\langle  \alpha,\beta \rangle|}{|\alpha||\beta|} \leq 1$ ，所以可以保证 $\theta$ 是一定存在的</p>
<p>并且通过夹角公式可以反过来计算内积： $\langle  \alpha,\beta \rangle = |\alpha||\beta|cos\theta $</p>
<h5 id="正交【垂直】的定义"><a href="#正交【垂直】的定义" class="headerlink" title="正交【垂直】的定义"></a>正交【垂直】的定义</h5><p>任意空间【可以是欧式空间，也可以是酉空间】中的向量$\alpha,\beta$，如果垂直即 $\alpha\bot\beta$ ，则满足</p>
<script type="math/tex; mode=display">\langle  \alpha,\beta \rangle = 0</script><p>如果是实空间中且$\alpha,\beta$都不为0，垂直可以得到 $\alpha,\beta$ 之间的夹角 $\theta = \frac{\pi}{2}$</p>
<h5 id="勾股定理"><a href="#勾股定理" class="headerlink" title="勾股定理"></a>勾股定理</h5><p><strong><em>欧式空间中</em></strong>，下列等价</p>
<ul>
<li>(1)$\alpha \perp  \beta$</li>
<li>(2)$\theta = \frac{\pi}{2}$</li>
<li>(3)$\langle  \alpha,\beta \rangle = 0$</li>
<li>(4)$|\alpha+\beta|^2=|\alpha|^2+|\beta|^2$</li>
<li>(5)$|\alpha-\beta|^2=|\alpha|^2+|\beta|^2$</li>
</ul>
<p>酉空间中，<br>(1) $\Leftrightarrow$ (3)<br>(4) $\Leftrightarrow$ (5)<br>(1) $\Rightarrow$ (4)</p>
<h4 id="投影定理"><a href="#投影定理" class="headerlink" title="投影定理"></a>投影定理</h4><h5 id="投影定理的背景"><a href="#投影定理的背景" class="headerlink" title="投影定理的背景"></a>投影定理的背景</h5><p>问题： $V$ 是 $\mathbb{C}$ 上的内积空间， $\beta \in V, W$  是 $V$ 的有限维子空间，在 $W$ 中求一个向量 $\alpha$离$\beta$ 的距离最近，即求</p>
<script type="math/tex; mode=display">min \{d(\beta,\alpha)|\alpha \in W\}</script><p>设 $W$ 的一组基， $\beta_1,\beta_2,…,\beta_s$ ，则 $\alpha$ 可以表示为 $\alpha = \beta_1 k_1 + …+\beta_sk_s$</p>
<p>根据距离公式</p>
<script type="math/tex; mode=display">
d(\beta,\alpha) = d(\beta,\sum_{j=1}^{s}{\beta_jk_j})=\|\beta-\sum_{j=1}^{s}{\beta_jk_j}\|\\ d(\beta,\sum_{j=1}^{s}{\beta_jk_j})^2 =\langle \beta-\sum_{j=1}^{s}{\beta_jk_j},\beta-\sum_{j=1}^{s}{\beta_jk_j} \rangle\\</script><p>可以将 $\langle \beta-\sum_{j=1}^{s}{\beta_jk_j},\beta-\sum_{j=1}^{s}{\beta_jk_j} \rangle$ <strong><em>看成 $s$个变元的多元函数</em></strong>，按照数学分析中多元函数求极值的问题去解<br>假设考虑在实数域上，则</p>
<script type="math/tex; mode=display">
\langle \beta-\sum_{j=1}^{s}{\beta_jk_j},\beta-\sum_{j=1}^{s}{\beta_jk_j} \rangle = \|\beta\|^2-2\langle \beta,\sum_{j=1}^{s}{\beta_jk_j} \rangle+ \langle \sum_{j=1}^{s}{\beta_jk_j},\sum_{j=1}^{s}{\beta_jk_j} \rangle\\ = \|\beta\|^2-2\sum_{j=1}^{s}\langle \beta,\beta_j \rangle k_j+ \begin{bmatrix} k_1\cdots  k_s   \end{bmatrix}G(\{\beta_i\})\begin{bmatrix} k_1\\ \vdots\\ k_s   \end{bmatrix}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{bmatrix} k_1\cdots  k_s   \end{bmatrix}G(\{\beta_i\})\begin{bmatrix} k_1\\ \vdots\\ k_s   \end{bmatrix}\\</script><p>为二次型，因此可以将 $\langle \beta-\sum_{j=1}^{s}{\beta_jk_j},\beta-\sum_{j=1}^{s}{\beta_jk_j} \rangle$ 看成是$s$ 元二次多项式</p>
<p>利用多元函数求极值的方法，令各个变量的偏导为0。当然在这里我们会考虑使用代数的方式去解决这个问题，但是这个处理的思想还是很重要的，将其看成$s$ 元二次多项式求最小值的问题就转化为了原本熟悉的问题了</p>
<h5 id="投影定理及证明"><a href="#投影定理及证明" class="headerlink" title="投影定理及证明"></a>投影定理及证明</h5><p>设 $V$ 是 $\mathbb{C}$ 上的内积空间【如果是 $\mathbb{R}$ 上的也成立】， $\beta \in V ， W$ 是 $V$ 的有限维子空间，则 <strong><em>存在唯一</em></strong> 的 $\alpha \in W$ ，使得</p>
<script type="math/tex; mode=display">d(\beta,\alpha)\leq d(\beta,w),\forall w \in W</script><p>其中$\alpha$ 可以被记为</p>
<script type="math/tex; mode=display">\alpha = arg\ min\ d(\beta,w),w \in W</script><p>其中$arg$ 表示的含义为使 $d(\beta,w)$ 最小时自变量的取值<br>设 $\beta_1,…,\beta_s$ 是 $W$ 中的一个基，则</p>
<script type="math/tex; mode=display">\alpha = \beta_1 k_1 + ...+\beta_sk_s=\left[ \beta_1,...,\beta_s \right]k</script><p>其中</p>
<script type="math/tex; mode=display">
k=\begin{bmatrix} k_1\\ \vdots\\ k_s   \end{bmatrix}</script><p>则</p>
<script type="math/tex; mode=display">
k = G({\beta_i})^{-1}G({\beta_i},\beta)</script><p>$\alpha =\left[ \beta_1,…,\beta_s \right]k$,使得$\alpha$到$\beta$的距离最近<br>证明：</p>
<ul>
<li><p>投影距离最近的证明<br>  <img src="./images/投影证明.jpg" alt="投影证明"><br>  设 $\vec{AB}=\beta,\vec{AC}=\alpha$ ，则</p>
<script type="math/tex; mode=display">d(\alpha,\beta)=\|\vec{BC}\|\\</script><p>  其中 $\alpha$ 是$\beta$ 在平面的投影， $BC$ 是平面的法线，则 $BC$ 垂直该平面中的所有向量</p>
<p>  任意在平面中找一向量 $\vec{AD}$ ，则</p>
<script type="math/tex; mode=display">d(\vec{AD},\beta)=\|\vec{BD}\|</script><p>  由勾股定理</p>
<script type="math/tex; mode=display">\|\vec{CD}\|^2+\|\vec{BC}\|^2= \|\vec{BD}\|^2</script><p>  所以投影距离最短</p>
</li>
<li><p>存在性的证明<br>即正面平面法线的存在，设平面的一组基 $\beta_1,\beta_2,…,\beta_s$ ， $\vec{BC} = \beta - \alpha$ 垂直平面的充要条件是</p>
<script type="math/tex; mode=display">(\beta-\alpha)\bot \beta_j,j=1,2,...,s</script><p>即</p>
<script type="math/tex; mode=display">
\langle \beta_j,(\beta-\alpha) \rangle ,j=1,2,...,s\\ \Leftrightarrow \langle \beta_j,\beta \rangle - \langle \beta_j,\alpha \rangle  = 0</script></li>
</ul>
<p>$\alpha$可由基向量线性表出，即 $\alpha=\sum_{i=1}^{s}{\beta_ik_i}$ ，则</p>
<script type="math/tex; mode=display">
\langle \beta_j,\beta \rangle    =  \langle \beta_j,\sum_{i=1}^{s}{\beta_ik_i} \rangle =\sum_{i=1}^{s} \langle \beta_j,{\beta_i} \rangle k_i ,\quad j=1,2,...,s</script><p>将其写成矩阵的表达形式</p>
<script type="math/tex; mode=display">
\begin{bmatrix}   \langle \beta_1,\beta_1 \rangle & \cdots &  \langle \beta_1,\beta_s \rangle \\       \vdots & &\vdots \\      \langle \beta_s,\beta_1 \rangle & \cdots &  \langle \beta_s,\beta_s \rangle    \end{bmatrix}  \begin{bmatrix} k_1\\ \vdots\\ k_s   \end{bmatrix}  = \begin{bmatrix}   \langle \beta_1,\beta \rangle\\       \vdots\\      \langle \beta_s,\beta\rangle    \end{bmatrix}</script><p>可以得到</p>
<script type="math/tex; mode=display">
k = G({\beta_i})^{-1}G({\beta_i},\beta)</script><ul>
<li>唯一性的证明<br>假设存在 $AD=\widetilde\alpha$ 到 $\beta$ 的距离最短，则<script type="math/tex; mode=display">
\|\vec{BC}\|= \|\vec{BD}\|</script>与勾股定理<script type="math/tex; mode=display">\|\vec{CD}\|^2+\|\vec{BC}\|^2= \|\vec{BD}\|^2, \quad \|\vec{CD}\|>0</script>矛盾</li>
</ul>
<p>所以 $\alpha$ 唯一</p>
<h4 id="投影定理的应用"><a href="#投影定理的应用" class="headerlink" title="投影定理的应用"></a>投影定理的应用</h4><h5 id="最小二乘拟合问题"><a href="#最小二乘拟合问题" class="headerlink" title="最小二乘拟合问题"></a>最小二乘拟合问题</h5><p>有一组观测数据<br><img src="./images/观测数据.jpg" alt="观测数据"><br>在已有库中找一个 $k$ 次函数</p>
<p>$y = f(x)=a_0+a_1x+…+a_kx^k$ 使得$f(a_0,a_1,…,a_k)=\sum_{i=1}^{n}{\left| f(x_i)-y_i \right|}^2$ 取值最小 <strong><em>最小二乘拟合准则</em></strong></p>
<p>求最小的问题因为 $f$ 是$k+1$元函数，可以令 $\frac{\partial f}{\partial a_i} = 0$ 求解 $a_i$ 的值</p>
<p>但是这里将其转换为投影定理的应用来解，就需要对问题进行抽象化</p>
<p>令 </p>
<script type="math/tex; mode=display">\beta=\begin{bmatrix}      y_1 \\     \vdots \\    y_n\end{bmatrix}, \beta_i = \begin{bmatrix}      x_1^i \\      \vdots \\    x_n^i\\   \end{bmatrix} ,i =0,1,...,k</script><p>则求 $f$ 的最小值等价于求 $|\beta-\sum_{i=0}^{n}{\beta_ia_i}|^2 $的最小值，而 <strong><em>求 $|\beta-\sum_{i=0}^{n}{\beta_ia_i}|^2$ 的最小值问题可以理解为在 $R^n$ 中将 $\beta$ 向 $W=span\{\beta_0,\beta_1,…,\beta_k\}$ 子空间的投影</em></strong></p>
<p>证明：<br>因为 </p>
<script type="math/tex; mode=display">
\sum_{i=1}^{n}{\left| f(x_i)-y_i \right|}^2  =  \sum_{i=1}^{n}{\left| \sum_{j=1}^{k}{x_i^ja_j}-y_i \right|}^2\\ = \|  \begin{bmatrix}      \sum_{j=0}^{k}{x_1^ja_j}-y_1 \\      \vdots \\    \sum_{j=0}^{k}{x_n^ja_j}-y_n    \end{bmatrix} \|^2\\ =  \|  \begin{bmatrix}      x_1^0 \\      \vdots \\    x_n^0    \end{bmatrix}a_0 + ... +  \begin{bmatrix}      x_1^k \\      \vdots \\    x_n^k    \end{bmatrix}a_k -   \begin{bmatrix}      y_1\\      \vdots \\    y_n   \end{bmatrix} \|^2\\ = \|\beta-\sum_{i=0}^{n}{\beta_ia_i}\|^2</script><p>说明第二个等式可以过去的原因：</p>
<script type="math/tex; mode=display">
c_1^2+c_2^2+...+c_{100}^2= \left[ c_1,...,c_{100} \right] \begin{bmatrix}      c_1 \\      \vdots \\    c_{100}    \end{bmatrix} = \|c\|^2</script><p>其中</p>
<script type="math/tex; mode=display">
c=\begin{bmatrix}      c_1 \\      \vdots \\    c_{100}    \end{bmatrix}</script><p>即等于列向量所有元素长度的平方</p>
<p>因此最小二乘拟合问题的几何实质是：将因变量的观测值拼成的一个 n 维列向量向自变量的观测值构成的 k+1 个向量作为一组基的子空间的投影，该组基为</p>
<script type="math/tex; mode=display">
\begin{bmatrix}       1 \\       \vdots \\     x_1  \end{bmatrix}, \begin{bmatrix}       x_1 \\       \vdots \\     x_n  \end{bmatrix},..., \begin{bmatrix}       x_1^k \\       \vdots \\     x_n^k  \end{bmatrix}</script><h5 id="投影矩阵"><a href="#投影矩阵" class="headerlink" title="投影矩阵"></a>投影矩阵</h5><p>这里通过一道例子引用投影矩阵的概念</p>
<p>空间$C^n$上的向量 $\beta ， A \in C^{n \times s} $并且列满秩， 求$\beta$ 往子空间$ imA $的投影 $\alpha$</p>
<p>设子空间 $imA$ 的一组基为 $\beta_1,\beta_2,…,\beta_s$ ，套用投影定理的公式</p>
<script type="math/tex; mode=display">
\alpha=\left[ \beta_1,\beta_2,...,\beta_s \right]k\\其中k = G({\beta_i})^{-1}G({\beta_i},\beta)</script><p>因此</p>
<script type="math/tex; mode=display">
P_A(\beta)=Ak=AG(A)^{-1}G(A,\beta) = A(\overline A^TA)^{-1}\overline A^T\beta = \left[  A(\overline A^TA)^{-1}\overline A^T \right]\beta\\ 记P_A=A(\overline A^TA)^{-1}\overline A^T</script><p>则$P_A$ 即为投影矩阵</p>
<p>$P_A(\beta) $符号表示的含义为 $\beta $在 $A$上的投影</p>
<p>注：考虑的标准内积空间，即</p>
<script type="math/tex; mode=display">
 \langle \beta_i,\beta_j \rangle=  \overline \beta_i^T \beta_j</script><h6 id="投影矩阵的性质"><a href="#投影矩阵的性质" class="headerlink" title="投影矩阵的性质"></a>投影矩阵的性质</h6><ul>
<li>$\overline {P_A}^T= P_A$</li>
<li>$P_A ^2 = P_A$</li>
<li>$rank(P_A)=rank(A)$</li>
</ul>
<p>若为实数域上的投影矩阵，则第一条性质中投影矩阵不需要取共轭</p>
<h3 id="标准正交基、Gram-Schmidt-正交化及正交矩阵和酉矩阵"><a href="#标准正交基、Gram-Schmidt-正交化及正交矩阵和酉矩阵" class="headerlink" title="标准正交基、Gram-Schmidt 正交化及正交矩阵和酉矩阵"></a>标准正交基、Gram-Schmidt 正交化及正交矩阵和酉矩阵</h3><h4 id="正交组、标准正交组、标准正交基的定义"><a href="#正交组、标准正交组、标准正交基的定义" class="headerlink" title="正交组、标准正交组、标准正交基的定义"></a>正交组、标准正交组、标准正交基的定义</h4><p>设 V 是 C 上的内积空间</p>
<ul>
<li>正交向量组【正交组】：$V$ 中若干个非零向量 $\{\alpha_i\}$ 构成的向量组称为正交向量组, 若组中任意两个向量正交，即  $\langle \alpha_i,\alpha_j\rangle=0,i=1,2,…,s$</li>
<li>标准正交组：正交向量组称为标准正交组, 若其中每个向量的长度为 1，即 $|\alpha_i|=1,i=1,2,…,s$</li>
<li>标准正交基：标准正交组若还是$V$ 的一个基, 则称为标准正交基<h6 id="标准正交组构成的gram矩阵的性质"><a href="#标准正交组构成的gram矩阵的性质" class="headerlink" title="标准正交组构成的gram矩阵的性质"></a>标准正交组构成的gram矩阵的性质</h6>因为<script type="math/tex; mode=display">
\langle \alpha_i,\alpha_j \rangle = \delta_{ij} =\begin{cases} 1 ,i = j\\ 0  ,i \ne j \end{cases}</script>所以<script type="math/tex; mode=display">
G(\{\alpha_i\}) = \begin{bmatrix}    \langle \alpha_1,\alpha_1 \rangle & \cdots &  \langle \alpha_1,\alpha_s \rangle \\        \vdots & &\vdots \\       \langle \alpha_s,\alpha_1 \rangle & \cdots &  \langle \alpha_s,\alpha_s \rangle    \end{bmatrix}  =E</script><h6 id="标准正交组的性质"><a href="#标准正交组的性质" class="headerlink" title="标准正交组的性质"></a>标准正交组的性质</h6>标准正交组一定是线性无关的<br>证明：<script type="math/tex; mode=display">
\alpha_1k_1+\alpha_2k_2+...+\alpha_sk_s=0,k_i=0,i=1,2,...,s</script>$\alpha_i$ 分别于等式两边做内积<script type="math/tex; mode=display">
\langle \alpha_i,\alpha_1k_1+\alpha_2k_2+...+\alpha_sk_s\rangle  =  \langle \alpha_i,0\rangle</script>利用内积的性质<script type="math/tex; mode=display">
\langle \alpha_i,\alpha_1\rangle k_1+...+ \langle \alpha_i,\alpha_i\rangle k_i+  ...+\langle \alpha_i,\alpha_s\rangle k_s = 0</script>可以得到<script type="math/tex; mode=display">
\langle \alpha_i,\alpha_i\rangle k_i = 0\\ 又因为 \langle \alpha_i,\alpha_i\rangle = 1 ，</script>因此得到<script type="math/tex; mode=display">
k_i=0,i=1,2,...,s</script>由证明过程可以看出，标准正交组具有 <strong><em>解耦性</em></strong>，两边用 $\alpha_i$做内积，可以得到系数 $k_i$ 的值</li>
</ul>
<h6 id="标准正交组的好处"><a href="#标准正交组的好处" class="headerlink" title="标准正交组的好处"></a>标准正交组的好处</h6><p><strong><em>标准正交组具有解耦性！</em></strong>用一道例子强调它解耦性的作用</p>
<p>实数域上的内积空间 $V=\mathscr L^2{(\left[ 0,2\pi \right])}$ ，即所有定义域在 $\left[ 0,2\pi \right]$ 上的平方可积函数的全体， $f,g\in V $，其内积满足</p>
<script type="math/tex; mode=display">
\langle f,g \rangle = \int_{0}^{2\pi}f(t)g(t)dt</script><p>$V$上的一组标准正交组</p>
<script type="math/tex; mode=display">
\{\frac{1}{2\pi}, \frac{1}{\sqrt{\pi}}\sin nx, \frac{1}{\sqrt{\pi}}\cos nx\},\quad n=1,2,...,N</script><p>很容易验证这组向量满足正交性和标准性即 <strong><em>模长为1</em></strong></p>
<p>将任意函数 f(x) 展开成傅里叶级数的形式，即</p>
<script type="math/tex; mode=display">
f(x) \sim c_0\frac{1}{\sqrt{2 \pi}}+\sum_{k=1}^{N}{a_k\frac{1}{\sqrt{\pi}}\sin kx} + \sum_{k=1}^{N}{b_k\frac{1}{\sqrt{\pi}}\cos kx}</script><p>分别用标准正交组中的向量与 $f(x)$ 做内积，可以得到</p>
<script type="math/tex; mode=display">
c_0=\frac{1}{\sqrt{2 \pi}}\int_{0}^{2\pi}f(x)dx\\ a_n=\frac{1}{\sqrt{\pi}}\int_{0}^{2\pi}f(x)\sin nxdx \\ b_n=\frac{1}{\sqrt{\pi}}\int_{0}^{2\pi}f(x)\cos nxdx</script><p>其中 $n=1,2,…,N$</p>
<p>因此利用标准正交组的解耦性很容易求出系数</p>
<h5 id="Gram-Schmidt-正交化"><a href="#Gram-Schmidt-正交化" class="headerlink" title="Gram-Schmidt 正交化"></a>Gram-Schmidt 正交化</h5><p>设一组基 $\alpha_1,\alpha_2,…,\alpha_s$，构造正交基$ \beta_1,\beta_2,…,\beta_s$</p>
<ul>
<li>正交化<br>令 $\begin{cases} \beta_1=\alpha_1\\ \beta_2= \alpha_2-k_{21}\beta_1\\ \vdots\\ \beta_s= \alpha_s- k_{s1}\beta_1…-k_{s,s-1}\beta_{s-1} \end{cases}$<br>满足<br>$\langle \beta_i,\beta_j \rangle = \delta_{ij}=\begin{cases} 1 ,i = j\\ 0  ,i \ne j \end{cases}$<br>因此可以得到<script type="math/tex; mode=display">
k_{21}=\frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle},..., k_{s1}=\frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle},..., k_{ss}=\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}</script>即<script type="math/tex; mode=display">
\begin{cases} \beta_1=\alpha_1\\ \beta_2= \alpha_2-\frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\beta_1\\ \vdots\\ \beta_s= \alpha_s- \frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\beta_1-...-\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}\beta_{s-1} \end{cases}</script></li>
<li>单位化<br>$\widetilde \beta_1=\frac{\beta_1}{\ | \beta_1|},…,\widetilde \beta_s=\frac{\beta_s}{\ | \beta_s|}$</li>
</ul>
<p>由这个正交化过程可以得出：<strong><em>标准正交基是一定存在的</em></strong></p>
<h5 id="正交矩阵、酉矩阵"><a href="#正交矩阵、酉矩阵" class="headerlink" title="正交矩阵、酉矩阵"></a>正交矩阵、酉矩阵</h5><p>正交矩阵： $A \in R^{n \times n}$ 称为正交矩阵，若满足 $A^TA=E$<br>酉矩阵： $A \in C^{n \times n}$ 称为酉矩阵，若满足 $\overline A^TA=E$</p>
<h6 id="正交矩阵、酉矩阵列向量具有的性质"><a href="#正交矩阵、酉矩阵列向量具有的性质" class="headerlink" title="正交矩阵、酉矩阵列向量具有的性质"></a>正交矩阵、酉矩阵列向量具有的性质</h6><p>设$A=\left[ a_1,a_2,…,a_n \right]$ 在标准欧式空间中，即内积满足$ \langle x,y \rangle =x^T y$ ，正交矩阵的列向量组是一组标准正交基</p>
<script type="math/tex; mode=display">
(A^TA)_{ij}=a_i^Ta_j=\langle \alpha_i,\alpha_j \rangle =\delta_{ij}=\begin{cases} 1 ,i = j\\ 0  ,i \ne j \end{cases}</script><p>因此 <strong><em>正交矩阵的列向量组是一组标准正交基</em></strong></p>
<p>在标准酉空间中，即内积满足$ \langle x,y \rangle =\overline x^T y $，酉矩阵的列向量组是一组标准正交基</p>
<script type="math/tex; mode=display">
(\overline A^TA)_{ij}=\overline a_i^Ta_j=\langle \alpha_i,\alpha_j \rangle =\delta_{ij}=\begin{cases} 1 ,i = j\\ 0  ,i \ne j \end{cases}</script><p>因此 <strong><em>酉矩阵的列向量组也是一组标准正交基</em></strong></p>
<p>正交矩阵作为线性变换，以下等价</p>
<ul>
<li>$U$ 是正交矩阵</li>
<li>$U$ 作为线性变换保内积，即 $\langle Ux,Uy\rangle = \langle x,y\rangle$</li>
<li>U 作为线性变换保长度，即 $|Ux|=|x|$</li>
</ul>
<p>$1 \Rightarrow  2$</p>
<p>已知 $U^TU=E$</p>
<p>因此</p>
<script type="math/tex; mode=display">\langle Ux,Uy\rangle  =\overline{ (Ux)} ^T(Uy)= x^TU^TUy=x^Ty=<x,y></script><p>$2\Rightarrow1$</p>
<p>已知</p>
<script type="math/tex; mode=display">
\langle Ux,Uy\rangle  =\overline{ (Ux)} ^T(Uy)= x^TU^TUy=x^Ty=<x,y></script><p>因此<script type="math/tex">U^TU=E</script><br>即$U$是正交矩阵</p>
<p>$2 \Rightarrow 3$ 令 $y=x$</p>
<script type="math/tex; mode=display">
\|Ux\|^2=\langle Ux,Ux\rangle = \langle x,x\rangle=\|x\|^2 \Rightarrow \|Ux\|=\|x\||</script><p>$3 \Rightarrow 2$<br>已知 $|Ux|=|x|$ ，等价 $\langle Ux,Ux\rangle = \langle x,x\rangle$</p>
<p>则</p>
<script type="math/tex; mode=display">
\langle U(x+y),U(x+y)\rangle = \langle x+y,x+y\rangle</script><p>等式两边展开</p>
<script type="math/tex; mode=display">
\langle Ux,Ux\rangle +\langle Uy,Uy\rangle +\langle Ux,Uy\rangle +\langle Uy,Ux\rangle\\  \langle Ux,Ux\rangle +\langle Uy,Uy\rangle +2\langle Ux,Uy\rangle\\ = \langle x,x\rangle +\langle y,y\rangle +\langle x,y\rangle +\langle y,x\rangle\\ = \langle x,x\rangle +\langle y,y\rangle +2\langle x,y\rangle</script><p>因此</p>
<script type="math/tex; mode=display">
\langle Ux,Uy\rangle = \langle x,y\rangle</script><p>酉矩阵 U 作为线性变换，以下等价</p>
<ul>
<li>U 是酉矩阵</li>
<li>U 作为线性变换保内积，即 $\langle Ux,Uy\rangle = \langle x,y\rangle$</li>
<li>U 作为线性变换保长度，即 $|Ux|=|x|$<br>证明 $3 \Rightarrow 2 $，其他证明思路同上</li>
</ul>
<h3 id="基于Gram-Schmidt-正交化及QR分解"><a href="#基于Gram-Schmidt-正交化及QR分解" class="headerlink" title="基于Gram-Schmidt 正交化及QR分解"></a>基于Gram-Schmidt 正交化及QR分解</h3><h4 id="Gram-Schmidt-正交化变形"><a href="#Gram-Schmidt-正交化变形" class="headerlink" title="Gram-Schmidt 正交化变形"></a>Gram-Schmidt 正交化变形</h4><p>上一节已经得到任意一组基 $\alpha_1,\alpha_2,…,\alpha_s $，可以构造正交基 $\beta_1,\beta_2,…,\beta_s$</p>
<script type="math/tex; mode=display">
\begin{cases} \beta_1=\alpha_1\\ \beta_2= \alpha_2-\frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\beta_1\\ \vdots\\ \beta_s= \alpha_s- \frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\beta_1...-\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}\beta_{s-1} \end{cases}</script><p>将 $\alpha_i$ 移到左边得到</p>
<script type="math/tex; mode=display">
\begin{cases}  \alpha_1 = \beta_1\\  \alpha_2 = \beta_2 + \frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\beta_1\\  \vdots\\  \alpha_s = \beta_s + \frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\beta_1+...+\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}\beta_{s-1} \end{cases}</script><p>可以得到矩阵乘积的形式</p>
<script type="math/tex; mode=display">
\begin{bmatrix} \alpha_1 & \alpha_2 & \cdots & \alpha_s  \end{bmatrix} = \begin{bmatrix} \beta_1 & \beta_2 & \cdots & \beta_s  \end{bmatrix} \begin{bmatrix}  1 &  \frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle} & \cdots & \frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\\ &1&&\vdots\\ &&\ddots&\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}\\ &&&1 \end{bmatrix}</script><p>对  $\beta_, \beta_2,  \cdots  ,\beta_s $  进行单位化</p>
<script type="math/tex; mode=display">
\widetilde \beta_1=\frac{\beta_1}{\ \| \beta_1\|},...,\widetilde \beta_s=\frac{\beta_s}{\ \| \beta_s\|}</script><p>则 </p>
<script type="math/tex; mode=display">\begin{bmatrix} \beta_1 & \beta_2 & \cdots & \beta_s  \end{bmatrix} =  \begin{bmatrix} \widetilde \beta_1 & \widetilde \beta_2 & \cdots & \widetilde \beta_s  \end{bmatrix} \begin{bmatrix}  \|\beta_1\|\\ &\ddots\\ &&\|\beta_s\| \end{bmatrix}</script><p>所以</p>
<script type="math/tex; mode=display">
\begin{bmatrix} \alpha_1 & \alpha_2 & \cdots & \alpha_s  \end{bmatrix} =  \begin{bmatrix} \widetilde \beta_1 & \widetilde \beta_2 & \cdots & \widetilde \beta_s  \end{bmatrix} \begin{bmatrix}  \|\beta_1\|\\ &\ddots\\ &&\|\beta_s\| \end{bmatrix} \begin{bmatrix}  1 &  \frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle} & \cdots & \frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\\ &1&&\vdots\\ &&\ddots&\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}\\ &&&1 \end{bmatrix}\\</script><p>即</p>
<script type="math/tex; mode=display">\begin{bmatrix} \alpha_1 & \alpha_2 & \cdots & \alpha_s  \end{bmatrix} =  \begin{bmatrix} \widetilde \beta_1 & \widetilde \beta_2 & \cdots & \widetilde \beta_s  \end{bmatrix} \begin{bmatrix}  \|\beta_1\| &  \|\beta_1\|\frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle} & \cdots & \|\beta_1\| \frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\\ &\|\beta_2\|&&\vdots\\ &&\ddots&\|\beta_{s-1}\|\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}\\ &&&\|\beta_s\| \end{bmatrix}</script><p>记 </p>
<script type="math/tex; mode=display">
A=\begin{bmatrix} \alpha_1 & \alpha_2 & \cdots & \alpha_s  \end{bmatrix}, Q=\begin{bmatrix} \widetilde \beta_1 & \widetilde \beta_2 & \cdots & \widetilde \beta_s  \end{bmatrix}, R=\begin{bmatrix}  \|\beta_1\| &  \|\beta_1\|\frac{\langle \alpha_2,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle} & \cdots & \|\beta_1\| \frac{\langle \alpha_s,\beta_1 \rangle}{\langle \beta_1,\beta_1 \rangle}\\ &\|\beta_2\|&&\vdots\\ &&\ddots&\|\beta_{s-1}\|\frac{\langle \alpha_s,\beta_{s-1} \rangle}{\langle \beta_{s-1},\beta_{s-1} \rangle}\\ &&&\|\beta_s\| \end{bmatrix}</script><p>即</p>
<script type="math/tex; mode=display">A=QR</script><p>其中 $A$ 为列满秩矩阵，$Q$ 为标准正交矩阵， $R$ 为主对角线元素都大于0的上三角矩阵</p>
<h4 id="QR分解定理"><a href="#QR分解定理" class="headerlink" title="QR分解定理"></a>QR分解定理</h4><p>任意一个非奇异矩阵 $A$ 都可以分解为一个正交矩阵 $Q$ 和一个主对角元素都为正数的上三角矩阵 $R$ 的乘积，并且分解是唯一的</p>
<p>下面证明矩阵分解的唯一性：</p>
<p>假设分解方式不唯一，即存在 $Q_1,Q_2$ 和 $R_1,R_2$ 成立</p>
<script type="math/tex; mode=display">A=Q_1R_1=Q_2R_2</script><p>则</p>
<script type="math/tex; mode=display">
Q_2^{-1}Q_1=R_1R_2^{-1}</script><p>因为$ Q_1,Q_2$ 为标准正交矩阵，因此</p>
<script type="math/tex; mode=display">
Q_2^{-1}Q_1=R_1R_2^{-1}=E</script><p>所以</p>
<script type="math/tex; mode=display">
Q_1=Q_2,R_1=R_2</script><p> 所以分解是唯一的</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/24/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/2.%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%9F%A9%E9%98%B5%E4%B8%8EJordan%E6%A0%87%E5%87%86%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/24/%E7%9F%A9%E9%98%B5%E5%88%86%E6%9E%90/2.%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%9F%A9%E9%98%B5%E4%B8%8EJordan%E6%A0%87%E5%87%86%E5%9E%8B/" class="post-title-link" itemprop="url">多项式矩阵与Jordan标准型</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-01-24 19:45:55 / 修改时间：15:16:07" itemprop="dateCreated datePublished" datetime="2022-01-24T19:45:55+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Matrix-Analysis/" itemprop="url" rel="index"><span itemprop="name">Matrix Analysis</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="lambda-矩阵及其Smith型"><a href="#lambda-矩阵及其Smith型" class="headerlink" title="$\lambda$ 矩阵及其Smith型"></a>$\lambda$ 矩阵及其Smith型</h2><h5 id="lambda-矩阵"><a href="#lambda-矩阵" class="headerlink" title="$\lambda$ 矩阵"></a>$\lambda$ 矩阵</h5><p>以多项式为元素的矩阵</p>
<ul>
<li><p>$\mathbb{F}[\lambda]$ ：表示以数域 $\mathbb{F}$ 中的元素为系数的多项式集合</p>
</li>
<li><p>$(\mathbb{F}[\lambda])^{m\times n}=\{[a_{ij}(\lambda)]_{m\times n}|a_{ij}(\lambda)\in \mathbb{F}[\lambda]\}$ （类比 $\mathbb{F}^{m\times n}$）</p>
</li>
<li><p>$\lambda$ 矩阵： $\boldsymbol{A}(\lambda)=[a_{ij}(\lambda)]_{m\times n},\ a_{ij}(\lambda)\in\mathbb{F}[\lambda],\ \boldsymbol{A}(\lambda)\in(\mathbb{F}[\lambda])^{m\times n}$</p>
</li>
<li><p>从映射观点看 $\lambda$ 矩阵：$\mathbb{F}\rightarrow\mathbb{F}^{m\times n},\ \lambda\mapsto\boldsymbol{A}(\lambda)$</p>
</li>
<li><p>从矩阵多项式观点看 $\lambda$ 矩阵：$\boldsymbol{A}(\lambda) = A_0+A_1\lambda+A_2\lambda^2+\cdots+$ ，即以矩阵为系数的多项式</p>
<h5 id="lambda-矩阵的秩"><a href="#lambda-矩阵的秩" class="headerlink" title="$\lambda$ 矩阵的秩"></a>$\lambda$ 矩阵的秩</h5><p>不为零多项式的子式的最大阶数 （以多项式为元素算行列式）</p>
</li>
</ul>
<h5 id="单位模阵（幺模阵）"><a href="#单位模阵（幺模阵）" class="headerlink" title="单位模阵（幺模阵）"></a>单位模阵（幺模阵）</h5><ul>
<li>对 $\boldsymbol{U}(\lambda)\in(\mathbb{F}[\lambda])^{n\times n}$ ，若 $\exists\ \boldsymbol{V}(\lambda)\in(\mathbb{F}[\lambda])^{n\times n},\ s.t.\ \boldsymbol{U}(\lambda)\boldsymbol{V}(\lambda)=\boldsymbol{V}(\lambda)\boldsymbol{U}(\lambda)=\boldsymbol{I}_n$ ，则称 $\boldsymbol{V}(\lambda)$ 为 $\boldsymbol{U}(\lambda)$ 的单位模阵。</li>
</ul>
<p>多项式矩阵相对于前面所谈论的数值矩阵，主要不同就在于除法运算上，即多项式相除结果不一定还是多项式。用抽象数学的概念，数值矩阵谈论是域上的矩阵，而多项式矩阵谈论的是环上的矩阵</p>
<ul>
<li>$\boldsymbol{U}(\lambda)\in(\mathbb{F}[\lambda])^{n\times n}$ 为单位模阵 $\Leftrightarrow |\boldsymbol{U}(\lambda)|$ 是非零常值多项式（即 $a_0\neq 0$ 的零次多项式）<br>下面给出证明：</li>
</ul>
<p>$\Leftarrow$ ：显然</p>
<p>$\Rightarrow$ ：存在多项式矩阵 $\boldsymbol{V}(\lambda)$ ，使得 $\boldsymbol{U}(\lambda)\boldsymbol{V}(\lambda)=\boldsymbol{I}_n\Rightarrow|\boldsymbol{U}(\lambda)||\boldsymbol{V}(\lambda)|=1\Rightarrow f(\lambda)g(\lambda)=1$ ，两个多项式相乘等于1，这两个多项式只能是 $a_0\neq 0$ 的零次多项式，否则相乘一定有变量在</p>
<p>$\lambda$ 矩阵的初等行（列）变换<br>以初等行变换为例 $\Leftrightarrow$ 左乘一个相应的初等矩阵：</p>
<ul>
<li><p>某两行互换 $\Leftrightarrow$ eg. $\begin{bmatrix} 0&amp;1&amp;0\\1&amp;0&amp;0\\0&amp;0&amp;1\end{bmatrix}$</p>
</li>
<li><p>某行乘以非零常数 $\Leftrightarrow$ eg. $\begin{bmatrix} c&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix}$</p>
</li>
</ul>
<p><code>注：不是乘以非零多项式！因为乘以多项式是不可逆的，即对应乘上的矩阵没有单位模阵</code></p>
<ul>
<li>某行乘以一个多项式再加到另一行上 $\Leftrightarrow$ eg. $\begin{bmatrix}1&amp;0&amp;0\\f(\lambda)&amp;1&amp;0\\0&amp;0&amp;1\end{bmatrix}$</li>
</ul>
<h6 id="lambda-矩阵经左右初等变换化简"><a href="#lambda-矩阵经左右初等变换化简" class="headerlink" title="$\lambda$ 矩阵经左右初等变换化简"></a>$\lambda$ 矩阵经左右初等变换化简</h6><ul>
<li><p>$\lambda$ 矩阵的等价：经初等行列变换可以互相转化的两个 $\lambda$ 矩阵等价，记作 $\boldsymbol{A}(\lambda) \sim \boldsymbol{B}(\lambda) $</p>
</li>
<li><p>【用初等行变换为矩阵左上角降次】设 $\boldsymbol{A}(\lambda) =[a_{ij}(\lambda)]_{m\times n},\ a_{11}(\lambda)\neq0$ ， 且至少有一个元素不能被 $a_{11}(\lambda)$ 整除（$\Leftrightarrow a_{11}(\lambda)$ 不为非零常值多项式 ），则有 $\boldsymbol{A}(\lambda) \sim\boldsymbol{B}(\lambda) $ 且 $b_{11}(\lambda)\neq0,\ \partial(b_{11}(\lambda))&lt;\partial(a_{11}(\lambda))$ （$\partial(多项式)$ 表示该多项式的次数）</p>
</li>
</ul>
<p>下面给出证明概要：</p>
<ul>
<li><p>同行中有一个元素 $a_{1j}(\lambda)$ 不能被 $a_{11}(\lambda)$ 整除，即 $a_{1j}(\lambda)=a_{11}(\lambda)q(\lambda)+r(\lambda),\ 且\ \partial(r(\lambda))&lt;\partial(a(\lambda))$ ，所以，将第一列乘以 $-q(\lambda)$ 加到第 $j$ 列后交换第一列与第 $j$ 列，此时 $a_{11}(\lambda)’=r(\lambda),\partial(a_{11}(\lambda)’)&lt;\partial(a_{11}(\lambda))$ ，从而实现降次</p>
</li>
<li><p>同列中有一个元素 $a_{i1}(\lambda)$ 不能被 $a_{11}(\lambda)$ 整除，同理</p>
</li>
<li><p>同行同列都能被整除，但有 $a_{ij}(\lambda)$ 不能被 $a_{11}(\lambda)$ 整除。</p>
<p>  首先可以通过一系列行列变换把第一列、第一行除了 $a_{11}(\lambda)$ 外都化为0，同时保证变换后的 $a_{ij}(\lambda)’$ 仍旧不能被 $a_{11}(\lambda)$ 整除（不能被整除的数加上除数的倍数依然不能被整除），将变换后的第 $j$ 行加到第1行，就回到第一种情况了</p>
</li>
</ul>
<h5 id="Smith-型-lambda-矩阵"><a href="#Smith-型-lambda-矩阵" class="headerlink" title="Smith 型 $\lambda$ 矩阵"></a>Smith 型 $\lambda$ 矩阵</h5><p>设：</p>
<script type="math/tex; mode=display">
\boldsymbol{A}(\lambda)\sim \boldsymbol{B}(\lambda)=
\left[
\begin{array}{ccc|ccc}
    d_1({\lambda}) & & & 0 & \cdots & 0\\
    & \ddots & &\vdots &\ddots &\vdots \\
    & & d_r(\lambda) &0 &\cdots &0 \\
    \hline 
    0 & \cdots & 0 &0 &\cdots &0 \\
    \vdots &\ddots &\vdots &\vdots &\ddots &\vdots \\
    0 & \cdots & 0 &0 &\cdots &0
\end{array}
\right]</script><p>其中 $r=\text{rank}(\boldsymbol{A}(\lambda))$ ，$d_i(\lambda)$ 为非零多项式，且 $d_i(\lambda)|d_{i+1}(\lambda)$ ($f(\lambda)|g(\lambda)$ 表示 $f$ 整除 $g$ )，则称 $\boldsymbol{B}(\lambda)$ 为 $\boldsymbol{A}(\lambda)$ 的 Smith标准型 。</p>
<p>下面给出证明：</p>
<ul>
<li><p>存在性</p>
<p>  若为零矩阵，则显然；若为非零矩阵，必存在非零多项式，通过行列变换将次数最低的非零多项式移到 $a_{11}(\lambda)$ ，会出现以下两种情况：</p>
<ol>
<li><p>矩阵中所有元素都能被 $a_{11}(\lambda)$ 整除</p>
</li>
<li><p>存在若干元素不能被 $a_{11}(\lambda)$ 整除</p>
<p>对于第二种情况，【用初等行变换为左上角降次】，降次必然是有限的，不能再降次的时候，也就是矩阵中的所有元素都能被 $a_{11}(\lambda)$ 整除的时候 ，进而同行同列的元素都能化成0</p>
<p>对右下子矩阵进行相同的操作，以此类推，最终得到 Smith标准型</p>
</li>
</ol>
</li>
<li><p>唯一性</p>
<p>  约定 $d_i(\lambda)$ 都是首1多项式，即最高次数系数为1</p>
<ul>
<li><p>$\lambda$ 矩阵的 $k$ 阶行列式因子 ：$\boldsymbol{A}(\lambda)$ 的所有 $k$ 阶子式的最高公因式</p>
</li>
<li><p><code>初等变换不改变</code> $k$ <code>阶行列式因子(不是很理解)</code></p>
<p>下面给出证明：</p>
<p>设 $\mathscr{A}=\{\boldsymbol{A}\ 的\ k\ 阶子式\},\mathscr{B}=\{\boldsymbol{B}\ 的\ k\ 阶子式\}$ ，只需证 $\mathscr{A},\mathscr{B}$ 的最高公因式相同。</p>
<p>任取 $f(\lambda)\in\mathscr{B}$ ，考察它与 $\mathscr{A}$ 中多项式的关系，由于初等变换是三种基本变换的组合，所以只需分别对三种基本变换进行讨论即可：</p>
</li>
</ul>
<ol>
<li><p>某两行互换：只影响行列式的符号，因为约定了所有多项式都是首1多项式，所以这种变换对子式集合无影响，即 $\mathscr{A}=\mathscr{B}$ ，那么必有 $f(\lambda)\in\mathscr{A}$</p>
</li>
<li><p>某行乘以一个非零常数：同样，因为约定了所有多项式都是首1多项式，所以也不会有影响，即 $\mathscr{A}=\mathscr{B}$ ，那么必有 $f(\lambda)\in\mathscr{A}$</p>
</li>
<li><p>某行乘以一个多项式再加到另一行上。假设是第 $i$ 行乘以 $h(\lambda)$ 加到第 $j$ 行。那么：</p>
</li>
</ol>
<ul>
<li><p>如果 $f(\lambda)$ 没有取到第 $j$ 行，那么该子式不发生变化，即 $f(\lambda)\in\mathscr{A}$ ；</p>
</li>
<li><p>如果 $f(\lambda)$ 取到了第 $j$ 行：</p>
<ul>
<li><p>如果同时取到了第 $i$ 行，那么该子式不发生变化，即 $f(\lambda)\in\mathscr{A}$</p>
</li>
<li><p>如果没有取到第 $i$ 行，显然 $f(\lambda)\notin\mathscr{A}$ ，但是可以将它对应的行列式拆分：</p>
<script type="math/tex; mode=display">
\left|
\begin{array}{c}
  \boldsymbol{a}_p \\
  \vdots \\
  \boldsymbol{a}_j+\boldsymbol{a_i}\cdot h(\lambda) \\
  \vdots \\
  \boldsymbol{a}_{p+k-1}
\end{array}
\right| = 
\left|
\begin{array}{c}
  \boldsymbol{a}_p \\
  \vdots \\
  \boldsymbol{a}_j \\
  \vdots \\
  \boldsymbol{a}_{p+k-1}
\end{array}
\right|+
\left|
\begin{array}{c}
  \boldsymbol{a}_p \\
  \vdots \\
  \boldsymbol{a_i} \\
  \vdots \\
  \boldsymbol{a}_{p+k-1}
\end{array}
\right|\cdot h(\lambda)</script><p>即：$f(\lambda)=g(\lambda)+w(\lambda)\cdot h(\lambda)$，其中 $g(\lambda),w(\lambda)\in\mathscr{A}$ ，所以 $\mathscr{A}$ 的公因式必整除 $f(\lambda)$</p>
</li>
</ul>
<p>综上，$\mathscr{A}$ 的公因式必能整除 $\mathscr{B}$ 中的所有子式，由于初等变换可逆知，$\mathscr{B}$ 的公因式也必能整除 $\mathscr{A}$ 中的所有子式，从而二者的最高公因式相同</p>
</li>
<li><p>不变因子 $d_{i}(\lambda)$</p>
<p>记 $k$ 阶行列式因子为 $D_k(\lambda)$ ，初等变换不改变行列式因子，所以：<br>$d_1(\lambda) = D_1(\lambda),\ d_i(\lambda)=D_i(\lambda)/D_{i-1}(\lambda)$</p>
<p>因此，称 $d_i(\lambda)$ 为不变因子（行列式因子唯一，所以 $d_i(\lambda)$ 一定唯一）</p>
</li>
</ul>
</li>
</ul>
<h5 id="两种方法求Smith型"><a href="#两种方法求Smith型" class="headerlink" title="两种方法求Smith型"></a>两种方法求Smith型</h5><ul>
<li>初等变换：找次数最低项，要么能整除，要么能降次（带余除法）</li>
</ul>
<p>例：</p>
<script type="math/tex; mode=display">
\begin{align*}
\begin{bmatrix}\lambda(\lambda+1)&0&0\\0&\lambda&0\\0&0&(\lambda+1)^2\end{bmatrix} 
&\xrightarrow[]{最低次放左上角}
&\begin{bmatrix}\lambda&0&0\\0&\lambda(\lambda+1)&0\\0&0&(\lambda+1)^2\end{bmatrix} \\
&\xrightarrow[加到第一行]{不能整除的某行}
&\begin{bmatrix}\lambda&0&(\lambda+1)^2\\0&\lambda(\lambda+1)&0\\0&0&(\lambda+1)^2\end{bmatrix} \\
&\xrightarrow[]{带余除法降次}
&\begin{bmatrix}\lambda&0&1\\0&\lambda(\lambda+1)&0\\0&0&(\lambda+1)^2\end{bmatrix} \\
&\xrightarrow[重复带余除法直到能整除]{最低次换到左上角}
&\begin{bmatrix}1&0&\lambda\\0&\lambda(\lambda+1)&0\\(\lambda+1)^2&0&0\end{bmatrix} \\
&\xrightarrow[]{同行同列化零}
&\begin{bmatrix}1&0&0\\ 0&\lambda(\lambda+1)&0\\ 0&0&-\lambda(\lambda+1)^2\end{bmatrix} \\
&\xrightarrow[]{化为首1多项式}
&\begin{bmatrix}1&0&0\\ 0&\lambda(\lambda+1)&0\\ 0&0&\lambda(\lambda+1)^2\end{bmatrix} \\
\end{align*}</script><ul>
<li><p>计算所有行列式因子</p>
<p>  同上例，有 $D_1(\lambda)=1,D_2(\lambda)=\lambda(\lambda+1),D_3=\lambda^2(\lambda+1)^3$ ，故而 $d_1(\lambda)=1, d_2(\lambda)=\lambda(\lambda+1),d_3(\lambda)=\lambda(\lambda+1)^2$</p>
</li>
</ul>
<h5 id="单位模阵可写成初等矩阵乘积"><a href="#单位模阵可写成初等矩阵乘积" class="headerlink" title="单位模阵可写成初等矩阵乘积"></a>单位模阵可写成初等矩阵乘积</h5><ul>
<li><p>单位模阵的 Smith 型为单位矩阵</p>
<p>  下面给出证明：</p>
<p>  $\boldsymbol{U}(\lambda)$ 是单位模阵 $\Leftrightarrow |\boldsymbol{U}(\lambda)|=c\neq0$ （为非零常数）$\Leftrightarrow\boldsymbol{U}(\lambda)$ 的 $n$ 阶行列式因子为1，且 $r=\text{rank}(\boldsymbol{U})=n$</p>
<p>  又：$D_n(\lambda)=D_1(\lambda)\frac{D_2(\lambda)}{D_1(\lambda)}\cdots\frac{D_n(\lambda)}{D_{n-1}(\lambda)}=d_1(\lambda)d_2(\lambda)\cdots d_n(\lambda)$</p>
<p>  所以：$d_i(\lambda)=1$ ，证毕</p>
</li>
<li><p>单位模阵可写成有限个初等矩阵的乘积<br>  据上条性质，单位模阵经过若干次初等变换后可以变成单位矩阵，写成矩阵的形式即为：<br>  $\boldsymbol{P}_s(\lambda)\cdots\boldsymbol{P}_1(\lambda)\boldsymbol{U}(\lambda)\boldsymbol{Q}_1(\lambda)\cdots\boldsymbol{Q}_t(\lambda)=\boldsymbol{I}$</p>
<p>  初等矩阵的逆矩阵仍为初等矩阵，因此：<br>  $\boldsymbol{U}(\lambda)=\boldsymbol{P}_1^{-1}(\lambda)\cdots\boldsymbol{P}_s^{-1}(\lambda)\boldsymbol{Q}_t^{-1}(\lambda)\boldsymbol{Q}_1^{-1}(\lambda)$</p>
</li>
</ul>
<h5 id="数域矩阵相似最简型问题"><a href="#数域矩阵相似最简型问题" class="headerlink" title="数域矩阵相似最简型问题"></a>数域矩阵相似最简型问题</h5><ul>
<li>特征矩阵：给定 $\boldsymbol{A}\in\mathbb{F}^{n\times n}$ ，称多项式矩阵 $\lambda\boldsymbol{I}-\boldsymbol{A}$ 为矩阵 $\boldsymbol{A}$ 的特征矩阵</li>
</ul>
<p><strong><em>定理</em></strong>：数域矩阵相似 $\Leftrightarrow$ 特征矩阵等价</p>
<p>把一个受约束的相似问题【 $\boldsymbol{AP}=\boldsymbol{PB}$ 行列变换必须配套】转换成了一个不受约束的等价问题【 $\boldsymbol{U}(\lambda)(\lambda\boldsymbol{I}-\boldsymbol{A})\boldsymbol{V}(\lambda)=\lambda\boldsymbol{I}-\boldsymbol{B}$ 行列变换无需配套】</p>
<p>下面给出证明：<br><code>预备知识</code></p>
<ul>
<li><p>多项式矩阵的<code>次数</code></p>
<p>  若 $\boldsymbol{A}(\lambda) = A_0+A_1\lambda+A_2\lambda^2+\cdots+A_r\lambda^r,\ 且\ A_r\neq0$ ，则称 $\boldsymbol{A}(\lambda)$ 的次数为 $r$ ，记作 $\partial(\boldsymbol{A}(\lambda))=r$ 。特别地，零多项式矩阵的次数无意义。</p>
</li>
<li><p>引理1：多项式矩阵<code>乘积的次数</code></p>
<p>  $\boldsymbol{A}(\lambda)\boldsymbol{B}(\lambda)=\boldsymbol{C}(\lambda)$，且 $\boldsymbol{A}(\lambda) = A_0+A_1\lambda+\cdots+A_r\lambda^r$ ，其中$\boldsymbol{A}(\lambda)\in(\mathbb{F}[\lambda])^{m\times m}, \boldsymbol{B}(\lambda),\boldsymbol{C}(\lambda)\in(\mathbb{F}[\lambda])^{m\times n}$若满足：</p>
<p>  ​ 1）三个矩阵均非零（零矩阵的次数无意义）</p>
<p>  ​ 2）$A_r\in \mathbb{F}^{m\times m}$ 可逆（保证 $\boldsymbol{C}(\lambda)$ 的最高次矩阵系数 $C_{r+s}=A_r \cdot B_s$ 非零）</p>
<p>  那么 $\partial(\boldsymbol{C}(\lambda))=\partial(\boldsymbol{A}(\lambda))+\partial(\boldsymbol{B}(\lambda))$</p>
</li>
<li><p>引理2：多项式矩阵的带余除法（以左除为例）：<br>  $\boldsymbol{A}(\lambda) = A_0+A_1\lambda+\cdots+A_r\lambda^r\in(\mathbb{F}[\lambda])^{m\times m}$ ，若满足 $A_r\in\mathbb{F}^{m\times m}$ 可逆，那么对非零矩阵 $\boldsymbol{B}(\lambda)\in(\mathbb{F}[\lambda])^{m\times n}$ ，存在唯一矩阵 $\boldsymbol{Q}(\lambda), \boldsymbol{R}(\lambda)$ ，使得：$\boldsymbol{B}(\lambda)=\boldsymbol{A}(\lambda)\boldsymbol{Q}(\lambda)+\boldsymbol{R}(\lambda)$ ，其中 $\boldsymbol{R}(\lambda)=\boldsymbol{0}$ 或 $\partial(\boldsymbol{R}(\lambda))&lt;\partial(\boldsymbol{A}(\lambda))$</p>
</li>
</ul>
<p><code>证明如下：</code></p>
<ul>
<li><p>必要性：有$\boldsymbol{P}$找$\boldsymbol{U},\boldsymbol{V}$<br>$\Rightarrow$ ：取 $\boldsymbol{U}(\lambda)=\boldsymbol{P}^{-1},\boldsymbol{V}(\lambda)=\boldsymbol{P}$ ，因为 $\boldsymbol{P}$ 可逆，故行列式为非零常数，满足单位模阵的要求。$U(\lambda)(\lambda I-A)V(\lambda) =P^{-1}(\lambda I - A)P=\lambda P^{-1}IP- P^{-1}AP=\lambda I - B$</p>
</li>
<li><p>充分性：从 $\boldsymbol{U},\boldsymbol{V}$ 找 $\boldsymbol{P}$<br>$\Leftarrow$ ：根据多项式矩阵带余除法，因为 $\boldsymbol{I}$ 可逆，所以：</p>
<script type="math/tex; mode=display">\boldsymbol{U}(\lambda)=(\lambda\boldsymbol{I}-\boldsymbol{B})\boldsymbol{Q}(\lambda)+\boldsymbol{R}(\lambda)  \qquad(*)</script><p>  其中：$\boldsymbol{R}(\lambda)=\boldsymbol{0}$ ，或 $\partial(\boldsymbol{R}(\lambda))&lt;\partial(\lambda\boldsymbol{I}-\boldsymbol{B})=1$ ，即 $\boldsymbol{R}(\lambda)=\boldsymbol{R}\in\mathbb{F}^{n\times n}$ 为常数矩阵。因此：</p>
<script type="math/tex; mode=display">
  \begin{align*}
  &\boldsymbol{U}(\lambda)(\lambda\boldsymbol{I}-\boldsymbol{A})\boldsymbol{V}(\lambda)=\lambda\boldsymbol{I}-\boldsymbol{B} \\
  \Leftrightarrow &[(\lambda\boldsymbol{I}-\boldsymbol{B})\boldsymbol{Q}(\lambda)+\boldsymbol{R}](\lambda\boldsymbol{I}-\boldsymbol{A})\boldsymbol{V}(\lambda)=\lambda\boldsymbol{I}-\boldsymbol{B} \\
  \Leftrightarrow &\boldsymbol{R}(\lambda\boldsymbol{I}-\boldsymbol{A})=(\lambda\boldsymbol{I}-\boldsymbol{B})[\boldsymbol{V}^{-1}(\lambda)-\boldsymbol{Q}(\lambda)(\lambda\boldsymbol{I}-\boldsymbol{A})] 
  \end{align*}</script><p>  由引理1，比较等式两端变量 $\lambda$ 的系数可知，$\boldsymbol{V}^{-1}(\lambda)-\boldsymbol{Q}(\lambda)(\lambda\boldsymbol{I}-\boldsymbol{A})$ 也是常数矩阵，记作 $\boldsymbol{S}$ ，故而：</p>
<script type="math/tex; mode=display">
  \boldsymbol{R}(\lambda\boldsymbol{I}-\boldsymbol{A})=(\lambda\boldsymbol{I}-\boldsymbol{B})\boldsymbol{S}</script><p>  且由 $\lambda$ 系数知：$\boldsymbol{R}=\boldsymbol{S}$ ，从而有 $\boldsymbol{RA}=\boldsymbol{BR}$ ，只需证 $\boldsymbol{R}$ 可逆即可</p>
<p>  要证明可逆，我们期望找出一个矩阵 $\boldsymbol{P}$ ，使得 $\boldsymbol{RP}=\boldsymbol{I}$ .</p>
<p>  考虑到 $\boldsymbol{R}$ 是由矩阵 $\boldsymbol{U}(\lambda)$ 对矩阵 $\lambda\boldsymbol{I}-\boldsymbol{B}$ 作带余除法所得，对应地，也可以有：</p>
<script type="math/tex; mode=display">\boldsymbol{U}^{-1}(\lambda)=(\lambda\boldsymbol{I}-\boldsymbol{A})\widetilde{\boldsymbol{Q}}(\lambda)+\widetilde{\boldsymbol{R}} \qquad(**)</script><p>  将方程 (<em>) (*</em>) 代入 $\boldsymbol{U}(\lambda)\boldsymbol{U}^{-1}(\lambda)=\boldsymbol{I}$ ，整理得：</p>
<script type="math/tex; mode=display">\boldsymbol{R}(\lambda\boldsymbol{I}-\boldsymbol{A})\widetilde{\boldsymbol{Q}}(\lambda)+\boldsymbol{R}\widetilde{\boldsymbol{R}}=\boldsymbol{I}-(\lambda\boldsymbol{I}-\boldsymbol{B})\boldsymbol{Q}(\lambda)\boldsymbol{U}^{-1}(\lambda)</script><p>  等式左边因为 $\boldsymbol{R}(\lambda\boldsymbol{I}-\boldsymbol{A})=(\lambda\boldsymbol{I}-\boldsymbol{B})\boldsymbol{R}$ ，等价为：</p>
<script type="math/tex; mode=display">
  (\lambda\boldsymbol{I}-\boldsymbol{B})\boldsymbol{S}\widetilde{\boldsymbol{Q}}(\lambda) +\boldsymbol{R}\widetilde{\boldsymbol{R}}=\boldsymbol{I}-(\lambda\boldsymbol{I}-\boldsymbol{B})\boldsymbol{Q}(\lambda)\boldsymbol{U}^{-1}(\lambda)</script><p>  合并同类项得：</p>
<script type="math/tex; mode=display">
  \boldsymbol{R}\widetilde{\boldsymbol{R}}=\boldsymbol{I}-(\lambda\boldsymbol{I}-\boldsymbol{B})[\boldsymbol{Q}(\lambda)\boldsymbol{U}^{-1}(\lambda)+\boldsymbol{S}\widetilde{\boldsymbol{Q}}(\lambda)] \qquad(***)</script><p>  比较方程$(<em>*</em>)$两边 $\lambda$ 系数知：$\boldsymbol{R}\widetilde{\boldsymbol{R}}=\boldsymbol{I}$ ，从而 $\boldsymbol{R}$ 可逆。</p>
<p>  证毕。</p>
</li>
</ul>
<h5 id="特征矩阵的-Smith-型"><a href="#特征矩阵的-Smith-型" class="headerlink" title="特征矩阵的 Smith 型"></a>特征矩阵的 Smith 型</h5><p>定理1：因为 $|\lambda\boldsymbol{I}-\boldsymbol{A}|$ 的最高次项 $\lambda^n$ 的系数必为1，因此，$\lambda\boldsymbol{I}-\boldsymbol{A}$ 作为多项式矩阵的秩为 $n$ 。从而它的Smith型矩阵为：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
d_1(\lambda)&\\
&\ddots&\\
&&d_n(\lambda)
\end{bmatrix}</script><p>定理2：由定理1可得，$d_1(\lambda)\cdots d_n(\lambda)=|\lambda\boldsymbol{I}-\boldsymbol{A}|\Rightarrow\partial(d_1(\lambda))+\cdots+\partial(d_n(\lambda))=n$ 。若某个 $d_i(\lambda)$ 的次数为 $n_i\neq1$ ，那么必将多出 $n_i-1$ 个常数不变因子，根据首1多项式的约定，这些不变因子最终都会化为1。因此上述Smith型矩阵进一步表示为：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
1&&&&&\\
&\ddots&&&&\\
&&1&&&\\
&&&h_1(\lambda)&&\\
&&&&\ddots&\\
&&&&&h_p(\lambda)
\end{bmatrix}</script><p>其中，$\partial(h_i(\lambda))=n_i$ 。假设有 $p$ 个非常数不变因子，那么等于1的不变因子共有 $n-p$ 个，且满足 $n-p=(n_1-1)+\cdots+(n_p-1)$ ，因此将对角线上的元素按一个 $h_i(\lambda)$ ，$n_i-1$ 个1配对进行重组，Smith型矩阵等价于如下矩阵：</p>
<script type="math/tex; mode=display">
\left[
\begin{array}{cccc|c|cccc}
    1 & & & & & & & & \\
    & \ddots & & & & & & & \\
    & & 1 & & & & & & \\
    & & & h_1(\lambda) & & & & & \\
    \hline
    & & & & \ddots & & & & \\
    \hline
    & & & & & 1 & & & \\
    & & & & & & \ddots & & \\
    & & & & & & & 1 & \\
    & & & & & & & & h_p(\lambda)
\end{array}
\right]</script><p>每一小块都是 $\begin{bmatrix} 1&amp;&amp;&amp;\\&amp;\ddots&amp;&amp;\\&amp;&amp;1&amp;\\&amp;&amp;&amp;h_i(\lambda)\end{bmatrix}_{n_i\times n_i}\qquad (<em>**</em>)$</p>
<ul>
<li>特征矩阵的初等因子组</li>
</ul>
<p>初等因子指的是将特征矩阵的所有非常数不变因子在 $\mathbb{F}[\lambda]$ 中作质因式分解时出现的质因式的方幂。所有初等因子组成初等因子组（显然初等因子组中的某个初等因子可能重复出现）。</p>
<p>$h_i(\lambda)\xlongequal{质因式分解}w_1^{r_{i1}}(\lambda)\cdots w_j^{r_{ij}}(\lambda)$ ，则每个质因式的方幂 $w_j^{r_{ij}}(\lambda)$ 就是一个初等因子，所有方幂组成了特征矩阵的初等因子组。<br><code>初等因子之间互素。</code></p>
<p>（实数域中的质因式包括 $x-c$ 和 $x^2+bx+c,且\ b^2-4c&lt;0$ 两种情况；复数域中的质因式只有 $x-c$ 一种情况）</p>
<p>初等因子组与不变因子（行列式因子、初等因子）互相唯一决定</p>
<p>由不变因子决定初等因子组：根据定义，显然</p>
<p>​ 例：已知矩阵 $\boldsymbol{A}$ 的特征矩阵 $\lambda\boldsymbol{I}-\boldsymbol{A}$ 的 Smith型 为：$\begin{bmatrix}1&amp;0&amp;0\\0&amp;(\lambda+1)^2&amp;0\\0&amp;0&amp;\lambda(\lambda+1)^2\end{bmatrix}$</p>
<p>​ 显然初等因子组为：$\{(\lambda+1)^2,\lambda, (\lambda+1)^2\}$</p>
<p>由初等因子组决定不变因子：</p>
<p>​ 例：设 $\boldsymbol{A}\in\mathbb{C}^{8\times8}$ ，且初等因子组为：$\{\lambda,\lambda^2,\lambda,(\lambda+1)^2,(\lambda+1)^2\}$ ，求不变因子。</p>
<p>​ 由于不变因子满足 $d_i(\lambda)|d_{i+1}(\lambda)$ 所以最后一个不变因子的次数应该最高且所有质因式都应出现，以此类推：</p>
<script type="math/tex; mode=display">
\begin{align*}
&d_8(\lambda)=\lambda^2(\lambda+1)^2 \\
&d_7(\lambda)=\lambda(\lambda+1)^2 \\
&d_6(\lambda)=\lambda \\
&d_5(\lambda)=\cdots=d_1(\lambda)=1 
\end{align*}</script><p>下面通过对非常数不变因子 $h_i(\lambda)$ 在复数域上作质因式分解，将上述$(<em>**</em>)$进一步化为一种更特殊的形式</p>
<script type="math/tex; mode=display">
h_i(\lambda)=(\lambda-c_1)^{r_1}(\lambda-c_2)^{r_2}\cdots(\lambda-c_k)^{r_k}</script><p>显然有 $r_1+r_2+\cdots+r_k=n_i$ ，逆用初等因子分解，因为子矩阵 $\begin{bmatrix} 1&amp;&amp;&amp;\\&amp;\ddots&amp;&amp;\\&amp;&amp;1&amp;\\&amp;&amp;&amp;h_i(\lambda)\end{bmatrix}_{n_i\times n_i}$ 有上述 $k$ 个初等因子，所以可以等价于（有相同的初等因子组）：</p>
<script type="math/tex; mode=display">
\left[
\begin{array}{cccc|c|cccc}
    1 & & & & & & & & \\
    & \ddots & & & & & & & \\
    & & 1 & & & & & & \\
    & & & (\lambda-c_1)^{r_1} & & & & & \\
    \hline
    & & & & \ddots & & & & \\
    \hline
    & & & & & 1 & & & \\
    & & & & & & \ddots & & \\
    & & & & & & & 1 & \\
    & & & & & & & & (\lambda-c_k)^{r_k}
\end{array}
\right]</script><p><strong><em>矩阵相似在多项式特征邻域中的各种刻画</em></strong><br>若 $\boldsymbol{A},\boldsymbol{B}\in\mathbb{F}^{n\times n}$ ，则下列结论等价：</p>
<p>$\boldsymbol{A}$ 与 $\boldsymbol{B}$ 相似；<br>$\lambda\boldsymbol{I}-\boldsymbol{A}$ 与 $\lambda\boldsymbol{I}-\boldsymbol{B}$ 作为多项式矩阵等价；<br>$\lambda\boldsymbol{I}-\boldsymbol{A}$ 与 $\lambda\boldsymbol{I}-\boldsymbol{B}$ 作为多项式矩阵有相同的 Smith型；<br>$\lambda\boldsymbol{I}-\boldsymbol{A}$ 与 $\lambda\boldsymbol{I}-\boldsymbol{B}$ 作为多项式矩阵有相同的 不变因子；<br>$\lambda\boldsymbol{I}-\boldsymbol{A}$ 与 $\lambda\boldsymbol{I}-\boldsymbol{B}$ 作为多项式矩阵有相同的 行列式因子；<br>$\lambda\boldsymbol{I}-\boldsymbol{A}$ 与 $\lambda\boldsymbol{I}-\boldsymbol{B}$ 作为多项式矩阵有相同的 初等因子组；<br>例：证明 $\boldsymbol{A}$ 与 $\boldsymbol{A}^T$ 相似</p>
<p>因为只提供了抽象矩阵，所以适用于具体形式的2/3/4/6方法都不容易处理，因此考虑行列式因子</p>
<p>因为 $(\lambda\boldsymbol{I}-\boldsymbol{A})^T=\lambda\boldsymbol{I}-\boldsymbol{A}^T$ ，而矩阵转置行列式不变，因此显然 $\lambda\boldsymbol{I}-\boldsymbol{A}$ 与 $\lambda\boldsymbol{I}-\boldsymbol{A}^T$ 有相同的行列式因子</p>
<p>多项式矩阵等价的证明方法<br>定义法：初等变换能否转化<br>相同的行列式因子 / Smith型 / 不变因子 / 初等因子组</p>
<h3 id="Jordan标准型"><a href="#Jordan标准型" class="headerlink" title="Jordan标准型"></a>Jordan标准型</h3><p>以下均在复数域上考虑，因而质因式只有 $\lambda-c$ 一种类型。</p>
<p>综合上面所说，特征矩阵可以有如下等价：</p>
<script type="math/tex; mode=display">
\lambda\boldsymbol{I}-\boldsymbol{A}\sim \boldsymbol{J}(\lambda)=
\begin{bmatrix}
\boldsymbol{J}_1(\lambda) &&\\
&\ddots&\\
&&\boldsymbol{J}_q(\lambda)
\end{bmatrix},
\boldsymbol{J}_i(\lambda)=
\begin{bmatrix}
1 &&& \\
& \ddots && \\
&& 1 & \\
&&& (\lambda-c_i)^{r_i}
\end{bmatrix}_{r_i\times r_i}</script><p>其中：$(\lambda-c_i)^{r_i},i=1,…,q$ 是特征矩阵的初等因子组。</p>
<p>从而：$\boldsymbol{A}\longrightarrow\lambda\boldsymbol{I}-\boldsymbol{A}\sim\boldsymbol{J}(\lambda)\sim\lambda\boldsymbol{I}-\boldsymbol{J}\longleftarrow\boldsymbol{J}$ ，满足这个条件的矩阵都与 $\boldsymbol{A}$ 相似</p>
<p>我们要做的，就是求出其中最简单的矩阵，也就是相似最简型</p>
<p>进一步，这个问题又可以拆分成针对各个子块的类似的问题，即：求一个尽可能简单的矩阵 $\boldsymbol{J}_i\in\mathbb{C}^{r_i\times r_i}$ ，使得 $\lambda\boldsymbol{I}_{r_i}-\boldsymbol{J}_i\sim\boldsymbol{J}_i(\lambda)$</p>
<h5 id="Jordan块的Simth型"><a href="#Jordan块的Simth型" class="headerlink" title="Jordan块的Simth型"></a>Jordan块的Simth型</h5><p>若当块：对角线元素全为 $c_1$ ，次对角线元素均为1的矩阵：</p>
<p>结论：若当块所对应特征矩阵的Simth型即满足 $\lambda\boldsymbol{I}_{r_i}-\boldsymbol{J}_i\sim\boldsymbol{J}_i(\lambda)$</p>
<p>下面给出证明（本质就是证明两个多项式矩阵等价） ：</p>
<p>方法一：定义法</p>
<script type="math/tex; mode=display">
左边=\begin{bmatrix}
\lambda-c_1 & -1 & \cdots & 0 \\
\vdots & \ddots & \ddots & \vdots \\
\vdots & & \ddots & -1 \\
0 & \cdots & \cdots& \lambda-c_1
\end{bmatrix}_{r_i\times r_i}</script><p>方法二：计算两边的行列式因子<br>由于左边的 $r_i-1$ 阶行列式中有一个是1（右上角子块），所以左边矩阵的 $r_i-1$ 阶最高公因式只能是1，即左边矩阵的 $r_i-1$ 阶行列式因子为1。</p>
<p>从而所有比它低阶的行列式因子也只能是1，因为 $D_{i-1}(\lambda)$ 必须整除 $D_{i}(\lambda)$ 。</p>
<p>而左侧行列式等于右侧行列式，即左侧 $r_i$ 阶行列式因子与右侧相同。</p>
<p>故而两边矩阵的任意阶行列式因子都相同，从而矩阵等价。</p>
<p>综上所述，有如下定理：</p>
<p>给定矩阵 $\boldsymbol{A}\in\mathbb{C}^{n\times n}$ ，其特征矩阵 $\lambda\boldsymbol{I}-\boldsymbol{A}$ 的初等因子组为：$(\lambda-c_1)^{r_1},\cdots,(\lambda-c_q)^{r_q}$ ，取</p>
<script type="math/tex; mode=display">
\boldsymbol{J}_i=\begin{bmatrix}
c_i & 1 & \cdots & 0 \\
\vdots & \ddots & \ddots & \vdots \\
\vdots & & \ddots & 1 \\
0 & \cdots & \cdots& c_i
\end{bmatrix}_{r_i\times r_i}</script><p>则 $\boldsymbol{A}$ 的相似最简型为：</p>
<script type="math/tex; mode=display">
\boldsymbol{A}\sim\boldsymbol{J}=
\begin{bmatrix}
J_1 &  \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & J_q
\end{bmatrix}</script><p>而 $\boldsymbol{J}$ 就是矩阵 $\boldsymbol{A}$ 的 Jordan标准型</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/3.%20%E5%8D%B7%E7%A7%AF%E3%80%81%E5%99%AA%E9%9F%B3%E3%80%81%E7%BA%B9%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/3.%20%E5%8D%B7%E7%A7%AF%E3%80%81%E5%99%AA%E9%9F%B3%E3%80%81%E7%BA%B9%E7%90%86/" class="post-title-link" itemprop="url">卷积、噪音、纹理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-23 19:45:57" itemprop="dateCreated datePublished" datetime="2022-01-23T19:45:57+08:00">2022-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-01-24 09:45:54" itemprop="dateModified" datetime="2022-01-24T09:45:54+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="卷积与图像去噪"><a href="#卷积与图像去噪" class="headerlink" title="卷积与图像去噪"></a>卷积与图像去噪</h2><h3 id="图像类型"><a href="#图像类型" class="headerlink" title="图像类型"></a>图像类型</h3><ul>
<li><ol>
<li>二值图像（Binary Image）<br>一幅二值图像的二维矩阵仅由0、1两个值构成，“0”代表黑色，“1”代白色。</li>
</ol>
</li>
<li><ol>
<li>灰度图像（Gray Image）<br>灰度图像矩阵元素的取值范围通常为[0，255]。因此其数据类型一般为8位无符号整数的（int8），这就是人们经常提到的256灰度图像。“0”表示纯黑色，“255”表示纯白色，中间的数字从小到大表示由黑到白的过渡色。</li>
</ol>
</li>
<li><ol>
<li>RGB彩色图像（Color Image）<br>RGB图像分别用红（R）、绿（G）、蓝（B）三原色的组合来表示每个像素的颜色。RGB图像的数据类型一般为8位无符号整形，通常用于表示和存放真彩色图像，当然也可以存放灰度图像。</li>
</ol>
</li>
</ul>
<h3 id="图像去噪与卷积"><a href="#图像去噪与卷积" class="headerlink" title="图像去噪与卷积"></a>图像去噪与卷积</h3><h4 id="噪音图像"><a href="#噪音图像" class="headerlink" title="噪音图像"></a>噪音图像</h4><p>噪音图像：噪音也就是一个像素点和周围的像素点差别较大。</p>
<h5 id="图像去噪"><a href="#图像去噪" class="headerlink" title="图像去噪"></a>图像去噪</h5><p>对噪音像素点求加权平均值。如：</p>
<script type="math/tex; mode=display">
\text{去噪前} = \begin{bmatrix}64 & 76 & 69 \\ 71 & \color{Aqua}253 & 75\\68 & 74 & 78\end{bmatrix}\text{去噪后} = \begin{bmatrix}64 & 76 & 69 \\ 71 & \color{Aqua}92 & 75\\68 & 74 & 78\end{bmatrix}</script><p>其中$92 = 64 <em> \frac{1}{9} + 76</em> \frac{1}{9} + 69 <em> \frac{1}{9} + 71 </em> \frac{1}{9} + 253 <em> \frac{1}{9} + 78 </em> \frac{1}{9} + 68<em> \frac{1}{9} + 74</em> \frac{1}{9} + 78* \frac{1}{9}$ </p>
<p>像这类对像素进行统一操作，封装为卷积核。</p>
<h6 id="噪声的分类"><a href="#噪声的分类" class="headerlink" title="噪声的分类"></a>噪声的分类</h6><ul>
<li>椒盐噪声，就是黑点，白点。处理方式：中值滤波</li>
<li>脉冲噪声，只有白点</li>
<li>高斯噪声<br>这里主要介绍高斯噪声，高斯噪声数学模型是一个独立的加和模型$f(x,y) = \hat f(x, y) + \eta(x, y)$即认为图像是由真实图像+高斯噪声组成的，高斯噪声的产生一个是由于采集器附加的噪声，另一个是由于光学问题带来的噪声。将两者合起来就是最终看到的带有噪声的图像。因此对于高斯噪声就有了这样的假设：首先噪声的产生是相互独立的，而且服从均值为0的正态分布。<br><img src="./images/高斯噪音.jpg" alt="高斯噪音"><br>在应对高斯的噪声的处理时，自然就会想到高斯滤波，但是它也是有成本有代价，虽然它可能滤除噪声，但是它也会衰减部分信号，比如轮廓信息。</li>
</ul>
<h3 id="卷积的定义"><a href="#卷积的定义" class="headerlink" title="卷积的定义"></a>卷积的定义</h3><p>令F为图像，H为卷积核，F与H的卷积记为$R = F * H$</p>
<script type="math/tex; mode=display">
 R_{ij} = \sum_{u,v} H_{i-u,j-v} F_{u,v}</script><p>$<br>R_{ij} = \sum_{u,v} H_{i+u,j+v} F_{u,v} \text{称之为相关}<br>$</p>
<h4 id="卷积性质"><a href="#卷积性质" class="headerlink" title="卷积性质"></a>卷积性质</h4><ul>
<li>叠加性：$filter(f_1 + f_2) = filter(f_1) + filter(f_2)$</li>
<li>平移不变性：$filter(shift(f)) = shift(filter(f))$</li>
<li>交换律</li>
<li>结合律</li>
<li>分配律</li>
<li>标量</li>
</ul>
<h4 id="边界填充"><a href="#边界填充" class="headerlink" title="边界填充"></a>边界填充</h4><ul>
<li>拉伸填充</li>
<li>镜像填充</li>
<li>常数填充(零填充)</li>
</ul>
<p>卷积操作后的图像要小于输入时图像，通过边界填充，我们可以实现卷积前后图像的尺寸不变</p>
<h4 id="卷积的三种模式"><a href="#卷积的三种模式" class="headerlink" title="卷积的三种模式"></a>卷积的三种模式</h4><ul>
<li><ol>
<li>full<br>full模式的意思是，从filter和image刚相交开始做卷积，</li>
</ol>
</li>
<li><ol>
<li>same<br>当filter的中心(K)与image的边角重合时，开始做卷积运算</li>
</ol>
</li>
<li><ol>
<li>valid<br>当filter全部在image里面的时候，进行卷积运算</li>
</ol>
</li>
</ul>
<h4 id="卷积示例"><a href="#卷积示例" class="headerlink" title="卷积示例"></a>卷积示例</h4><ul>
<li><p>向左平移一个像素<br>$<br>\begin{bmatrix}<br>0  &amp;  0  &amp;  0 \\<br>0  &amp;  0  &amp;  1 \\<br>0  &amp;  0  &amp;  0 \\<br>\end{bmatrix}<br>$</p>
</li>
<li><p>向右平移一个像素<br>$<br>\begin{bmatrix}<br>0  &amp;  0  &amp;  0 \\<br>1  &amp;  0  &amp;  0 \\<br>0  &amp;  0  &amp;  0 \\<br>\end{bmatrix}<br>$</p>
</li>
</ul>
<ul>
<li><p>图像平滑<br>$<br>\begin{bmatrix}<br>1/9 &amp; 1/9 &amp; 1/9 \\<br>1/9 &amp; 1/9 &amp; 1/9 \\<br>1/9 &amp; 1/9 &amp; 1/9 \\<br>\end{bmatrix}<br>$<br><img src="./images/平滑卷积示意图.png" alt="平滑卷积示意图"></p>
</li>
<li><p>图像锐化<br>$<br>\begin{bmatrix}<br>0 &amp; 0 &amp; 0 \\<br>0 &amp; 2 &amp; 0 \\<br>0 &amp; 0 &amp; 0 \\<br>\end{bmatrix} -<br>\begin{bmatrix}<br>1/9 &amp; 1/9 &amp; 1/9 \\<br>1/9 &amp; 1/9 &amp; 1/9 \\<br>1/9 &amp; 1/9 &amp; 1/9 \\<br>\end{bmatrix}<br>$<br>即原图 - 平滑 = 边缘， 原图 + 边缘 = 锐化<br><img src="./images/锐化卷积示意图.png" alt="锐化卷积示意图"></p>
</li>
</ul>
<h4 id="高斯卷积核"><a href="#高斯卷积核" class="headerlink" title="高斯卷积核"></a>高斯卷积核</h4><p>均值平滑卷积核存在的问题<br><img src="./images/振铃.png" alt="振铃"><br>解决方法：根据邻域像素与中心的远近程度分配权重<br>引出高斯卷积核：高斯核用于图像的模糊处理,和均值滤波的作用类似.但不同的是高斯滤波进行的是一种加权平均,越靠近核中心的数值权重越大</p>
<script type="math/tex; mode=display">
𝐺\sigma = \frac{1}{2\pi \sigma^2}e^{− \frac{(x^2+y2)}{2\sigma^2}}</script><p><img src="./images/高斯卷积示意图.png" alt="高斯卷积示意图"></p>
<p>生成步骤：</p>
<ol>
<li><p>确定卷积核的尺寸 比如 5×5</p>
</li>
<li><p>设置高斯函数 的标准 差 ，比如 σ=1</p>
</li>
<li><p>计算卷积核各个位置权重值</p>
</li>
<li><p>对权重值进行归一化</p>
</li>
</ol>
<p>一般来说有个经验性的值。窗口大小为 $6\sigma + 1$</p>
<ul>
<li>高斯滤波，非常重要，贯穿整个计算机视觉，甚至现在的神经网络提取到的某些特征跟高斯滤波输出的结果都十分相似。其实高斯滤波就是滤除高频信息，是一个低通滤波器。 </li>
<li>高斯卷积的另外一个特性就是对一副图像进行连续两次  的高斯卷积输出结果等价于使用 $\sqrt{2}\sigma$ 的高斯卷积一次的输出结果。这个满足勾股定理的，比如连续的两次高斯卷积核大小为 $2\sigma,3\sigma$ 可以使用 $\sqrt{13}\sigma$ 高斯卷积核代替。大致意思就是两个小高斯核的连续卷积可以用一个大的高斯核代替。</li>
<li>高斯核还可以分解。<br><img src="./images/高斯核分解.jpg" alt="高斯核分解"></li>
</ul>
<p>现在就举个例子解释高斯核分解，假设有一个高斯卷积核与一个3x3大小的图像卷积得到应该是一个点。那么此时将高斯核拆解为两个一维向量，分别与图像进行卷积操作。它的主要作用就是加速运算。<br><img src="./images/高斯分解.jpg" alt="高斯分解"></p>
<p>如果使用一个mxm的卷积核对一副nxn大小的图像进行卷积，它的算法复杂度是 $O(n^2m^2)$ 而使用分解卷积算法复杂度 $O(n^2m)$ 【其实这是x或者y一个方向上的复杂度】，也就是说如果对核进行分离，那么复杂度就能够降低一个等级，这是一件很有意义的事情。从这里也可以看出来如果使用小核进行卷积也能够加速运算。</p>
<p>这些东西在深度神经网络中也会遇到，但是这里高斯卷积是完全等价的，在深度学习中不一定保证是等价的。</p>
<h2 id="卷积与边缘提取"><a href="#卷积与边缘提取" class="headerlink" title="卷积与边缘提取"></a>卷积与边缘提取</h2><h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h5><p>边缘检测是图像处理和计算机视觉中，尤其是特征提取中的一个研究领域。有许多方法用于边缘检测，它们的绝大部分可以划分为两类：</p>
<ul>
<li>基于一阶导数<br>首先计算边缘强度， 通常用一阶导数表示， 例如梯度模，然后，用计算估计边缘的局部方向， 通常采用梯度的方向，并利用此方向找到局部梯度模的最大值。即：图像一阶导数中的最大和最小值来检测边界，通常是将边界定位在梯度最大的方向。<br>一阶:Roberts Cross算子，Prewitt算子，Sobel算子， Kirsch算子，罗盘算子；<br>基于 零穿越/零交叉 的一类：二阶导数<br>基于零穿越的方法通过寻找图像二阶导数零穿越来寻找边界，通常是Laplacian过零点或者非线性差分表示的过零点。</li>
<li>基于二阶导数： Marr-Hildreth，在梯度方向的二阶导数过零点，Canny算子，Laplacian算子。</li>
</ul>
<h4 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h4><p>为什么要进行边缘检测呢？因为这是稳定的视觉特征，是人类经验的结果。边缘检测的目的是标识数字图像中亮度变化明显的点。图像属性中的显著变化通常反映了属性的重要事件和变化。主要包括：</p>
<ul>
<li>深度上的不连续（物体处在不同的物平面上）；</li>
<li>表面方向不连续（如正方体的不同的两个面）</li>
<li>物质属性变化（表面颜色不同，会导致光的反射系数不同）</li>
<li>场景照明变化（阴影）</li>
</ul>
<p>那么，对于下面这张图像进行边缘检测时，沿着这条红色的水平线，得到其每个像素点上的强度（也就是灰度值），由此可知边缘就是在像素值发生突变的地方。那么如果从一个信号中找到突变的地方呢？<br><img src="./images/边缘.jpg" alt="边缘"></p>
<p>显然，根据数学知识，对信号曲线进行求一阶导数，对于边缘来说，并不需要关注方向，只需要关注极值即可，所以可以通过求导得到边缘所在位置。由此将这根红色的水平线从上至下滑动即可得到整个图像的竖直方向上的边缘。<br><img src="./images/边缘导数.jpg" alt="边缘导数"></p>
<p>对于一个二元函数 $f(x,y)$ ，响应的的求导公式： </p>
<script type="math/tex; mode=display">
\frac{\partial f(x,y)}{\partial x} = \lim_{\epsilon \rightarrow 0} \frac{f(x+\varepsilon, y) - f(x,y)}{\varepsilon}</script><p>在图像处理过程中，对于像素值的位置的最小单位是 1 px，因此令 $\varepsilon = 1$ ，使用这种近似来作为点前位置的导数，则有：</p>
<script type="math/tex; mode=display">
\frac{\partial f(x,y)}{\partial x} \approx \lim_{\varepsilon \rightarrow 0} \frac{f(x+1, y) - f(x,y)}{1}</script><p>其实由这个公式可以看出，就是右面一个像素减去左面一个像素，作为当前位置的导数，这样简化之后其实就可以把这个过程使用卷积代替，即卷积核为：这就是检测竖直方向上边缘的卷积核。同理也可以得到水平方向上边缘的卷积核。<br><img src="./images/导数.png" alt="导数"></p>
<p>那么接下来举个例子，下面这两个边缘检测结果哪个是水平方向卷积核检测到的？哪个是竖直方向卷积核检测到的？因为只有水平卷积核检测的是左右差异较大的像素值，自然而然连成线之后就是竖直方向的线条。<br><img src="./images/图像偏导.png" alt="图像偏导"></p>
<p>接下来，解释一下梯度（一维叫导数，高维叫梯度），对于一副图像的一个像素点$<br>\triangledown f = [\frac{\partial f}{\partial x} , \frac{\partial f}{\partial y}]<br>$<br><img src="./images/像素梯度.jpg" alt="像素梯度"><br>对于夹角的计算方式 $\theta = \tan^{-1} (\frac{\partial f}{\partial y} / \frac{\partial f}{\partial x})$ ，而梯度的幅值： </p>
<script type="math/tex; mode=display">
\parallel \triangledown \parallel = \sqrt{(\frac{\partial f}{\partial x})^2 + (\frac{\partial f}{\partial y})^2}</script><p>这个幅值越大说面这点附近像素值变化越剧烈，就越有可能是边缘。</p>
<p>其实梯度对于一副图像来说就是图像变化剧烈的方向。而且 <strong><em>梯度方向与边缘是垂直</em></strong> 的。</p>
<h4 id="边缘检测流程"><a href="#边缘检测流程" class="headerlink" title="边缘检测流程"></a>边缘检测流程</h4><p>由于在实际应用过程中信号的采集往往伴随着噪声的出现，假设有下面这么一个一维信号，很显然边缘就在突变的地方。但是由于真实点附近存在噪声，如果直接使用边缘滤波器（边缘卷积）得到的结果会是什么样？<br><img src="./images/噪音图像.jpg" alt="噪音图像"><br><img src="./images/噪音求导.jpg" alt="噪音求导"></p>
<p>显然，通过边缘检测器之后得到导数（梯度）是无法确定极大值极小值的，因此无法判断边缘位置。所以，往往在进行边缘检测前首先要进行滤波。这是因为边缘检测算子主要是基于图像强度的一阶导数和二阶导数，但是通常情况下导数对噪声十分敏感，因此必须使用滤波器来进行平滑噪声。<br><img src="./images/过滤后.jpg" alt="噪音过滤"></p>
<p>因此对于一维图像 $f$ 使用高斯卷积核 $g$ 进行滤波， 通过将 $f<em>g$ (卷积)得到的结果可以看出，最左边和最右面为什么没了呢？ **</em>这就是因为卷积过程中如果不对边界补充的话，卷积结果相比原来图像会小一圈。<em>*</em> 经过高斯卷积之后的图像就相对平滑很多，在这个基础上再进行边缘检测，得到结果如下。很显然求极值就十分简单了。<br><img src="./images/结果.jpg" alt="噪音过滤结果"></p>
<p>因此，我们在对图像进行边缘检测前首先用高斯卷积对图像进行平滑就是了，因为我们也无法确定图像是否包含噪声。虽然上面这个过程实现了边缘检测，但是在这个过程中使用了两次卷积，首先是滤波过程的卷积，然后是求导过程的卷积，显然卷积是十分耗时的，那么能否使用一次卷积完成这个操作呢？</p>
<script type="math/tex; mode=display">
\frac{d}{dx} (f*g) = f * \frac{d}{dx} g</script><p>这个公式成立是因此卷积是满足交换律，结合律和分配率的。所以使用右面的公式，先对高斯卷积核进行求导，这个模板一般比较小，求导也相对简单，然后再进行卷积。 这样就能加速运算过程了。</p>
<p>这就是高斯卷积核求导之后的三维图像<br>虽然，使用平滑对图像进行去噪，但是它也会模糊图像，因此我们可以考虑在不同的scale下进行边缘检测。也就是选择响应的窗宽和标准差即可对图像进行平滑并边缘检测，由于窗宽一般默认经验值 $3\sigma$ ，因此只需要指定参数 $\sigma$ 即可执行这两个过程。所以可以考虑使用不用的$\sigma$在不同的scale下进行边缘检测。</p>
<p>因此，接下来对比重新认识一下高斯卷积核与高斯一阶导数核的区别：</p>
<p>高斯卷积核（smoothing filters）：高斯卷积实际上是滤除高频信号，是低通滤波器，滤波器模板中的数值没有负数，而且这些值相加和为1。<br><img src="./images/高斯卷积核.jpg" alt="高斯卷积核"></p>
<ul>
<li>高斯一阶导数核（derivative filters）：滤波器模板中的数值一定有负数，而且这些相加为0。<br><img src="./images/高斯一阶导数核.jpg" alt="高斯一阶导数核"><br>总结一下，对于一副图像进行边缘检测的流程</li>
<li>滤波</li>
<li>增强，增强算法可以将图像灰度点邻阈强度值有显著变化的点凸显出来。</li>
<li>边缘检测，经过增强的图像，往往邻域中有很多点的梯度值比较大，而在特定应用中，这些点并不是要找的边缘点，所以应该采用某些方法对这些点进行取舍。实际工程中，常用的方法是通过阈值化的方法进行检测。<br>三、Canny边缘检测<br><img src="./images/原图.jpg" alt="原图"><br>对于这样一张图像进行边缘检测时，首先第一步，对图像进行滤波处理，然后计算两个方向的梯度，先计算每个像素点的梯度，然后计算幅值，得到下面这张图像。<script type="math/tex; mode=display">
\parallel \triangledown f \parallel = \sqrt{(\frac{\partial f}{\partial x})^2 + (\frac{\partial f}{\partial y})^2} \\</script><img src="./images/边缘提取后.jpg" alt="边缘提取后"><br>在进行梯度计算时，梯度较大的地方可能是边缘也有可能是噪声，虽然已经进行过一次平滑滤波，但是仍然还会有一些高强度的噪声无法滤除，因此在这里选择使用阈值对其进行第二次过滤，去除一些梯度相对较小的点。<br><img src="./images/非极大值抑制.png" alt="非极大值抑制"></li>
</ul>
<p>然而经过阈值处理后，还是会有一些小问题，就是图像中的边缘会很宽，这是由于图像中的边缘像素值都是缓慢变化的，不会是一个垂直的突变，即使原始图像中的边缘是一个垂直的突变，经过高斯平滑之后它就会变得不那么垂直了，所以这就是为什么边会那么宽。那如何解决呢？</p>
<p>接下来就介绍了一个著名的算法： <strong><em>NMS非极大值抑制</em></strong>。它的一个主要思想就是，首先确定边上的一个点，然后沿着边的梯度方向比较跟相邻点的梯度进行比较，也即是右图中的 q 与 p，r 进行比较。如果 q 最大则保留，如果不是则舍去。<br><img src="./images/NMS非最大值抑制.jpg" alt="NMS非极大值抑制"></p>
<p>经过抑制以后：显然这就细化了很多，但是也会存在一定的问题，比如脖子下面的边缘消失了，出现了断断续续的情况，出现这种情况的原因是什么呢？这是因为设置的阈值太高了，导致这部分梯度被滤除掉了，但是如果阈值设置的较低又会出现很多“假边”，因此这里需要对刚刚设置阈值过滤这一步进行改进。<br><img src="./images/边消失.jpg" alt="双阈值"></p>
<p>改进的思路：就是使用双阈值法，首先使用一个较高的阈值去将那些确定度较高的边检测出来，称为“强边”，然后再使用一个较小的阈值显露更多的边，称为“弱边”，此时选择保留那些跟强边有连接关系的边。这个想法就很巧妙。<br><img src="./images/双阈值.jpg" alt="双阈值"></p>
<p>左边是高阈值，中间是低阈值，右边是双阈值</p>
<h5 id="Canny边缘检测器总结"><a href="#Canny边缘检测器总结" class="headerlink" title="Canny边缘检测器总结"></a>Canny边缘检测器总结</h5><ul>
<li><ol>
<li>用高斯一阶偏导核卷积图像</li>
</ol>
</li>
<li><ol>
<li>计算每个点的梯度幅值和方向</li>
</ol>
</li>
<li><ol>
<li>非极大值抑制：<ul>
<li>将宽的“边缘”细化至单个像素宽度</li>
</ul>
</li>
</ol>
</li>
<li>4.连接与阈值（滞后）：<ul>
<li>定义两个阈值：低和高</li>
<li>使用高阈值开始边缘曲线，使用低阈值继续边缘曲线</li>
</ul>
</li>
</ul>
<p>关于Canny边缘检测是有严格意义上的数学推导的，这个后续补充一下！</p>
<h3 id="纹理表示"><a href="#纹理表示" class="headerlink" title="纹理表示"></a>纹理表示</h3><h6 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h6><p>什么是纹理？计算机图形学中的纹理既包括通常意义上物体表面的纹理即使物体表面呈现凹凸不平的沟纹，同时也包括在物体的光滑表面上的彩色图案，通常我们更多地称之为花纹。</p>
<p>纹理是由于物体表面的物理属性的多样性而造成的,物理属性不同表示某个特定表面特征的灰度或者颜色信息不同,不同的物理表面会产生不同的纹理图像,因而纹理作为图像的一个极为重要的属性,在计算机视觉和图像处理中占有举足轻重的地位。纹理是图像中特征值强度的某种局部重复模式的宏观表现。然而,对于自然纹理图像而言这种重复模式往往是近似的和复杂的,难以用语言描述,而人类对纹理的感受多是与心理效果相结合的,因此,迄今都没有一个对纹理的正式的、广泛认可的和一致的定义。</p>
<p>Hawkins曾经对纹理给出了一个比较详细的描述,他认为纹理有三个主要的标志:</p>
<p>1)某种局部的序列性在比该序列更大的区域内不断重复</p>
<p>2)序列是由基本元素非随机排列组成的</p>
<p>3)各部分大致是均匀的统体,在纹理区域内的任何地方都有大致相同的结构尺</p>
<p>除了下面这种规则的纹理，也有不规则的，然而对于纹理（texture）的关注，其意义或者用途在于：Texture-related tasks。纹理相关任务</p>
<ul>
<li>shape from texture。从纹理中恢复形状。</li>
<li>segmentation/classification from texture cues。纹理分析</li>
<li>synthesis。用于图像合成</li>
</ul>
<p>该怎么表示纹理呢</p>
<h4 id="基于卷积核组的纹理表示方法"><a href="#基于卷积核组的纹理表示方法" class="headerlink" title="基于卷积核组的纹理表示方法"></a>基于卷积核组的纹理表示方法</h4><p>思路：利用 <strong><em>卷积核组</em></strong> 提取图像中的纹理基；利用基元的统计信息来表示图像中的纹理<br><img src="./images/卷积核组.png" alt="卷积核组"></p>
<h5 id="表示方法及其过程"><a href="#表示方法及其过程" class="headerlink" title="表示方法及其过程"></a>表示方法及其过程</h5><ul>
<li>法1<ul>
<li>设计卷积核组；</li>
<li>利用卷积核组对图像进行卷积操作获得对应的特征响应图组</li>
<li>利用特征响应图的某种统计信息来表示图像中的纹理 。</li>
</ul>
</li>
</ul>
<p><img src="./images/表示法1.png" alt="表示法1"><br>其中$r_i = [r_{i1},r_{i2},…,r_{i\times n}]$表示第i个特征图展开的向量<br>$r_{i\times n}$表示第i个特征图上第n个位置的响应值</p>
<p>将每个特征图都拉成向量使用全连接层进行分类。</p>
<ul>
<li>法2：忽略基元位置；关注出现了哪种基元对应的纹理以及基元出现的频率</li>
</ul>
<p>过程同上<br><img src="./images/表示法2.png" alt="表示法2"><br>其中的$\overline{r_1}$是取特征图$r_1$的均值，是一个值，这个值越大表示出现该特征的频率越高。<br>如下所示<br><img src="./images/小游戏.png" alt="小游戏"></p>
<h5 id="卷积核组设计"><a href="#卷积核组设计" class="headerlink" title="卷积核组设计"></a>卷积核组设计</h5><p>设计重点：</p>
<ul>
<li><p>卷积核类型 (边缘、条形以及点状)</p>
</li>
<li><p>卷积核尺度 (36 个尺度)</p>
</li>
<li><p>卷积核方向 ( 6 个角度)</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2.%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2.%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">全连接神经网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-23 19:45:56" itemprop="dateCreated datePublished" datetime="2022-01-23T19:45:56+08:00">2022-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-01-24 09:45:39" itemprop="dateModified" datetime="2022-01-24T09:45:39+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h1><h5 id="全连接神经网络组成"><a href="#全连接神经网络组成" class="headerlink" title="全连接神经网络组成"></a>全连接神经网络组成</h5><p>一个输入层、一个输出层以及多个隐层</p>
<p>输入层与输出层的神经元个数由任务决定，而隐层数量以及每个隐层的神经元个数需要人为指定</p>
<h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>激活函数是全连接神经网络中的一个重要部分，缺少了激活函数，全连接神经网络将退化为线性分类器。</p>
<p>常用的激活函数有：relu、tanh、sigmoid</p>
<h5 id="网络结构设计"><a href="#网络结构设计" class="headerlink" title="网络结构设计"></a>网络结构设计</h5><ol>
<li>用不用隐层，用一个还是几个隐层？（深度设计）</li>
<li>每个隐层设置多少个神经元比较合适?（宽度设计）</li>
</ol>
<p><code>结论：神经元个数越多，分界面就可以越复杂，在这个集合上的分类能力就越强</code></p>
<p><code>为什么一般的神经网络都需要两层全连接隐层，是因为第一层会提取同个类别的多个&quot;不同角度&quot;比如，头朝向、角度等等，这样分类更精准</code></p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><ul>
<li>softmax<br>将分类的预测值，变成概率（0-1之间）。</li>
</ul>
<ul>
<li>交叉熵<br>度量分类器预测分布与真实分布的距离<br><img src="./images/交叉熵损失.png" alt="交叉熵损失"></li>
</ul>
<p><code>真实分布是 one-hot编码</code></p>
<ul>
<li>交叉熵相关概念<ul>
<li>熵$ H(p) = - \sum_xp(x)logp(x)$</li>
<li>交叉熵$ H(p, q) = - \sum_xp(x)logq(x)$</li>
<li>相对熵$ KL(p || q) = - \sum_xp(x)log\frac{q(x)}{p(x)}$</li>
</ul>
</li>
</ul>
<p><code>相对熵也叫KL散度；用来度量两个分布之间的不相似性——即p，q不可交换</code><br><code>交叉熵会将预测正确的分数提高，其他分数降低，如下图所示，[10,9,9]还是预测准确，但还是有0.23损失，</code><br><img src="./images/交叉熵功能.png" alt="交叉熵功能"></p>
<p>三者之间的关系：</p>
<script type="math/tex; mode=display">
\begin{align*}
H(p, q) =& - \sum_xp(x)logq(x) \\
    =& - \sum_xp(x)logp(x) - \sum_xp(x)log\frac{q(x)}{p(x)} \\
    =& H(p) + KL(p || q)
\end{align*}</script><p><code>真实分布为one-hot编码时，交叉熵损失简化为</code>$L_i =  - log(q_j)\text{, 其中j为真实类别}$</p>
<h4 id="什么是计算图？"><a href="#什么是计算图？" class="headerlink" title="什么是计算图？"></a>什么是计算图？</h4><p>计算图是一种有向图，它用来表达输入、输出以及中间变量之间得计算关系，图中的每个节点对应着一种数学运算。<br><img src="./images/计算图例子.png" alt="计算图例子"><br><code>正向计算得到变量之间的关系，反向计算可计算对应的梯度</code></p>
<ul>
<li>前向计算的过程<br><img src="./images/前向计算过程.png" alt="前向计算过程"></li>
<li>反向计算的过程<br><img src="./images/反向计算的过程.png" alt="反向计算过程"><br>即可通过上游梯度和局部梯度快速的得到当前梯度</li>
</ul>
<h5 id="计算图的颗粒度"><a href="#计算图的颗粒度" class="headerlink" title="计算图的颗粒度"></a>计算图的颗粒度</h5><p><img src="./images/计算图的颗粒度.png" alt="计算图的颗粒度"><br>如上图将常用的函数变为一个整体计算，增大了计算的颗粒度，简化计算，缺点：可能需要自己定义颗粒度。</p>
<h5 id="计算图总结"><a href="#计算图总结" class="headerlink" title="计算图总结"></a>计算图总结</h5><ul>
<li>任意复杂的函数，都可以用计算图的形式表示</li>
<li>在整个计算图中，每个门单位都会得到一些输入，然后，进行下面两个计算：<ul>
<li>a) 这个门的输出值</li>
<li>b) 其输出值关于输入值的局部梯度</li>
</ul>
</li>
<li>利用链式法则，门单位应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</li>
</ul>
<h4 id="再看激活函数"><a href="#再看激活函数" class="headerlink" title="再看激活函数"></a>再看激活函数</h4><h5 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h5><p>梯度消失是神经网络训练中非常致命的一个问题，其本质是由于链式法则的乘法特性导致的。即当中间一个结点的梯度为0时，后续的梯度也全为零。</p>
<p><code>sigmoid和tanh函数，也是因为梯度特性不好，才导致使用较少</code></p>
<h5 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h5><p>其本质也是由于链式法则的乘法特性导致的。即当中间一个结点的梯度乘以学习率后得到一个非常大的值，从而“飞”出了合理区域，最终导致算法不收敛。</p>
<p>解决：把沿梯度方向前进的步长限制在某个值内就可避免。这个方法也叫<code>梯度裁剪</code>。</p>
<h5 id="激活函数选择总结"><a href="#激活函数选择总结" class="headerlink" title="激活函数选择总结"></a>激活函数选择总结</h5><p>尽量选择ReLU函数或者Leakly ReLU函数，相对于Sigmoid/tanh,ReLU函数或者leaklyReLU函数会让梯度流更加顺畅，训练过程收敛得更快。</p>
<h4 id="梯度下降法存在的问题"><a href="#梯度下降法存在的问题" class="headerlink" title="梯度下降法存在的问题"></a>梯度下降法存在的问题</h4><p>损失函数特性：一个方向上变化迅速而在另一个方向上变化缓慢。<br>优化目标：从起点处走到底端笑脸处<br>梯度下降算法存在的问题：山壁间震荡，往谷低方向的行进较慢。<br><img src="./images/梯度下降法存在的问题.png" alt="梯度下降法存在的问题"><br>仅增大步长并不能加快算法收敛速度！</p>
<h4 id="梯度算法改进"><a href="#梯度算法改进" class="headerlink" title="梯度算法改进"></a>梯度算法改进</h4><ul>
<li>动量法</li>
<li>自适应梯度与RMSProp</li>
<li>ADAM</li>
<li>总结<h5 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h5>目标：改进梯度下降算法的问题，即减少震荡，加速通往谷低<br>改进思想：利用累加历史梯度信息更新梯度（震荡的方向经过累加会减少，平坦方向会增大）<br><img src="./images/动量法.png" alt="动量法"></li>
</ul>
<h6 id="动量法还有什么效果？"><a href="#动量法还有什么效果？" class="headerlink" title="动量法还有什么效果？"></a>动量法还有什么效果？</h6><p>现象：损失函数常具有不太好的局部最小值或鞍点(高维空间非常常见)<br><code>梯度下降算法存在的问题</code>：局部最小处与鞍点处梯度为0，算法无法通过。<br><code>动量法的优势</code>: 由于动量的存在算法可以冲出局部最小点以及鞍点，找到更优的解。<br><img src="./images/鞍点与局部最小点.png" alt="鞍点与局部最小点"></p>
<h5 id="自适应梯度法"><a href="#自适应梯度法" class="headerlink" title="自适应梯度法"></a>自适应梯度法</h5><p>自适应梯度法利用梯度的平方来减少震荡方向步长，增大平坦方向步长。来减少震荡，加速通往谷底方向（设定不同的学习率来更改步长）<br><img src="./images/AdaGrad.png" alt="AdaGrad"><br><code>缺点</code>：当累计过多时，r会过大，导致真实的学习率会过小。就有了改进的RMSProp方法</p>
<h6 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h6><p><img src="./images/RMSProp.png" alt="RMSProp"></p>
<h5 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h5><p>同时使用动量与自适应梯度思想<br><img src="./images/adam.png" alt="adam"><br><code>修正偏差</code>：极大缓解算法初期的冷启动问题</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><h4 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h4><ul>
<li><p>全零初始化：网络中不同的神经元有相同的输出，进行同样的参数更新；因此，这些神经元学到的参数都一样，等价于一个神经元。（没法训练）</p>
</li>
<li><p>随机初始化：使用随机数进行初始化，eg：权值采用N(0, 0.01)的高斯分布，N(0, 1)的高斯分布，等都会有问题</p>
</li>
</ul>
<p><code>建议：采用随机初始化，避免全零初始化！</code></p>
<p>要求：</p>
<pre><code>* 实验结论：初始化让权值不相等，并不能保证网络能够正常的被训练。

* 有效的初始化方法：使网络各层的激活值和局部梯度的方差在传播过程中尽量保持一致；以保持网络中正向和反向数据流动。
</code></pre><h5 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h5><p>网络结构：10个隐层，1个输出层，每个隐层包含500个神经元，使用的<code>双曲正切</code>激活函数。<br>随机初始化：权值采样自$N(0, 1/N)$的高斯分布，$N$为输入神经元个数</p>
<p>一个神经元，其输入为$z_1,z_2,…,z_N$,这N个输入是独立同分布的；其权值为$w_1,w_2,…,w_N$。它们也是独立同分布的，且$w\text{与}z$是独立的；其激活函数为$f$；其最总输出$y$的表达式为：</p>
<script type="math/tex; mode=display">
y = f(w_1 * z_1 + w_2 * z_2 + ... + w_N * z_N)</script><p><img src="./images/Xavier.png" alt="Xavier"><br>目标：使网络各层的激活值和局部梯度的方差在传播过程中尽量保持一致，即寻找w的分布使得输出y和输出z的方差一致。</p>
<h5 id="HE初始化-MSRA"><a href="#HE初始化-MSRA" class="headerlink" title="HE初始化(MSRA)"></a>HE初始化(MSRA)</h5><p>网络结构：10个隐层，1个输出层，每个隐层包含500个神经元，使用的<code>ReLU</code>激活函数。<br>随机初始化：权值采样自$N(0, 2/N)$的高斯分布，$N$为输入神经元个数</p>
<h4 id="权值初始化小结"><a href="#权值初始化小结" class="headerlink" title="权值初始化小结"></a>权值初始化小结</h4><ul>
<li>好的初始化方法可以防止前向传播过程中的消息消失，也可以解决反向传递过程中的梯度消失。</li>
<li>激活函数选择双曲正切或者sigmoid时，建议使用Xaizer初始化方法；</li>
<li>激活函数选择ReLU或者Leakly ReLU时，建议使用He初始化方法；</li>
</ul>
<h4 id="批归一化-BN"><a href="#批归一化-BN" class="headerlink" title="批归一化(BN)"></a>批归一化(BN)</h4><p>直接对神经元的输出进行批归一化<br>操作：对一批输出进行减均值除方差操作；可保证当前神经元的输出值的分布符合0均值1方差。</p>
<p><code>如果每一层的每个神经元进行批归一化，就能解决前向传递过程中的信号消失问题。</code></p>
<p><code>经常插入到全连接层后，非线性激活前</code><br><img src="./images/批归一化.png" alt="批归一化"><br><img src="./images/批归一化算法.png" alt="批归一化"></p>
<h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><ul>
<li>最优方案————获取更多的训练数据</li>
<li>次优方案————调节模型允许存储的信息量或者对模型允许存储的信息加以约束，该类方法也称为正则化。<ul>
<li>调节模型大小</li>
<li>越苏模型权重，即权重正则化(常用的有L1、L2正则化)</li>
<li>随机失活(Dropout)</li>
</ul>
</li>
</ul>
<h6 id="随机失活-Dropout"><a href="#随机失活-Dropout" class="headerlink" title="随机失活(Dropout)"></a>随机失活(Dropout)</h6><ul>
<li><p>随机失活: 让隐层的神经元以一定的概率不被激活。</p>
</li>
<li><p>实现方式：训练过程中，对某一层使用Dropout，就是随机将该层的一些输出舍弃(输出值设为0)，这些被舍弃的神经元就好像被网络删除了一样。</p>
</li>
<li><p>随机失活比率：是被设定为0的特征所占的比例，通常在0.2————0.5范围内。</p>
</li>
</ul>
<h6 id="为什么随机失活能够防止过拟合？"><a href="#为什么随机失活能够防止过拟合？" class="headerlink" title="为什么随机失活能够防止过拟合？"></a>为什么随机失活能够防止过拟合？</h6><ul>
<li><p>解释1：随机失火使得每次更新梯度时参与计算的网络参数减少了，降低了模型容量，所以能防止过拟合。</p>
</li>
<li><p>解释2：随机失活鼓励重分散，从这个角度来看随机失活也能起到正则化的作用，进而防止过拟合。（即失活前，可能每个神经元只学习单一的特征例如眼睛、鼻子等，失活后，神经元个数减少所以每个神经元学的的特征会变多，不再是单一特征）</p>
</li>
<li><p>解释3：随机失活可以看成模型集成</p>
</li>
</ul>
<h4 id="神经网络中的超参数"><a href="#神经网络中的超参数" class="headerlink" title="神经网络中的超参数"></a>神经网络中的超参数</h4><p>超参数：</p>
<pre><code>* 网络结构————隐层神经元个数，网络层数，非线性单元选择等
* 优化相关————学习率、dropout比率、正则项强度等
</code></pre><ul>
<li><p>常用超参数优化方法</p>
<ul>
<li>网格搜索法：<br>①每个超参数分别取几个值)组合这些超参数值，形成多组超参数;<br>②(在验证集上评估每组超参数的模型性能;<br>③选择性能最优的模型所采用的那组值作为最终的超参数的值。</li>
<li>随机搜索法：<br>①参数空间内随机取点，每个点对应一组超参数;<br>②在验证集上评估每组超参数的模型性能;<br>③选择性能最优的模型所采用的那组值作为最终的超参数的值。</li>
</ul>
</li>
<li><p>超参数搜索策略：</p>
<ul>
<li>粗搜索：使用随机法在交大范围采样，训练一个周期，依次缩小超参数范围。</li>
<li>精搜索：利用随机法在粗搜索后的范围内采样，运行模型五到十个周期，选择验证集上精度最高的那组超参数。</li>
</ul>
</li>
</ul>
<p><code>对于学习率、正则项强度这类超参数，在对数空间上进行随机采样更合适！（不敏感）即：0.0001， 0.001，0.01，0.1，1</code></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/4.%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/4.%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">卷积神经网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-23 19:45:56" itemprop="dateCreated datePublished" datetime="2022-01-23T19:45:56+08:00">2022-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-01-24 09:46:00" itemprop="dateModified" datetime="2022-01-24T09:46:00+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="卷积神经网络CNN"><a href="#卷积神经网络CNN" class="headerlink" title="卷积神经网络CNN"></a>卷积神经网络CNN</h2><p>网络主要成分：</p>
<ul>
<li>CONV————卷积层</li>
<li>RELU————激活层</li>
<li>POOL————池化层</li>
<li>FC————全连接层</li>
</ul>
<h5 id="卷积网络中的卷积核"><a href="#卷积网络中的卷积核" class="headerlink" title="卷积网络中的卷积核"></a>卷积网络中的卷积核</h5><p>卷积核：</p>
<ul>
<li><p>不仅具有宽 和 高 还具有深度 ，常写成如下形式<br>  宽度 x 高度 x 深度</p>
</li>
<li><p>卷积核参数不仅包括核中存储的权值，还包括一个偏置值</p>
</li>
</ul>
<p><img src="./images/卷积核示意图.png" alt="卷积核示意图"></p>
<h5 id="卷积网络中的卷积操作"><a href="#卷积网络中的卷积操作" class="headerlink" title="卷积网络中的卷积操作"></a>卷积网络中的卷积操作</h5><p>卷积结果，计算过程如下：</p>
<ul>
<li><p>将卷积核展成一个 5x5x3 的向量，同时将其覆盖的图像区域按相同的展开方式展成 5x5x3 的向量</p>
</li>
<li><p>计算两者的点乘。</p>
</li>
<li><p>在点乘的结果上加上偏移量</p>
</li>
</ul>
<p>公式如下：</p>
<script type="math/tex; mode=display">
𝒘𝑻x + 𝑏</script><p>$𝒘$为卷积核的权值， $𝑏$为卷积核的偏置<br><code>特征响应图中每个位置上的值反映了图像上对应位置是否存在卷积核所记录的基元结构信息</code></p>
<h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><ul>
<li><p>特征响应图组深度等于卷积核的个数</p>
</li>
<li><p>不同的特征响应图反映了输入图像对不同卷积核的响应结果；</p>
</li>
<li><p>同一特征响应图上不同位置的值表示输入图像上不同位置对同一卷积核的响应结果。<br><code>注意：卷积层输入不局限于图像，可以是任意三维数据矩阵该层的卷积核深度要求与输入的 三维 矩阵的深度一致 。</code></p>
<h5 id="卷积步长-stride"><a href="#卷积步长-stride" class="headerlink" title="卷积步长(stride)"></a>卷积步长(stride)</h5><p>卷积神经网络中，卷积核可以按照指定的间隔进行卷积操作这个间隔就是 <strong><em>卷积步长</em></strong> 。</p>
</li>
</ul>
<h5 id="边界填充-padding"><a href="#边界填充-padding" class="headerlink" title="边界填充(padding)"></a>边界填充(padding)</h5><p>卷积神经网络中最常用的填充方式是 <strong><em>零值填充</em></strong> 。</p>
<h5 id="常见的字母缩写及尺寸计算"><a href="#常见的字母缩写及尺寸计算" class="headerlink" title="常见的字母缩写及尺寸计算"></a>常见的字母缩写及尺寸计算</h5><ul>
<li>F————卷积核 尺寸</li>
<li>S————卷积步长</li>
<li>P————零填充 数量</li>
<li><p>K————卷积核个数</p>
</li>
<li><p>输入数据矩阵尺寸：$W_1 × H_1 × D_1$</p>
</li>
<li>输出特征图组尺寸：$W_2 × H_2 × D_2$<br>$W_2$ 与 $W_1$ 关系如下：<script type="math/tex; mode=display">
W_2=(W_1F+2P)/S+1\\
H_2=(H_2F +2P)/S+1\\
D_2 = K</script><code>作用：保持输入、输出尺寸的一致</code></li>
</ul>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><ul>
<li><p>池化的作用：对 每一个特征响应图独立进行 降低特征响应图组中每个特征响应图的宽度和高度，减少 后续卷积层的 参数的数量， 降低 计算资源耗费 ，进而 控制过拟合。</p>
</li>
<li><p>池化操作：对特征响应图某个区域进行池化就是在该区域上指定一个值来代表整个区域 。</p>
</li>
<li><p>常见的池化操作：</p>
<ul>
<li><p>最大池化 使用区域内的最大值来代表这个区域；</p>
</li>
<li><p>平均池化 采用区域内所有值的均值作为代表。</p>
</li>
</ul>
</li>
<li><p>池化层的超参数： 池化窗口和池化步长</p>
</li>
</ul>
<h5 id="池化操作示例"><a href="#池化操作示例" class="headerlink" title="池化操作示例"></a>池化操作示例</h5><ul>
<li><p>池化操作对每一个特征响应图独立进行</p>
</li>
<li><p>对特征响应图某个区域进行池化就是在该区域上指定一个值来代表整个区域 。</p>
</li>
</ul>
<p><img src="./images/池化操作.png" alt="池化操作"></p>
<h4 id="损失函数-amp-优化算法"><a href="#损失函数-amp-优化算法" class="headerlink" title="损失函数 &amp; 优化算法"></a>损失函数 &amp; 优化算法</h4><ul>
<li><p>损失函数 ：交叉熵损失</p>
</li>
<li><p>优化算法 SGD 、带动量的 SGD 以及 ADAM</p>
</li>
</ul>
<h5 id="图像增强"><a href="#图像增强" class="headerlink" title="图像增强"></a>图像增强</h5><ul>
<li><p>存在的问题 ：过拟合的原因是学习样本太少，导致无法训练出能够泛化到新数据的模型。</p>
</li>
<li><p>数据增强 ：是从现有的训练样本中生成更多的训练数据，其方法是利用多种能够生成可信图像的随机变换来增加样本。</p>
</li>
<li><p>数据增强的目标 ：模型在训练时不会两次查看完全相同的图像。这让模型能够观察到数据的更多内容，从而具有更好的泛化能力</p>
</li>
</ul>
<h6 id="样本增强——翻转"><a href="#样本增强——翻转" class="headerlink" title="样本增强——翻转"></a>样本增强——翻转</h6><p><img src="./images/图像增强-翻转.png" alt="图像增强-翻转"></p>
<h6 id="样本增强——随机缩放-amp-抠图"><a href="#样本增强——随机缩放-amp-抠图" class="headerlink" title="样本增强——随机缩放 &amp; 抠图"></a>样本增强——随机缩放 &amp; 抠图</h6><p><img src="./images/随机缩放&amp;抠图.png" alt="图像增强-随机缩放&amp;抠图"></p>
<h6 id="样本增强——色彩抖动"><a href="#样本增强——色彩抖动" class="headerlink" title="样本增强——色彩抖动"></a>样本增强——色彩抖动</h6><p>操作步骤：<br>1.利用主成分分析方法提取当前图像的色彩数据([R G B])的主轴<br>2.沿着主轴方向随机采样一个偏移；<br>3.将偏移量加入当前图像的每个像素。</p>
<p><img src="./images/色彩抖动.png" alt="图像增强-色彩抖动"></p>
<h6 id="样本增强——其他方案"><a href="#样本增强——其他方案" class="headerlink" title="样本增强——其他方案"></a>样本增强——其他方案</h6><p>随机联合下述操作</p>
<ul>
<li>平移</li>
<li>旋转</li>
<li>拉伸</li>
<li>径向畸变（相关描述见摄像机几何章节）</li>
<li>裁剪<br>…</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/6.%20%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2&%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/6.%20%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2&%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" class="post-title-link" itemprop="url">图像分割&目标检测</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-23 19:45:56" itemprop="dateCreated datePublished" datetime="2022-01-23T19:45:56+08:00">2022-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-01-24 09:46:30" itemprop="dateModified" datetime="2022-01-24T09:46:30+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="视觉识别"><a href="#视觉识别" class="headerlink" title="视觉识别"></a>视觉识别</h3><p>视觉识别任务</p>
<ul>
<li>分类：不考虑空间位置</li>
<li>语义分割：像素的类别</li>
<li>目标检测：多目标</li>
<li>实例分割：多目标</li>
</ul>
<h4 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h4><p>给每个像素分配类别标签，不区分实例，只考虑像素类别。</p>
<h6 id="分割思路：滑动窗口"><a href="#分割思路：滑动窗口" class="headerlink" title="分割思路：滑动窗口"></a>分割思路：滑动窗口</h6><p>简单来说，就是那个框，在图片上移动，就看框里面有没有你要找的目标，<br><img src="./images/滑动窗口示例.gif" alt="滑动窗口示例"></p>
<p>利用CNN对中心点像素分类,如下所示<br><img src="./images/滑动窗口.png" alt="滑动窗口"><br><code>问题: 效率太低 重叠区域的特征反复被计算</code></p>
<h6 id="分割思路：全卷积"><a href="#分割思路：全卷积" class="headerlink" title="分割思路：全卷积"></a>分割思路：全卷积</h6><p>让整个网络只包含卷积层，一次性输出所有像素的类别预测。如下所示<br><img src="./images/FCN网络结构.jpg" alt="FCN网络结构"></p>
<p>该网络在前面两步跟CNN的结构是一样的，但是在CNN网络Flatten的时候，FCN网络将之换成了一个卷积核size为5x5，输出通道为50的卷积层，之后的全连接层都换成了1x1的卷积层。我们知道1x1的卷积其实就相当于全连接操作。</p>
<p>换成全卷积操作后，由于没有了全连接层的输入层神经元个数的限制，所以卷积层的输入可以 <strong><em>接受不同尺寸的图像，也就不用要求训练图像和测试图像size一致</em></strong> 。</p>
<p>我们将网络放到一个正常的28x28x3的图像上，考虑上特征图的通道数，看下输出值的对应情况，如下图：<br><img src="./images/FCN工作过程.jpg" alt="FCN网络结构"><br>因为这是一个猫狗和背景的三分类任务，所以最后输出的图像大小为8x8x3，以输出图像左上角绿色点为例，该点深度为3，对应输入图像的绿色区域，该点的3个值反应了输入图的绿色区域是分类为猫狗还是背景的得分情况。</p>
<p>总的来说，FCN利用了输出结果和输入图像的对应关系，直接给出了输入图像相应区域的分类情况，取消了传统目标检测中的滑动窗口选取候选框。</p>
<h6 id="FCN的优缺点"><a href="#FCN的优缺点" class="headerlink" title="FCN的优缺点"></a>FCN的优缺点</h6><p>输出结果的每个值映射到输入图像上的感受野的窗口是固定的，也就是检测窗口是固定的，导致检测效果没那么好，但是速度却得到了很大的提升，而且可以输入任意尺寸的图片，为目标检测提供了一种新思路。</p>
<h4 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h4><p><code>问题：FCN处理过程中一直保持原始分辨率，对于显存的需求会非常庞大</code></p>
<p>解决方案：让整个网络只包含卷积层，并在网络中嵌入下采样与上采样过程。<br><img src="./images/全卷积.png" alt="全卷积"></p>
<p><code>问题：什么是上采样呢？</code></p>
<h6 id="FCN上采样理论"><a href="#FCN上采样理论" class="headerlink" title="FCN上采样理论"></a>FCN上采样理论</h6><p>FCN网络一般是用来对图像进行语义分割的，于是就需要对图像上的各个像素进行分类，这就需要一个上采样将最后得到的输出上采样到原图的大小。上采样对于低分辨率的特征图，常常采用上采样的方式将它还原高分辨率，这里陈述上采样的三种方法。</p>
<ul>
<li><p>反池化操作Unpooling<br>上采样不保留位置信息直接复制.<br><img src="./images/Unpooling.png" alt="Unpooling"></p>
</li>
<li><p>反池化操作: “Max Unpooling”<br>上池化保留位置信息补0<br><img src="./images/MaxPooling.png" alt="Unpooling"></p>
</li>
<li><p>上采样: 转置卷积(Transpose Convolution)<br>即通过卷积核和输出将输入还原出来。<br>如：</p>
<script type="math/tex; mode=display">
input = \begin{bmatrix}
x_1 & x_2 & x_3 & x_4 \\
x_6 & x_7 & x_8 & x_9 \\
x_{10} & x_{11} & x_{12} & x_{13} \\
x_{14} & x_{15} & x_{16} & x_{17} \\
\end{bmatrix} 
\\
kernel = \begin{bmatrix}
w_{0,0} & w_{0,1} & w_{0,2} \\
w_{1,0} & w_{1,1} & w_{1,2} \\
w_{2,0} & w_{2,1} & w_{2,2} \\
\end{bmatrix}</script><p>设 步长 stride=1、填充 padding=0，则按 “valid” 卷积模式，可得 2×2 输出矩阵 output</p>
<script type="math/tex; mode=display">
output=\begin{bmatrix}
y_0 & y_1 \\
y_2 & y_3 \\
\end{bmatrix}</script><p>这里，换一个表达方式，将输入矩阵 input 和输出矩阵 output 展开成 16×1 列向量 X 和 4×1 列向量 Y，可分别表示为：</p>
<script type="math/tex; mode=display">
X = \begin{bmatrix}
x_1 \\ x_2 \\...\\x_17
\end{bmatrix}
\\
Y = \begin{bmatrix}
y_0 \\ y_1 \\ y_2 \\y_3
\end{bmatrix}</script><p>接着，再用矩阵运算来描述标准卷积运算，设有 新卷积核矩阵 C：</p>
<script type="math/tex; mode=display">
Y = C X</script><p>经推导 (卷积运算关系)，可得 4×16 稀疏矩阵 C：</p>
<script type="math/tex; mode=display">
C = \begin{bmatrix}
w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 & 0\\
0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0\\
0 & 0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2}\\
\end{bmatrix}</script></li>
</ul>
<p>而转置卷积其实就是要对这个过程进行逆运算，即 通过 C 和 Y 得到 X：</p>
<script type="math/tex; mode=display">
X = C^T Y</script><p>此时，即为新的 16×4 稀疏矩阵。以下通过转置后的卷积矩阵运算。此处，用于转置卷积的权重矩阵不一定来自于原卷积矩阵(通常不会如此恰巧)，但其形状和原卷积矩阵的转置相同。</p>
<p>最后，将 16×1 的输出结果重新排序，即可通过 2×2 输入矩阵得到 4×4 输出矩阵。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/5.%20CNN%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/5.%20CNN%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%A7%A3%E6%9E%90/" class="post-title-link" itemprop="url">CNN经典神经网络解析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-23 19:45:56" itemprop="dateCreated datePublished" datetime="2022-01-23T19:45:56+08:00">2022-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-01-24 09:46:13" itemprop="dateModified" datetime="2022-01-24T09:46:13+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="经典网络解析"><a href="#经典网络解析" class="headerlink" title="经典网络解析"></a>经典网络解析</h2><ul>
<li><p>AlexNet</p>
</li>
<li><p>ZFNet</p>
</li>
<li><p>VGG</p>
</li>
<li><p>GoogleNet</p>
</li>
<li><p>ResNet</p>
</li>
</ul>
<h4 id="经典网络解析——AlexNet"><a href="#经典网络解析——AlexNet" class="headerlink" title="经典网络解析——AlexNet"></a>经典网络解析——AlexNet</h4><p>AlexNet——验证了深度卷积神经网络 的高效性</p>
<ul>
<li><p>主体贡献：<br>  1.提出了一种卷积层加全连接层的卷积神经网络结构<br>  2.首次使用 ReLU 函数 做为 神经网络的 激活函数<br>  3.首次提出 Dropout 正则化来控制过拟合<br>  4.使用加入动量的小批量 梯度下降算法加速了 训练过程的 收敛；<br>  5.使用数据增强策略，极大地抑制了训练过程的 过拟合；<br>  6.利用了GPU的并行计算能力，加速了网络的训练与推断。<br><img src="./images/Alex网络架构.jpg" alt="Alex网络架构"><br><img src="./images/Alex结构.png" alt="Alex结构"><br><code>NORM 是局部响应归一化层</code></p>
</li>
<li><p>层数统计说明：</p>
<ul>
<li><p>计算网络层数时，仅统计卷积层与全连接层；</p>
</li>
<li><p>池化层与各种归一化层都是对它们前面卷积层输出的特征图进行后处理，不单独算作一层。</p>
</li>
<li><p>AlexNet共8层</p>
<ul>
<li><p>5 个卷积层 (CONV1 CONV5)</p>
</li>
<li><p>3 个全连接层 (FC6 FC8)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>在第3，4层卷积层没有进行最大池化与局部归一化</code></p>
<ul>
<li><p>重要说明</p>
<ul>
<li><p>用于提取图像特征的卷积层以及用于分类的全连接层是同时学习的</p>
</li>
<li><p>卷积层与全连接层在学习过程中会相互影响、相互促进</p>
</li>
</ul>
</li>
<li><p>重要技巧</p>
<ul>
<li>Dropout 策略防止过拟合</li>
<li><p>使用加入动量的随机梯度下降算法 ，加速收敛</p>
</li>
<li><p>验证集损失不下降时 手动降低 10 倍的学习率</p>
</li>
<li><p>采用样本增强策略增加训练样本数量， 防止过拟合；</p>
</li>
<li><p>集成多个模型，进一步提高精度。</p>
</li>
</ul>
</li>
</ul>
<h6 id="AlexNet-卷积层在做什么？"><a href="#AlexNet-卷积层在做什么？" class="headerlink" title="AlexNet 卷积层在做什么？"></a>AlexNet 卷积层在做什么？</h6><ul>
<li><p>从数据中学习对于分类有意义的结构特征，</p>
</li>
<li><p>描述输入图像中的结构信息</p>
</li>
<li><p>描述结果存储在 256 个 6 × 6 的特征响应图里。</p>
</li>
</ul>
<h3 id="经典网络解析——ZFNet"><a href="#经典网络解析——ZFNet" class="headerlink" title="经典网络解析——ZFNet"></a>经典网络解析——ZFNet</h3><p>与AlexNet网络结构基本一致</p>
<p>主要改进：</p>
<ul>
<li>将第一个卷积层的卷积核大小改为了 7 × 7(可以感受更细致的东西，浅层丢掉细致的东西，后面无法学习)</li>
<li>将第二、第三个卷积层的卷积步长都设置为 2(图像信息尺度减少速度不会太快，能多学特征信息)</li>
<li>增加了第三、第四个卷积层的卷积核个数。(层数高了后会学到语义信息，卷积核个数太少描述不好，就学不好；如：同一张图的猫的头朝向，角度等等)<br><img src="./images/Alex结构.png" alt="Alex结构"></li>
</ul>
<h3 id="经典网络解析——VGG"><a href="#经典网络解析——VGG" class="headerlink" title="经典网络解析——VGG"></a>经典网络解析——VGG</h3><p>VGG网络贡献：</p>
<ul>
<li><p>使用尺寸更小的 3x3 卷积核串联来获得更大的感受野</p>
</li>
<li><p>放弃使用 11x11 和 5x5 这样的大尺寸卷积核</p>
</li>
<li><p>深度更深、非线性更强，网络的参数也更少</p>
</li>
<li><p>去掉了 AlexNet 中的局部响应归一化层（ LRN ）层 。</p>
</li>
</ul>
<p><img src="./images/VGG结构.png" alt="VGG结构"></p>
<p>VGG16</p>
<ul>
<li><p>13 个卷积层与 3 个全连接</p>
</li>
<li><p>分为 5 段 conv1,…,conv5 每一段中卷积层的卷积核个数均相同</p>
</li>
<li><p>卷积层均采用 3x3 的卷积核及 ReLU 激活函数；</p>
</li>
<li><p>所有的池化层都采用最大池化，其窗口大小为 2x2 、步长为 2</p>
</li>
<li><p>经过一次池化操作，其后卷积层的卷积核个数就增加一倍 ，直至到达 512</p>
</li>
<li><p>全连接层中也使用了Dropout策略</p>
</li>
</ul>
<h4 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h4><p><img src="./images/小卷积.png" alt="小卷积"></p>
<p><img src="./images/思考2.png" alt="思考2"></p>
<p><img src="./images/思考3.png" alt="思考3"></p>
<h3 id="经典网络解析——GoogLeNet"><a href="#经典网络解析——GoogLeNet" class="headerlink" title="经典网络解析——GoogLeNet"></a>经典网络解析——GoogLeNet</h3><p>GoogleNet的创新点</p>
<ul>
<li><p>提出了一种 Inception 结构，它能保留输入信号中的更多特征信息；</p>
</li>
<li><p>去掉了 AlexNet 的前两个全连接层，并采用了平均池化，这一设计使得GoogLeNet 只有 500 万参数，比 AlexNet 少了 12 倍；</p>
</li>
<li><p>在网络的中部引入了辅助分类器，克服了训练过程中的梯度消失问题。</p>
</li>
</ul>
<p><img src="./images/GoogleNet网络.jpg" alt="GoogleNet网络"></p>
<p><img src="./images/Inception模块.png" alt="Inception模块"><br><img src="./images/Inception模块v1.png" alt="Inception模块v1"></p>
<p><code>层数更深、参数更少、计算效率更高、非线性表达能力也更强</code></p>
<p>GoogleNet的后面几层<br><img src="./images/GoogleNet深层.png" alt="GoogleNet深层"></p>
<ul>
<li><p>VGG 的第一个全连接层参数占了整个网络 74% 的参数 ，需dropout策略应对过拟合。</p>
</li>
<li><p>GoogLeNet 采用平均池化 GoogLeNet 的参数总量不到 500 万，无须使用 Dropout 策略 。</p>
</li>
</ul>
<p><img src="./images/辅助分类器.png" alt="辅助分类器"></p>
<h6 id="为什么引入辅助分类器？"><a href="#为什么引入辅助分类器？" class="headerlink" title="为什么引入辅助分类器？"></a>为什么引入辅助分类器？</h6><p>原因：虽然 ReLU 单元能够一定程度解决梯度消失问题，但是并不能完全解决深层网络难以训练的问题 。 离输出远的层就不如靠近输出的层训练得好。</p>
<p>结果：让低层的卷积层学习到的特征也有很好的区分能力，从而让网络更好地被训练，而且低层的卷积层学到了好的特征也能加速整个网络的收敛。</p>
<p>网络推断：仅利用网络最后的输出作为预测结果，忽略辅助分类器的输出。</p>
<h5 id="思考-1"><a href="#思考-1" class="headerlink" title="思考"></a>思考</h5><p>问题1: 平均池化向量化与直接展开向量化有什么区别？</p>
<ul>
<li><p>特征响应图上每个位置的值反应了图像对应位置的结构与卷积核记录的语义结构的相似程度</p>
</li>
<li><p>平均池化丢失了语义结构的空间位置信息</p>
</li>
<li><p>忽略语义结构的位置信息，有助于提升卷积层提取到的特征的平移不变性</p>
</li>
</ul>
<p>问题2: 利用 1x1 卷积进行压缩会损失信息吗？<br><img src="./images/1×1压缩.png" alt="1×1压缩"></p>
<ul>
<li><p>位置A的这个 64 维向量是一个非常稀疏向量</p>
</li>
<li><p>利用1x1卷积进行非线性压缩通常不会损失信息。</p>
</li>
</ul>
<h4 id="经典网络解析——ResNet"><a href="#经典网络解析——ResNet" class="headerlink" title="经典网络解析——ResNet"></a>经典网络解析——ResNet</h4><p>实验：持续向一个“基础”的卷积神经网络上面叠加更深的层数会发生什么？<br><img src="./images/Res.png" alt="Res"></p>
<p>ResNet具有以下贡献：</p>
<ul>
<li><p>提出了一种残差模块，通过堆叠残差模块可以构建任意<br>深度的神经网络，而不会出现“退化”现象。</p>
</li>
<li><p>提出了批归一化方法来对抗梯度消失，该方法降低了网<br>络训练过程对于权重初始化的依赖；</p>
</li>
<li><p>提出了一种针对 ReLU 激活函数的初始化方法；</p>
</li>
</ul>
<p><img src="./images/残差模块.png" alt="残差模块"><br>研究者考虑了这样一个问题<br>浅层网络学习到了有效的分类模式后，如何向上堆积新层来建立更深的网络， 使其满足即使不能提升浅层网络的性能，深层网络也不应降低性能。</p>
<p>解决方案：残差模块<br>假设卷积层学习的变换为$𝐹(𝑋)$ ，残差结构的输出是$𝐻(𝑋)$ ，则有：</p>
<script type="math/tex; mode=display">
𝐻(𝑋)  =  𝐹(𝑋)  + 𝑋 \\
F(X) = H(X) - X</script><p>残差(F(x))也是由此得来</p>
<p>关于残差结构：</p>
<ol>
<li><p>残差结构能够避免普通的卷积层堆叠存在信息丢失问题 保证前向信息流的顺畅 。</p>
</li>
<li><p>残差结构能够应对梯度反传过程中的梯度消失问题 保证反向梯度流的通顺 。</p>
</li>
</ol>
<h6 id="思考-2"><a href="#思考-2" class="headerlink" title="思考"></a>思考</h6><p><img src="./images/瓶颈结构.png" alt="瓶颈结构"><br>问题1：为什么在残差结构里是先1×1卷积再3×1再1×1，有什么好处？</p>
<p>先通过1×1卷积，将数据压缩(不降低数据量)，减少数据参数和计算，最后再放大成原来的通道数。</p>
<p>问题2：为什么残差网络性能这么好？<br>一种典型的解释：残差网络可以看作是一种集成模型！<br><img src="./images/展开的残差.png" alt="瓶颈结构"><br>看成多个子网络的集成<br>结论：残差网络是一种集成模型,这是重要特点也是它高效的一个原因！</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>介绍了 5 种经典的卷积神经网络 AlexNet、 ZFNet、 VGG、 GoogLeNet 和 ResNet</p>
<p>残差网络和 Inception V4 是 公认 的推广性能最好的两个分类模型</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/1.%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhouxy">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/1.%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">图像分类任务介绍</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-23 19:45:55" itemprop="dateCreated datePublished" datetime="2022-01-23T19:45:55+08:00">2022-01-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-01-24 09:44:55" itemprop="dateModified" datetime="2022-01-24T09:44:55+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/cv/" itemprop="url" rel="index"><span itemprop="name">cv</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h3><h4 id="1-什么是图像分类任务？"><a href="#1-什么是图像分类任务？" class="headerlink" title="1. 什么是图像分类任务？"></a>1. 什么是图像分类任务？</h4><p>图像分类：从一至的类别标签中为给定的输入图片选定一个标签。eg：标签：{狗，猫，卡车，飞机，…}</p>
<h4 id="2-它有哪些应用场景？"><a href="#2-它有哪些应用场景？" class="headerlink" title="2. 它有哪些应用场景？"></a>2. 它有哪些应用场景？</h4><ul>
<li>淘宝搜索同类宝贝</li>
<li>识别个人不认识的动物/植物</li>
</ul>
<p><code>即将语言不好描述的东西，使用图像描述来识别。</code><br><code>跨越&quot;语义鸿沟&quot;建立像素到语义的映射</code></p>
<h4 id="3-图像识别有哪些难点"><a href="#3-图像识别有哪些难点" class="headerlink" title="3. 图像识别有哪些难点"></a>3. 图像识别有哪些难点</h4><ul>
<li>视角<br><img src="./images/视角.png" alt="视角"></li>
<li>光照<br><img src="./images/光照.png" alt="光照"></li>
<li>尺度<br><img src="./images/尺度.png" alt="尺度"><br>即不管人站的近还是远，都能识别出人脸</li>
<li>遮挡<br><img src="./images/遮挡.png" alt="遮挡"></li>
<li>形变<br><img src="./images/形变.png" alt="形变"></li>
<li>背景杂波<br><img src="./images/背景杂波.png" alt="背景杂波"></li>
<li>类内形变(多形态)<br><img src="./images/类内形变.png" alt="类内形变"></li>
<li>运动模糊<br><img src="./images/运动模糊.png" alt="运动模糊"><br>可以使用高速相机或将图像恢复成正常状态来进行识别</li>
<li>类别繁多<br><img src="./images/类别繁多.png" alt="类别繁多"></li>
</ul>
<p><code>不是每个识别都要处理所有的难点，而是根据场景的不同，处理部分的难点。</code></p>
<h4 id="4-基于规则的方法是否可行？"><a href="#4-基于规则的方法是否可行？" class="headerlink" title="4. 基于规则的方法是否可行？"></a>4. 基于规则的方法是否可行？</h4><p>通过<code>硬编码</code>的方法识别猫或其他类，是一件<code>很困难</code>的事。</p>
<h4 id="5-什么是数据驱动的图像分类"><a href="#5-什么是数据驱动的图像分类" class="headerlink" title="5. 什么是数据驱动的图像分类"></a>5. 什么是数据驱动的图像分类</h4><p>数据驱动的图像分类步骤：</p>
<ol>
<li>数据集构建</li>
<li>分类器设计与学习</li>
<li>分类器决策<br><img src="./images/训练步骤.png" alt="训练步骤"></li>
</ol>
<h5 id="图像表示"><a href="#图像表示" class="headerlink" title="图像表示"></a>图像表示</h5><ul>
<li>像素表示<ul>
<li>全局特征表示(GIST)</li>
<li>局部特征表示(SIFT特征+词袋模型)</li>
</ul>
</li>
</ul>
<h5 id="分类器"><a href="#分类器" class="headerlink" title="分类器"></a>分类器</h5><ul>
<li>近邻分类器</li>
<li>贝叶斯分类器</li>
<li><code>线性分类器</code></li>
<li>支持向量机分类器</li>
<li><code>神经网络分类器</code></li>
<li>随机森林</li>
<li>adaboost</li>
</ul>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><ul>
<li>0-1损失</li>
<li>多类支撑向量机损失</li>
<li>交叉熵损失</li>
<li>L1损失</li>
<li>L2损失</li>
<li>折页损失</li>
</ul>
<h5 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h5><p>一阶方法</p>
<ul>
<li>梯度下降</li>
<li>随机梯度</li>
<li>小批量梯度下降</li>
</ul>
<p>二阶方法</p>
<ul>
<li>牛顿法</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<h5 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h5><ul>
<li>数据集划分</li>
<li>数据预处理</li>
<li>数据增强</li>
<li>欠拟合和过拟合<ul>
<li>减小算法复杂度</li>
<li>使用权重正则项</li>
<li>使用droput正则化</li>
</ul>
</li>
<li>超参数调整</li>
<li>模型集成</li>
</ul>
<h4 id="5-常用分类任务的评价指标"><a href="#5-常用分类任务的评价指标" class="headerlink" title="5. 常用分类任务的评价指标"></a>5. 常用分类任务的评价指标</h4><ul>
<li><p>正确率：分对的样本数/全部样本数</p>
</li>
<li><p>错误率：1 - 正确率</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhouxy</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
