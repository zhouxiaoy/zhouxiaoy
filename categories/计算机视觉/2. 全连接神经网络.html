<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="全连接神经网络全连接神经网络组成一个输入层、一个输出层以及多个隐层 输入层与输出层的神经元个数由任务决定，而隐层数量以及每个隐层的神经元个数需要人为指定 激活函数激活函数是全连接神经网络中的一个重要部分，缺少了激活函数，全连接神经网络将退化为线性分类器。 常用的激活函数有：relu、tanh、sigmoid 网络结构设计 用不用隐层，用一个还是几个隐层？（深度设计） 每个隐层设置多少个神经元比较合">
<meta property="og:type" content="website">
<meta property="og:title" content="My Blog">
<meta property="og:url" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2.%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="全连接神经网络全连接神经网络组成一个输入层、一个输出层以及多个隐层 输入层与输出层的神经元个数由任务决定，而隐层数量以及每个隐层的神经元个数需要人为指定 激活函数激活函数是全连接神经网络中的一个重要部分，缺少了激活函数，全连接神经网络将退化为线性分类器。 常用的激活函数有：relu、tanh、sigmoid 网络结构设计 用不用隐层，用一个还是几个隐层？（深度设计） 每个隐层设置多少个神经元比较合">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E4%BA%A4%E5%8F%89%E7%86%B5%E5%8A%9F%E8%83%BD.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%BE%8B%E5%AD%90.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E5%8F%8D%E5%90%91%E8%AE%A1%E7%AE%97%E7%9A%84%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E9%A2%97%E7%B2%92%E5%BA%A6.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E5%8A%A8%E9%87%8F%E6%B3%95.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E9%9E%8D%E7%82%B9%E4%B8%8E%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E7%82%B9.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/AdaGrad.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/RMSProp.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/adam.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/Xavier.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96.png">
<meta property="og:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%AE%97%E6%B3%95.png">
<meta property="article:published_time" content="2022-01-15T03:07:54.864Z">
<meta property="article:modified_time" content="2022-01-15T03:07:54.864Z">
<meta property="article:author" content="zhouxy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.png">


<link rel="canonical" href="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2.%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":false,"lang":"zh-CN","comments":true,"permalink":"http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/2.%20%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html","path":"categories/计算机视觉/2. 全连接神经网络.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title> | My Blog
</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">My Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.</span> <span class="nav-text">全连接神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%84%E6%88%90"><span class="nav-number">1.0.0.0.1.</span> <span class="nav-text">全连接神经网络组成</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.0.0.0.2.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="nav-number">1.0.0.0.3.</span> <span class="nav-text">网络结构设计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.0.0.0.4.</span> <span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%9F"><span class="nav-number">1.0.0.1.</span> <span class="nav-text">什么是计算图？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E9%A2%97%E7%B2%92%E5%BA%A6"><span class="nav-number">1.0.0.1.1.</span> <span class="nav-text">计算图的颗粒度</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E6%80%BB%E7%BB%93"><span class="nav-number">1.0.0.1.2.</span> <span class="nav-text">计算图总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%8D%E7%9C%8B%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.0.0.2.</span> <span class="nav-text">再看激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">1.0.0.2.1.</span> <span class="nav-text">梯度消失</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="nav-number">1.0.0.2.2.</span> <span class="nav-text">梯度爆炸</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E9%80%89%E6%8B%A9%E6%80%BB%E7%BB%93"><span class="nav-number">1.0.0.2.3.</span> <span class="nav-text">激活函数选择总结</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.0.0.3.</span> <span class="nav-text">梯度下降法存在的问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E6%94%B9%E8%BF%9B"><span class="nav-number">1.0.0.4.</span> <span class="nav-text">梯度算法改进</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="nav-number">1.0.0.4.1.</span> <span class="nav-text">动量法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8A%A8%E9%87%8F%E6%B3%95%E8%BF%98%E6%9C%89%E4%BB%80%E4%B9%88%E6%95%88%E6%9E%9C%EF%BC%9F"><span class="nav-number">1.0.0.4.1.1.</span> <span class="nav-text">动量法还有什么效果？</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-number">1.0.0.4.2.</span> <span class="nav-text">自适应梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#RMSProp"><span class="nav-number">1.0.0.4.2.1.</span> <span class="nav-text">RMSProp</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Adam"><span class="nav-number">1.0.0.4.3.</span> <span class="nav-text">Adam</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">1.0.1.</span> <span class="nav-text">训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%83%E5%80%BC%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.0.1.1.</span> <span class="nav-text">权值初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Xavier%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">1.0.1.1.1.</span> <span class="nav-text">Xavier初始化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#HE%E5%88%9D%E5%A7%8B%E5%8C%96-MSRA"><span class="nav-number">1.0.1.1.2.</span> <span class="nav-text">HE初始化(MSRA)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%83%E5%80%BC%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B0%8F%E7%BB%93"><span class="nav-number">1.0.1.2.</span> <span class="nav-text">权值初始化小结</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96-BN"><span class="nav-number">1.0.1.3.</span> <span class="nav-text">批归一化(BN)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">1.0.1.4.</span> <span class="nav-text">过拟合与欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="nav-number">1.0.1.4.1.</span> <span class="nav-text">解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BB-Dropout"><span class="nav-number">1.0.1.4.1.1.</span> <span class="nav-text">随机失活(Dropout)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BB%E8%83%BD%E5%A4%9F%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="nav-number">1.0.1.4.1.2.</span> <span class="nav-text">为什么随机失活能够防止过拟合？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E8%B6%85%E5%8F%82%E6%95%B0"><span class="nav-number">1.0.1.5.</span> <span class="nav-text">神经网络中的超参数</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhouxy</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner page posts-expand">


    
    
    
    <div class="post-block" lang="zh-CN"><header class="post-header">

<h1 class="post-title" itemprop="name headline">
</h1>

<div class="post-meta-container">
  <ul class="breadcrumb">
            <li><a href="/categories/">CATEGORIES</a></li>
            <li><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></li>
            <li>2. 全连接神经网络</li>
          
  </ul>
</div>

</header>

      
      
      <div class="post-body">
          <h1 id="全连接神经网络"><a href="#全连接神经网络" class="headerlink" title="全连接神经网络"></a>全连接神经网络</h1><h5 id="全连接神经网络组成"><a href="#全连接神经网络组成" class="headerlink" title="全连接神经网络组成"></a>全连接神经网络组成</h5><p>一个输入层、一个输出层以及多个隐层</p>
<p>输入层与输出层的神经元个数由任务决定，而隐层数量以及每个隐层的神经元个数需要人为指定</p>
<h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><p>激活函数是全连接神经网络中的一个重要部分，缺少了激活函数，全连接神经网络将退化为线性分类器。</p>
<p>常用的激活函数有：relu、tanh、sigmoid</p>
<h5 id="网络结构设计"><a href="#网络结构设计" class="headerlink" title="网络结构设计"></a>网络结构设计</h5><ol>
<li>用不用隐层，用一个还是几个隐层？（深度设计）</li>
<li>每个隐层设置多少个神经元比较合适?（宽度设计）</li>
</ol>
<p><code>结论：神经元个数越多，分界面就可以越复杂，在这个集合上的分类能力就越强</code></p>
<p><code>为什么一般的神经网络都需要两层全连接隐层，是因为第一层会提取同个类别的多个&quot;不同角度&quot;比如，头朝向、角度等等，这样分类更精准</code></p>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><ul>
<li>softmax<br>将分类的预测值，变成概率（0-1之间）。</li>
</ul>
<ul>
<li>交叉熵<br>度量分类器预测分布与真实分布的距离<br><img src="./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1.png" alt="交叉熵损失"></li>
</ul>
<p><code>真实分布是 one-hot编码</code></p>
<ul>
<li>交叉熵相关概念<ul>
<li>熵$ H(p) = - \sum_xp(x)logp(x)$</li>
<li>交叉熵$ H(p, q) = - \sum_xp(x)logq(x)$</li>
<li>相对熵$ KL(p || q) = - \sum_xp(x)log\frac{q(x)}{p(x)}$</li>
</ul>
</li>
</ul>
<p><code>相对熵也叫KL散度；用来度量两个分布之间的不相似性——即p，q不可交换</code><br><code>交叉熵会将预测正确的分数提高，其他分数降低，如下图所示，[10,9,9]还是预测准确，但还是有0.23损失，</code><br><img src="./images/%E4%BA%A4%E5%8F%89%E7%86%B5%E5%8A%9F%E8%83%BD.png" alt="交叉熵功能"></p>
<p>三者之间的关系：<br>$$<br>\begin{align*}<br>H(p, q) =&amp; - \sum_xp(x)logq(x) \<br>    =&amp; - \sum_xp(x)logp(x) - \sum_xp(x)log\frac{q(x)}{p(x)} \<br>    =&amp; H(p) + KL(p || q)<br>\end{align*}<br>$$</p>
<p><code>真实分布为one-hot编码时，交叉熵损失简化为</code>$L_i =  - log(q_j)\text{, 其中j为真实类别}$</p>
<h4 id="什么是计算图？"><a href="#什么是计算图？" class="headerlink" title="什么是计算图？"></a>什么是计算图？</h4><p>计算图是一种有向图，它用来表达输入、输出以及中间变量之间得计算关系，图中的每个节点对应着一种数学运算。<br><img src="./images/%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%BE%8B%E5%AD%90.png" alt="计算图例子"><br><code>正向计算得到变量之间的关系，反向计算可计算对应的梯度</code></p>
<ul>
<li>前向计算的过程<br><img src="./images/%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png" alt="前向计算过程"></li>
<li>反向计算的过程<br><img src="./images/%E5%8F%8D%E5%90%91%E8%AE%A1%E7%AE%97%E7%9A%84%E8%BF%87%E7%A8%8B.png" alt="反向计算过程"><br>即可通过上游梯度和局部梯度快速的得到当前梯度</li>
</ul>
<h5 id="计算图的颗粒度"><a href="#计算图的颗粒度" class="headerlink" title="计算图的颗粒度"></a>计算图的颗粒度</h5><p><img src="./images/%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E9%A2%97%E7%B2%92%E5%BA%A6.png" alt="计算图的颗粒度"><br>如上图将常用的函数变为一个整体计算，增大了计算的颗粒度，简化计算，缺点：可能需要自己定义颗粒度。</p>
<h5 id="计算图总结"><a href="#计算图总结" class="headerlink" title="计算图总结"></a>计算图总结</h5><ul>
<li>任意复杂的函数，都可以用计算图的形式表示</li>
<li>在整个计算图中，每个门单位都会得到一些输入，然后，进行下面两个计算：<ul>
<li>a) 这个门的输出值</li>
<li>b) 其输出值关于输入值的局部梯度</li>
</ul>
</li>
<li>利用链式法则，门单位应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。</li>
</ul>
<h4 id="再看激活函数"><a href="#再看激活函数" class="headerlink" title="再看激活函数"></a>再看激活函数</h4><h5 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h5><p>梯度消失是神经网络训练中非常致命的一个问题，其本质是由于链式法则的乘法特性导致的。即当中间一个结点的梯度为0时，后续的梯度也全为零。</p>
<p><code>sigmoid和tanh函数，也是因为梯度特性不好，才导致使用较少</code></p>
<h5 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h5><p>其本质也是由于链式法则的乘法特性导致的。即当中间一个结点的梯度乘以学习率后得到一个非常大的值，从而“飞”出了合理区域，最终导致算法不收敛。</p>
<p>解决：把沿梯度方向前进的步长限制在某个值内就可避免。这个方法也叫<code>梯度裁剪</code>。</p>
<h5 id="激活函数选择总结"><a href="#激活函数选择总结" class="headerlink" title="激活函数选择总结"></a>激活函数选择总结</h5><p>尽量选择ReLU函数或者Leakly ReLU函数，相对于Sigmoid/tanh,ReLU函数或者leaklyReLU函数会让梯度流更加顺畅，训练过程收敛得更快。</p>
<h4 id="梯度下降法存在的问题"><a href="#梯度下降法存在的问题" class="headerlink" title="梯度下降法存在的问题"></a>梯度下降法存在的问题</h4><p>损失函数特性：一个方向上变化迅速而在另一个方向上变化缓慢。<br>优化目标：从起点处走到底端笑脸处<br>梯度下降算法存在的问题：山壁间震荡，往谷低方向的行进较慢。<br><img src="./images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98.png" alt="梯度下降法存在的问题"><br>仅增大步长并不能加快算法收敛速度！</p>
<h4 id="梯度算法改进"><a href="#梯度算法改进" class="headerlink" title="梯度算法改进"></a>梯度算法改进</h4><ul>
<li>动量法</li>
<li>自适应梯度与RMSProp</li>
<li>ADAM</li>
<li>总结<h5 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h5>目标：改进梯度下降算法的问题，即减少震荡，加速通往谷低<br>改进思想：利用累加历史梯度信息更新梯度（震荡的方向经过累加会减少，平坦方向会增大）<br><img src="./images/%E5%8A%A8%E9%87%8F%E6%B3%95.png" alt="动量法"></li>
</ul>
<h6 id="动量法还有什么效果？"><a href="#动量法还有什么效果？" class="headerlink" title="动量法还有什么效果？"></a>动量法还有什么效果？</h6><p>现象：损失函数常具有不太好的局部最小值或鞍点(高维空间非常常见)<br><code>梯度下降算法存在的问题</code>：局部最小处与鞍点处梯度为0，算法无法通过。<br><code>动量法的优势</code>: 由于动量的存在算法可以冲出局部最小点以及鞍点，找到更优的解。<br><img src="./images/%E9%9E%8D%E7%82%B9%E4%B8%8E%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E7%82%B9.png" alt="鞍点与局部最小点"></p>
<h5 id="自适应梯度法"><a href="#自适应梯度法" class="headerlink" title="自适应梯度法"></a>自适应梯度法</h5><p>自适应梯度法利用梯度的平方来减少震荡方向步长，增大平坦方向步长。来减少震荡，加速通往谷底方向（设定不同的学习率来更改步长）<br><img src="./images/AdaGrad.png" alt="AdaGrad"><br><code>缺点</code>：当累计过多时，r会过大，导致真实的学习率会过小。就有了改进的RMSProp方法</p>
<h6 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h6><p><img src="./images/RMSProp.png" alt="RMSProp"></p>
<h5 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h5><p>同时使用动量与自适应梯度思想<br><img src="./images/adam.png" alt="adam"><br><code>修正偏差</code>：极大缓解算法初期的冷启动问题</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><h4 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h4><ul>
<li><p>全零初始化：网络中不同的神经元有相同的输出，进行同样的参数更新；因此，这些神经元学到的参数都一样，等价于一个神经元。（没法训练）</p>
</li>
<li><p>随机初始化：使用随机数进行初始化，eg：权值采用N(0, 0.01)的高斯分布，N(0, 1)的高斯分布，等都会有问题</p>
</li>
</ul>
<p><code>建议：采用随机初始化，避免全零初始化！</code></p>
<p>要求：<br>    * 实验结论：初始化让权值不相等，并不能保证网络能够正常的被训练。</p>
<pre><code>* 有效的初始化方法：使网络各层的激活值和局部梯度的方差在传播过程中尽量保持一致；以保持网络中正向和反向数据流动。
</code></pre>
<h5 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h5><p>网络结构：10个隐层，1个输出层，每个隐层包含500个神经元，使用的<code>双曲正切</code>激活函数。<br>随机初始化：权值采样自$N(0, 1/N)$的高斯分布，$N$为输入神经元个数</p>
<p>一个神经元，其输入为$z_1,z_2,…,z_N$,这N个输入是独立同分布的；其权值为$w_1,w_2,…,w_N$。它们也是独立同分布的，且$w\text{与}z$是独立的；其激活函数为$f$；其最总输出$y$的表达式为：<br>$$<br>y = f(w_1 * z_1 + w_2 * z_2 + … + w_N * z_N)<br>$$<br><img src="./images/Xavier.png" alt="Xavier"><br>目标：使网络各层的激活值和局部梯度的方差在传播过程中尽量保持一致，即寻找w的分布使得输出y和输出z的方差一致。</p>
<h5 id="HE初始化-MSRA"><a href="#HE初始化-MSRA" class="headerlink" title="HE初始化(MSRA)"></a>HE初始化(MSRA)</h5><p>网络结构：10个隐层，1个输出层，每个隐层包含500个神经元，使用的<code>ReLU</code>激活函数。<br>随机初始化：权值采样自$N(0, 2/N)$的高斯分布，$N$为输入神经元个数</p>
<h4 id="权值初始化小结"><a href="#权值初始化小结" class="headerlink" title="权值初始化小结"></a>权值初始化小结</h4><ul>
<li>好的初始化方法可以防止前向传播过程中的消息消失，也可以解决反向传递过程中的梯度消失。</li>
<li>激活函数选择双曲正切或者sigmoid时，建议使用Xaizer初始化方法；</li>
<li>激活函数选择ReLU或者Leakly ReLU时，建议使用He初始化方法；</li>
</ul>
<h4 id="批归一化-BN"><a href="#批归一化-BN" class="headerlink" title="批归一化(BN)"></a>批归一化(BN)</h4><p>直接对神经元的输出进行批归一化<br>操作：对一批输出进行减均值除方差操作；可保证当前神经元的输出值的分布符合0均值1方差。</p>
<p><code>如果每一层的每个神经元进行批归一化，就能解决前向传递过程中的信号消失问题。</code></p>
<p><code>经常插入到全连接层后，非线性激活前</code><br><img src="./images/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96.png" alt="批归一化"><br><img src="./images/%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%AE%97%E6%B3%95.png" alt="批归一化"></p>
<h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><ul>
<li>最优方案————获取更多的训练数据</li>
<li>次优方案————调节模型允许存储的信息量或者对模型允许存储的信息加以约束，该类方法也称为正则化。<ul>
<li>调节模型大小</li>
<li>越苏模型权重，即权重正则化(常用的有L1、L2正则化)</li>
<li>随机失活(Dropout)</li>
</ul>
</li>
</ul>
<h6 id="随机失活-Dropout"><a href="#随机失活-Dropout" class="headerlink" title="随机失活(Dropout)"></a>随机失活(Dropout)</h6><ul>
<li><p>随机失活: 让隐层的神经元以一定的概率不被激活。</p>
</li>
<li><p>实现方式：训练过程中，对某一层使用Dropout，就是随机将该层的一些输出舍弃(输出值设为0)，这些被舍弃的神经元就好像被网络删除了一样。</p>
</li>
<li><p>随机失活比率：是被设定为0的特征所占的比例，通常在0.2————0.5范围内。</p>
</li>
</ul>
<h6 id="为什么随机失活能够防止过拟合？"><a href="#为什么随机失活能够防止过拟合？" class="headerlink" title="为什么随机失活能够防止过拟合？"></a>为什么随机失活能够防止过拟合？</h6><ul>
<li><p>解释1：随机失火使得每次更新梯度时参与计算的网络参数减少了，降低了模型容量，所以能防止过拟合。</p>
</li>
<li><p>解释2：随机失活鼓励重分散，从这个角度来看随机失活也能起到正则化的作用，进而防止过拟合。（即失活前，可能每个神经元只学习单一的特征例如眼睛、鼻子等，失活后，神经元个数减少所以每个神经元学的的特征会变多，不再是单一特征）</p>
</li>
<li><p>解释3：随机失活可以看成模型集成</p>
</li>
</ul>
<h4 id="神经网络中的超参数"><a href="#神经网络中的超参数" class="headerlink" title="神经网络中的超参数"></a>神经网络中的超参数</h4><p>超参数：<br>    * 网络结构————隐层神经元个数，网络层数，非线性单元选择等<br>    * 优化相关————学习率、dropout比率、正则项强度等</p>
<ul>
<li><p>常用超参数优化方法</p>
<ul>
<li>网格搜索法：<br>①每个超参数分别取几个值)组合这些超参数值，形成多组超参数;<br>②(在验证集上评估每组超参数的模型性能;<br>③选择性能最优的模型所采用的那组值作为最终的超参数的值。</li>
<li>随机搜索法：<br>①参数空间内随机取点，每个点对应一组超参数;<br>②在验证集上评估每组超参数的模型性能;<br>③选择性能最优的模型所采用的那组值作为最终的超参数的值。</li>
</ul>
</li>
<li><p>超参数搜索策略：</p>
<ul>
<li>粗搜索：使用随机法在交大范围采样，训练一个周期，依次缩小超参数范围。</li>
<li>精搜索：利用随机法在粗搜索后的范围内采样，运行模型五到十个周期，选择验证集上精度最高的那组超参数。</li>
</ul>
</li>
</ul>
<p><code>对于学习率、正则项强度这类超参数，在对数空间上进行随机采样更合适！（不敏感）即：0.0001， 0.001，0.01，0.1，1</code></p>

      </div>
      
      
      
    </div>
  <ul class="breadcrumb">
            <li><a href="/categories/">CATEGORIES</a></li>
            <li><a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a></li>
            <li>2. 全连接神经网络</li>
          
  </ul>

    
    


</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhouxy</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  





</body>
</html>
